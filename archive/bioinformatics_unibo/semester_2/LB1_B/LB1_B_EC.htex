<h1 id="introduction">Introduction</h1>
<ul>
<li>Protein structure is more conserved than sequence: proteins sharing high sequence identity usually have similar structures</li>
<li>When sequence identity is sufficiently high, we can exploit the sequence alignment to tranfer structural information between proteins</li>
<li>A structural alignment is a rigid body transformation of 2 subsets from 2 sets of points that maximizes a given distance metric
<ul>
<li>The subsets need to have the same number of elements and define the corrispondence set for the alignment</li>
<li>Finding the corrispondece set is an NP-hard problem</li>
<li>Finding the optimal rigid transformation of the corrispondence set is an <eq env="math">O(n)</eq> problem</li>
</ul></li>
<li>Read Chothia C, Lesk AM. 1986</li>
<li>Measuring sequence similarity allows to eastimate structural similarity</li>
<li>In order to measure sequence similarity I need to
<ul>
<li>Define a distance metric</li>
<li>Find the alignment that minimizes that metric</li>
<li>Evaluate the statistical significance of the alignment</li>
</ul></li>
<li>The distance among 2 sequences is evaluated according to the amount of mutation incurred between them</li>
<li>The distance of 2 sequences can be evaluated with a substitution matrix and a gap penalty
<ul>
<li>The score of an alignment is the sum of the pairwise scores</li>
<li>A gap can be evaluated with a linear penalty that depends only on its length, or with an affine penalty that values differently gap opening and extension</li>
</ul></li>
<li>Global alignments are computed with the NW algorithm, local alignments with the SW</li>
<li>Local alignment aims at finding the most similar subsequence</li>
<li>Local alignments are useful for multidomain proteins, when only some domains are conserved</li>
<li>The significance of an alignment score can be evaluated by comparing with the score distribution of random alignments</li>
<li>Sanders and Schneider developed a twilight curve before Rost
<ul>
<li>Read paper</li>
<li>They developped a curve that separates pairs of proteins with more than 70% structural identity and pairs with less than 70% structural identity</li>
<li>This plot was in a 2D space of alignmnent lenght vs sequence identity</li>
<li>Sander’s curve becomes straight after 80 residues, while Rost’s curve never becomes straight</li>
</ul></li>
<li>Rost: above 20% of sequence identity 90% of the alignments correspond to homologous proteins, while below 20% only 10% of the alignment are homologous
<ul>
<li>This is true for alignments longer than 100 residues!</li>
<li>Over 20% identity sequences longer than 100 residues have similar structures, but this does NOT mean that under 20% the structure is necessarily different!</li>
<li>Proteins with low sequence identity but high structural similarity are referred to as remote homologs</li>
</ul></li>
<li>Rost distinguished 3 zones of protein alignments
<ul>
<li>In the safe zone (abova 30% of identity) practically all the alignments are true homologs</li>
<li>In the twilight zone (between 20% and 30%) there are many false positives</li>
<li>In the midnight zone (below 20%) homologs are abundant (remote homologs) but not recognizable among in a sea of non-homologs</li>
</ul></li>
<li>Comparative modelling matches a protein of unknown structure (target) with a potential template that has a structure through sequence alignment, it produces the a model of the target from it and then it evaluates the model
<ul>
<li>If the model is good I keep it, otherwise I try with a different template</li>
</ul></li>
<li>Structures can be predicted by comparative modeling, threading, and ab initio predictions</li>
<li>Structures have various degrees of quality and usefulness
<ul>
<li>Experimental NMR and X-Ray structures are around 1 <eq env="math">\AA</eq> resolution and can be used for studying catalytic mechanisms and for drug design</li>
<li>Homology models have are comparable to structures of 4-1.5 <eq env="math">\AA</eq> resolution and can be produced when we can find a template with sequence identity above 30%
<ul>
<li>They can be used for docking of small ligands, finding epitopes</li>
<li>Real structures at this resolution can be used for molecular replacement (a method for solving the phase problem by using the diffraction pattern of a known structure)</li>
</ul></li>
<li>Below 30% identity I am in the zone were threading is needed
<ul>
<li>These models are comparable to structures of 4 <eq env="math">\AA</eq> resolution</li>
<li>They can be used for establishing broad functionality by comparison with other structures</li>
</ul></li>
</ul></li>
<li>If I want to use sequence identity for trasferring annotation features, I need to identify the problem-specific twilight region
<ul>
<li>For subcellularlocalization, the twilight zone is 50% (!)</li>
</ul></li>
<li>The sequence identity needed for transferring subcellular localization is higher than that required for structure</li>
<li>Function of proteins with really high sequence identity can be completely different</li>
<li>In remote homologs the sequence alignment is often wrong</li>
<li>Important residues in a sequence can be identified by comparing conservation levels</li>
</ul>
<h1 id="structural-alignment">Structural alignment</h1>
<ul>
<li>Structural alignment is different from superimposition</li>
<li>Superimposition assumes that I already have the correspondence set, and it is relatively easy</li>
<li>Structural alignment requires the identification of the correspondence set, which is hard</li>
<li>The definition of domain is often heuristic and questionable</li>
<li>Proteins with similar spatial distribution but different topology are difficult to align</li>
<li>Alignment methods can be classified in different ways
<ul>
<li>Pairwise or multiple</li>
<li>Depending on the descriptor used
<ul>
<li>Backbone</li>
<li>All atoms</li>
<li>Sequence-based</li>
<li>Contact map</li>
<li>Surface</li>
</ul></li>
<li>Rigid body or flexible</li>
</ul></li>
<li>The comparison of torsion angles is <eq env="math">\Theta(n)</eq>
<ul>
<li>They are invariant for rotation and translation</li>
<li>It is good for local regions but problematic for whole structures</li>
</ul></li>
<li>A distance matrix is also invariant for rotaion and translation
<ul>
<li>Comparing matrices is hard, <eq env="math">\Theta(n^2)</eq></li>
<li>It is not sensitive to chirality</li>
</ul></li>
<li>At the moment, all methods are able to identify obvious similarities</li>
<li>Remote similarities are detected by a subset of methods, and different methods recognize different similarities</li>
<li>Speed is an issue in many algorithms</li>
<li>We want our method to be biologically meaningful, not only geometrically</li>
<li>The expected score or random pairwise alignments is an extreme value distribution
<ul>
<li>I would have a gaussian if there was no evolution</li>
<li>In real databases I have an excess of good-scoring pairs</li>
</ul></li>
<li>When I want to determine the distribution of scores, it is better to have an analitycal distribution than an empirical one
<ul>
<li>I don’t have tools for working with empirical distributions (!)</li>
</ul></li>
</ul>
<h2 id="ce-algorthm">CE Algorthm</h2>
<ul>
<li>Compares AFPs composed of 8 residues, stiches them together and finds an optimal path trough them with dynamic programming</li>
<li>It gives a statistical score</li>
<li>The alignment is the longest continuous path of AFPs in a similarity matrix S</li>
<li>The similarity matrix S is composed represent all AFPs conforming to a similaritt criterion</li>
<li>The dimensions of S are (na-m)(nb-m), where na and nb are the length of the sequences and m the size of the AFPs</li>
<li>The matrix is large to compute, therefore we need constraints</li>
<li>Two consecutive AFPs can be aligned with a gap in protein A, a gap in protein B or without gaps</li>
<li>The AFP lenght is set to 6 and the maximum possible gap to 30</li>
<li>Similarity measures are RMSD, full set of distances, and others</li>
<li>The best 20 alignments with Z score above 3.5 are compared based on RMSD and the best one is kept
<ul>
<li>I get an error in 1000 comparisons</li>
</ul></li>
<li>Each gap is assessed for relocation up to m/2 times</li>
<li>Iteritive optimization with dynamic programming</li>
<li>It cannot find non-topological alignments</li>
<li>The unit of comparison was originally the protein chain, but domains are optimal
<ul>
<li>Domains are difficult to define (!)</li>
</ul></li>
<li>The statistical distribution of alignment scores can be used to evaluate the Z score of an alignment</li>
</ul>
<h2 id="pdbe-fold">PDBe Fold</h2>
<ul>
<li>It uses secondarys structure elements (SSEs)</li>
<li>Secondary structure is typically conserved</li>
<li>SSE are represented as vectors that connected in a graph by edges
<ul>
<li>2 vertices and an edge describe position and orientation of the SSEs</li>
<li>SSEs are helices and strands</li>
</ul></li>
<li>Each edge is labelled by a property vector containing information on edge-vertices angles, torsion angles between vertices, lenght of the edge</li>
<li>The set of vertices, edges and labels defines the graph that is then matched with an algorthm</li>
<li>Vertex and edge lenghts are compare both in absolute and relative terms
<ul>
<li>In relative terms, the same absolute difference is less significative for longer edges</li>
</ul></li>
<li>Torsion angles are used for distinguishing mirror simmetries</li>
<li>The SSE matching gives correspondences among SSEs, and can be used to yeld an initial sequence alignment</li>
<li>Connectivity (topology) can be neglected, considered but allow for any number of missing SSEs (soft connectivity) or allow only for an equal number of unmatched SSEs (strict connectivity)</li>
</ul>
<h2 id="mammoth-algorithm">MAMMOTH Algorithm</h2>
<ul>
<li>Matching molecular models obtained from theory (MAMMOTH) is one of the fastest algorithms</li>
<li>The protein is represented as a set of unit vectors among Ca</li>
<li>It is based on dynamic programming</li>
<li>An unit vector is the normalized vector among Ca atoms
<ul>
<li>For each position, k consecutive vectors are mapped into a unit sphere that represents the local structure of k residues</li>
</ul></li>
<li>Each set of unit vectors is compare to all the sets in the other structure, building a matrix</li>
<li>Each comparison yelds a unit root mean square distance (URMS)
<ul>
<li>This is compared against the expected random URMS</li>
<li>THe alignment score is obtained by normalizing the URMS with its expected value</li>
</ul></li>
<li>The path trough the matrix is found with dynamic programming by a global alignment without end-gap penalties</li>
</ul>
<h1 id="rna-structure">RNA Structure</h1>
<ul>
<li>Most RNAs are around 50 bp</li>
<li>Secondary structure of RNAs is usually represented with parenteses
<ul>
<li>I cannot represent pseudo-knots in this way</li>
</ul></li>
<li>For RNA, the secondary strucutre is much more informative than for proteins
<ul>
<li>A certain secondary structure constraints a lot the tertiary structure</li>
</ul></li>
<li>There is less variability in RNA strucutures than in proteins</li>
<li>The best atom for representing the backbone is C3’, since it has the most constant inter-nuclotide distance</li>
<li>The professor adapted MAMMOTH to work with RNA C3’ atoms instead of Ca in proteins: SARA
<ul>
<li>The statistics of the score had to be re-evaluated</li>
<li>They still used the extreme value distribution, which is defined by <eq env="math">\mu</eq> and <eq env="math">\sigma</eq></li>
<li>They selected how the parameters change when RNA size changes</li>
<li>The set of unit vectors was 3 instead of 7</li>
<li>The method gives a -log(p-value) score</li>
<li>By comparing RNAs of known function, I can determine a score threshold that gives correct functional annotation</li>
</ul></li>
<li>Another method was developped in Israel: ARTS</li>
<li>Few people are working in RNA: not so many methods</li>
<li>The twilight zone of RNA sequence alignment is around 60%</li>
<li>Secondary structure identity (PSS) correlates well with tertiary structure identity (PSI) but not with sequence identity</li>
</ul>
<h1 id="multiple-sequence-alignment">Multiple Sequence Alignment</h1>
<ul>
<li>In MSA it is easier that in pariwise alignments to identify conserved regions, that could be functionally important
<ul>
<li>We can observe blocks of conservation in MSAs</li>
</ul></li>
<li>I can transform a MSA in a profile of the sequences</li>
<li>A profile is a matrix with a row for each possible residue and a column for each position
<ul>
<li>The value of each element reflects the frequency of a residue in a specific position</li>
<li>Each position is therefore a vector of 20 elements</li>
<li>I represent a profile as a matrix containing as many vectors as the number of positions</li>
<li>I can also have a row for the presence of a gap in the position</li>
</ul></li>
<li>A sequence logo is a plot showing the entropy of each residue in each position
<ul>
<li>It is obtained from a profile and it is a way to represent it</li>
</ul></li>
<li>Shannon entropy: information content of a message
<ul>
<li>For a single colum <eq env="math">S(p) = \sum_{i=1}^{20}-p_i \ln{p_i}</eq></li>
<li>Total conservation: <eq env="math">S(p)=0</eq></li>
<li>All residues are equally probable: <eq env="math">S(p)=ln(20)</eq></li>
<li>There are more sofisticated models that take into account the expected frequency of residues</li>
<li>The entropy of an alignment is obtained by summing the Shannon entropy over the all alignment</li>
</ul></li>
<li>Scoring an MSA: sum of pairwise scores or entropy score
<ul>
<li>Not all the position are equal in an MSA: some conservations are critical, others not</li>
<li>Scoring has necessarily to depend on the evolutionary history of the sequences</li>
<li>Almost all scoring functions assume positional independence</li>
</ul></li>
<li>I can score each pairwise alignment and sum it
<ul>
<li><eq env="math">S = \sum_{i&lt;j} S(A_i,A_j)</eq></li>
</ul></li>
<li>I can score an MSA depending on its entropy
<ul>
<li>The best alignment is the one with the lowest entropy (i.e. the most conserved one)</li>
<li>It is the sum over the alignment of the entropy in each position</li>
<li><eq env="math">S = \sum_{j=1}^{N cols}\sum_{i=1}^{20}-p_i \ln{p_i}</eq></li>
</ul></li>
<li>I can align a sequence to a profile
<ul>
<li>Each position is aligned to a vector for the position</li>
<li>The score for the position of the residue in the sequence with every possible residue is summed and weighted for the frequency encoded in the vector
<ul>
<li>This is a matrix by vector multiplication (!)</li>
</ul></li>
<li>These scores can be used with a dynamic programming algortihm</li>
</ul></li>
</ul>
<h1 id="algorithms-for-msas">Algorithms for MSAs</h1>
<ul>
<li>Dynamic programming approaches exist, but they are <eq env="math">O(N^M)</eq> and they are np-hard</li>
<li>An MSA method can be evaluated from the functionally important residues that are correctly aligned</li>
</ul>
<h2 id="progressive-msa">Progressive MSA</h2>
<ul>
<li>ClustalW is an example of progressive MSA</li>
<li>I allign sequences in pairs, one after the other</li>
<li>The result depends on the order of how I pair sequences (!)
<ul>
<li>I usually pair the most similar sequences first</li>
</ul></li>
<li>Similarity is measured by Kimura distance (see MUSCLE for more info)</li>
<li>From each pairwise alignment, I build a profile</li>
<li>I iterate until there are no sequences left, by aligning pairwise sequences and profiles</li>
<li>In order to do this I need to be able to align profiles (!)</li>
<li>I want to be conservative with gaps with the initial pairwise alignments, and introduce them later on profiles
<ul>
<li>When I get to profiles I have info about conservation (!)</li>
<li>Errors in the first alignments are propagated</li>
<li>If I am not conservative I can become full of gaps</li>
</ul></li>
<li>I can improve the alignment by changing the sequence tree
<ul>
<li>By default Clustal uses NJ</li>
<li>Maybe I have a tree available (!)</li>
</ul></li>
<li>Adding gaps is tricky, since their penalty logically depends on the position and conservation
<ul>
<li>They are usually added in the fisrt alignments</li>
</ul></li>
<li>In ClustalW the penalty is multiplied by a factor which is context specific
<ul>
<li>Gaps in hydrophobic regions are more penalised</li>
<li>These coefficients were derived from gaps frequencies in a large number of structural alignments</li>
<li>Gaps are discouraged if there is another gap nearby in the MSA</li>
</ul></li>
<li>Low-scoring alignments are postponed for later by adjusting the tree
<ul>
<li>ClustalW aligns them when it has more information deriving from the profiles</li>
</ul></li>
<li>A profile-to-profile alignments involve the pairwise comparison of same-dimentional vectors
<ul>
<li>I do a double sum all against all elements weighted with a substitution matrix</li>
<li>This is done via a simple vector to matrix multiplication, followed by a multiplicatio for the remaining vector (!)</li>
</ul></li>
<li>ClustalW corrects for biased representation of subfamilies</li>
<li>The scoring matrices used change depending on the similarity of the sequences to be compared</li>
<li>In general, ClustalW uses an heavily crafted heuristics</li>
<li>The main problem of progressive alignment: subalignments are frozen in place
<ul>
<li>Once aligned, a group of sequences cannot be re-aligned by taking advantage of the new information deriving from other sequences</li>
</ul></li>
</ul>
<h2 id="iterative-msa">Iterative MSA</h2>
<ul>
<li>Iterative MSA tries to overcome the problem of frozen subalignments</li>
<li>MUSCLE: multiple sequence comparison by log expectations</li>
<li>It is based on 3 steps: draft progressive, improved progressive, and refinement</li>
<li>Draft progressive: create a first progressive MSA
<ul>
<li>Sequence similarity is defined by k-mer distance, not pairwise alignment score
<ul>
<li>If a rare kmer is present in 2 sequences maybe they are related</li>
</ul></li>
<li>It creates a distance matrix with all sequences against each other</li>
<li>It uses UPGMA instead of NJ for building the tree from the matrix</li>
<li>The score is based on log expectations, not pairwise score for profile to profile alignments
<ul>
<li>It is the entropy score</li>
</ul></li>
</ul></li>
<li>Improved progressive: from the draft create a new matrix and from that a new tree and a new alignment
<ul>
<li>The pairwise distances are calculated from the Kimura distance
<ul>
<li><eq env="math">K_{dist} = - \ln{1-D-D^2/5}</eq>, where D is the pairwise identity</li>
</ul></li>
</ul></li>
<li>Refinement: cut and re-align the tree
<ul>
<li>I edge is deleted at random from the tree</li>
<li>The 2 resulting profiles are re-aligned to each other to get the full MSA</li>
<li>If the score improves, keep the new MSA otherwise keep the previous one</li>
<li>This is iterated until convergence on a local minimum</li>
</ul></li>
</ul>
<h2 id="consistency-based-msa">Consistency-Based MSA</h2>
<ul>
<li>Consistency: if residue X is aligned with Y and Y is aligned with Z, then X is aligned to Z
<ul>
<li>This is necessarily true in an MSA</li>
</ul></li>
<li>In reverse, I can use consistency to align two sub-alignments: I take advantage of transitivity of alignments</li>
<li>MSA are not necessarily consistent with the repsective pairwise alignments
<ul>
<li>Progressive MSA methods frequently are not consistent with the pairwise alignments used for building the tree</li>
</ul></li>
<li>T-Coffe (tree-based consistency objective function for alignment evaluation) is an MSA method based on consistency
<ul>
<li>Build the primary library
<ul>
<li>I do all the possible pairwise alignments and I measure the pairwise sequence identity</li>
<li>Each pairwise alignment is equipped with a weight equal to the average identity of matched residues, ignoring gaps</li>
</ul></li>
<li>Build the extended library
<ul>
<li>In order to align sequences A and B, I try all the possible alignment, direct and based on an intermediate sequence C</li>
<li>The weight of each alignment is the minimum of the pairwise weights for the intermediate alignments</li>
<li>The final weight of a position is the sum of the weights of all the possible alignments supporting it</li>
</ul></li>
<li>Maximise the pairwise alignments from the extended library with dynamic programming
<ul>
<li>The score of each match corresponds to its weight</li>
</ul></li>
<li>From the extended pairwise alignments, build a guide tree</li>
<li>Do a progressive MSA from this guide tree and the extended pairwise alignments</li>
<li>T-Coffe considers both global and local pairwise alignments and it can use information about domains and motifs</li>
</ul></li>
</ul>
<h1 id="msa-benchmark">MSA benchmark</h1>
<ul>
<li>BaliBASE was the first large-scale benchmark specifically designed for MSA
<ul>
<li>It is a dataset with manually refined alignments derived from structural superimposition</li>
</ul></li>
<li>BaliBASE is subdivided in several reference datasets
<ul>
<li>1 - Small number of equidistant sequences
<ul>
<li>This is further subdivided by identity levels</li>
</ul></li>
<li>2 - Families with one or more orphan sequences</li>
<li>3 - Pair of divergent subfamilies with less than 25% reciprocal identity</li>
<li>4 - Sequences with large extensions at the N or C terminal</li>
<li>5 - Sequences with large internal indels</li>
</ul></li>
<li>The evaluation of the benchmark is based on a series of scores
<ul>
<li>The scores are evaluated only for columns that are reliably aligned in the reference (core columns)</li>
<li>Sum of pairs score (SP score): proportion of correctly aligned residue pairs in the core columns</li>
<li>Total column score (TC score): proportion of completely correclty aligned core columns</li>
<li>TC and SP score both are a number between 0 and 1</li>
<li>In a pairwise alignment SP and TC score are necessarily equal</li>
<li>In an MSA with 3 or more sequences, SP &gt;= TC</li>
<li>Both scores encourage sensitivity, but they do not test for specificity
<ul>
<li>There is no penalty for wrong alignments (!)</li>
</ul></li>
</ul></li>
<li>BaliBASE also evaluates time of execution and peak memory usage</li>
<li>SP, TC, memory and time are reported as Z-scores on a spiderweb plot for each alignment tool</li>
<li>What comes out of the BaliBASE benchmark?
<ul>
<li>No single method is perfect in all cases (!)</li>
<li>On average, consistency-based methods are more accurate but slower</li>
<li>T-Coffe suffers with N and C terminal extension</li>
<li>ClustalW and MUSCLE are the least resource-heavy tools</li>
<li>T-Coffe and MAFFT are well suited for alignments larger that those in BaliBASE</li>
<li>Multi-threading can greatly speed-up these softwares, since there is a lot of parallel computing</li>
<li>Many algos take advantage of parallel processing</li>
</ul></li>
</ul>
<h1 id="probabilistic-sequence-models">Probabilistic Sequence Models</h1>
<ul>
<li>Generative definition: a model is an object producing different outcomes (sequences) from a probability distribution</li>
<li>The probability distribution in sequence space determines the specificity of the model</li>
<li>The probability for model M of generating sequence s is <eq env="math">p(s|M)</eq></li>
<li>Associative definition: a model is an object that given an outcome computes a probability value</li>
<li>Models are most useful if they are trainable systems</li>
<li>In a trainable model I can estimate the probability density function over the sequence space from a set of known sequences with a learning algorithm
<ul>
<li>If I want to model the globin family, I can train my model with sequences that are know to belong to that family</li>
<li>After training, I can use the model to compute the probability of an unknown sequence to belong to the globin family</li>
</ul></li>
<li>The model M given a sequence s returns the probability <eq env="math">p(s|M)</eq>
<ul>
<li>This is the probability of the model generating the sequence, not the probability of the sequence coming from the model</li>
</ul></li>
<li>Most times I am interested in the probability of a given sequence s to come from the model M <eq env="math">p(M|s)</eq>
<ul>
<li>This is the probability of a sequence being part of a specific family</li>
</ul></li>
<li>In order to compute <eq env="math">p(M|s)</eq> from <eq env="math">p(s|M)</eq> I need to use Bayes theorem <eq env="displaymath">p(M|s) = p(s|M)p(M)/p(s)</eq></li>
<li>The priors <eq env="math">p(M)</eq> and <eq env="math">p(s)</eq> needs to be estimated to do the conversion
<ul>
<li><eq env="math">p(M)</eq> is the a priori probability of any sequence belonging to the model
<ul>
<li>This is the relative abundance of the class, relative to all possible classes</li>
<li>It can be estimated from the abundance of the known sequences in the family</li>
</ul></li>
<li><eq env="math">p(s)</eq> is the a priori probability of the sequence and cannot be estimated reliably</li>
</ul></li>
<li>In order to avoid specifing <eq env="math">p(s)</eq> I can compare the probabilities of 2 different models
<ul>
<li>Instead of looking for <eq env="math">p(M_1|s)</eq>, I look for <eq env="math">p(M_1|s)/p(M_2|s)</eq> <eq env="displaymath">\frac{p(M_1|s)}{p(M_2|s)} = \frac{p(M_1|s)p(M_1)}{p(s)} \frac{p(s)}{p(M_2|s)p(M_2)} = \frac{p(M_1|s)p(M_1)}{p(M_2|s)p(M_2)}</eq></li>
<li>In this way, the conditional probabilities of the sequences are easy to estimate from the models themselves</li>
<li>The ratio <eq env="math">p(M_1)/p(M_2)</eq> can be estimated from the relative abundance of the 2 classes (the number of proteins in the 2 families)</li>
</ul></li>
<li>To make the calculation more standard, I can systematically compare any model to the NULL model</li>
<li>The NULL model N is a model that generates all the possible sequences with equal probabilities, only depending on the residue frequencies</li>
</ul>
<h2 id="markov-models">Markov Models</h2>
<ul>
<li>Markov Models have their most frequent application in speech recognition</li>
<li>A simple Markov Model, or Markov chain is a collection of states associated with probabilities for all the possible transitions between them</li>
<li>It is useful for modeling the probability of a sequence of states that only depend on the preciding state in the sequence</li>
<li>I can consider each residue as a state, and I can assume that its state depends only on the previous residue</li>
<li>The Markov model will contain all the possible residues and their transition probabilities</li>
<li>In this framework, the transition probability is the probability that residue B follows residue A in position <eq env="math">x_i</eq> of a sequence</li>
<li>The trasition probability <eq env="math">a_{AB}</eq> is the conditional probability of the position <eq env="math">i+1</eq> being B given that position i is A <eq env="displaymath">a_{A,B} = p(x_{i+1} = B| x_i = A)</eq></li>
<li>The probability of a sequence x of lenght n is the product of all the transition probabilities at the various positions
<ul>
<li>Here I am assuming independence of each transition <eq env="displaymath">p(x) = p(x_n|x_{n-1}) p(x_{n-1}|x_{n-2})...p(x_2|x_1) p(x_1)</eq> <eq env="displaymath">p(x) = p(x_1) \prod_{i=2}^n p(x_i|x_{i-1}) = a_{BEGIN,x_1} \prod_{i=2}^n (a_{(i-1),(i)}) a_{x_n, END}</eq></li>
</ul></li>
<li>I can also add a BEGIN and an END state to my model for avoiding irregularities
<ul>
<li>In this case the transition probability from BEGIN to a state is the probability of starting with that state</li>
<li>This is symmetrical for transitions from a state to the END state</li>
<li>We treat both BEGIN and END states as the same state 0 so <eq env="math">a_{0k}</eq> and <eq env="math">a_{j0}</eq> are transitions from BEGIN and to END
<ul>
<li>There is no ambiguity since transitions are only from BEGIN and only to END</li>
</ul></li>
</ul></li>
<li>Let’s say I want to model the probability that a given sequence is a CpG island
<ul>
<li>In such sequence, <eq env="math">a_C,G</eq> would be much higher than elsewhere</li>
<li>I can create 2 different Markov chains <eq env="math">M_+</eq> and <eq env="math">M_-</eq> for modelling the 2 sequences: CpG island and non CpG island</li>
<li>The 2 models will have the same states but different transition probabilities</li>
<li>To determine the likelihood S of a sequence x being a CpG island, i can compare the log-odds of the 2 models <eq env="displaymath">S(x) = \log{\frac{P(x|M_+)}{P(x|M_-)}} = \sum_{i=1}^n \log{\frac{a^+_{x_{i-1},x_i}}{a^-_{x_{i-1},x_i}}}</eq></li>
</ul></li>
<li>In a Markov model, the sum of probabilities going out of a state is always 1
<ul>
<li>It is certain that I will go out of the state</li>
</ul></li>
<li>When I have only 2 possible mutually exclusive models, I can have a measure for <eq env="math">p(s)</eq> <eq env="displaymath">p(s|M_1) + p(s|M_2) = p(s) \iff p(M_1) \cap p(M_2) = \emptyset</eq></li>
<li>From this, I can recover <eq env="math">p(M_1|s)</eq> and <eq env="math">p(M_2|s)</eq> <eq env="displaymath">p(M_1|s) = \frac{p(s|M_1)p(M_1)}{p(s)} = \frac{p(s|M_1)p(M_1)}{p(s|M_1) + p(s|M_2)}</eq> <eq env="displaymath">p(M_2|s) = \frac{p(s|M_2)p(M_2)}{p(s)} = \frac{p(s|M_2)p(M_2)}{p(s|M_1) + p(s|M_2)}</eq></li>
<li>We always work with Markov models of order 1: every state depends only on the previous 1 state
<ul>
<li>There are also MM of order 0 or greater than 1</li>
</ul></li>
<li>A Markov Model like the ones described is parametric: It can be completely described by a set of parameters <eq env="math">\theta_M</eq> (the transition probabilities)
<ul>
<li>Training the model means finding the optimal parameters</li>
<li>The parameters for a model can be estimated from a set of training data</li>
</ul></li>
<li>For any sequence <eq env="math">s</eq> and model <eq env="math">M</eq>, I can express <eq env="math">p(s|M)</eq> as the Markov chain that can produce <eq env="math">s</eq> <eq env="displaymath">p(s|M) = \prod_{j = 0}^{n+1} \prod_{k = 0}^{n+1} a_{jk}^{n^{jk}}</eq>
<ul>
<li>In this representation 0 is the BEGIN state and n+1 the END state</li>
<li>The probability is the product of all the possible transition probabilities <eq env="math">a_{jk}</eq> to the power of how many times they do occur <eq env="math">n_{jk}</eq></li>
<li>The probability of the sequence given the model is the joint probability of all the possible paths that generate it</li>
</ul></li>
<li>The model is always under the normalization constraint <eq env="displaymath">\forall j \: \sum_{k=1}^{n+1} a_{jk} = 1</eq>
<ul>
<li>The sum of outgoing transitions from any state must sum up to 1</li>
</ul></li>
<li>Maximum likelihood estimation: the value of a parameter <eq env="math">\theta_M</eq> is the one that maximises the probability of the dataset <eq env="math">D</eq> given the model and the parameter itself (given the model that uses that parameter) <eq env="displaymath">\theta_{ML} = argmax_\theta P(D|M, \theta)</eq>
<ul>
<li>The solution for any parameter <eq env="math">\theta</eq> can be obtained <eq env="displaymath">\theta = a_{ik} = \frac{n_{ik}}{\sum_j n_{ij}}</eq>
<ul>
<li>The optimal value of the parameter is the frequence of occurrence of the transition in the dataset</li>
<li>The normalization constraint forces to divide the count of transitions for the total number of outgoing transitions</li>
</ul></li>
</ul></li>
<li>Maximum a posteriori estimation: the Bayesian correction of the ML approach
<ul>
<li><eq env="math">\theta_{MAP} = argmax_\theta (p(\theta|M, D))</eq></li>
<li><eq env="math">p(\theta|M, D) = p(D| M, \theta)p(\theta)</eq></li>
</ul></li>
</ul>
<h2 id="hidden-markov-models">Hidden Markov Models</h2>
<ul>
<li>Let’s now try to model the presence of a CpG island in a larger sequence
<ul>
<li>I can integrate both models <eq env="math">M_+</eq> and <eq env="math">M_-</eq> in a single model</li>
<li>I will have 2 states for each nucleotide, one for each model</li>
<li>The transition probabilities inside states of the + and - models will be similar to before</li>
<li>In addition I will have a small probability of going from a state of one model to any state of the other model</li>
<li>It will be more probable to go from - to + than vice versa
<ul>
<li>This means that I will be most of the time in -, so most of the sequence is not a CpG island</li>
</ul></li>
</ul></li>
<li>This is an Hidden Markov Model since for every position the sequence itself I cannot no which state generated it
<ul>
<li>For each possible nucleotide I have 2 states, and I do not know which one it came from</li>
</ul></li>
<li>Differently from Markov chains, in HMM we need to distinguish the sequence of states from the sequence of observables (symbols here)</li>
<li>The sequence of states, which is hidden to us, is called the path <eq env="math">\pi</eq> and it is a simple Markov chain
<ul>
<li>The path has transition probabilities <eq env="math">a_{jk} = p(\pi_i = k | \pi_{i-1} = j)</eq></li>
</ul></li>
<li>A symbol can correspond to multiple states but also a state can generate different symbols (!)
<ul>
<li>In general, the outcome of a single state derives from a probability distribution</li>
<li>We define the emission probability of symbol b from state k as <eq env="math">e_k(b) = p(x_i = b|\pi_i = k)</eq></li>
<li>The sum of emission probabilities from a state is always 1, so the state always produces something</li>
</ul></li>
<li>Formal definition of HMM: a stochastic generator of sequences characterized by
<ul>
<li><eq env="math">N</eq> states</li>
<li>A set of transition probabilities <eq env="math">a_{kj}</eq> between states</li>
<li>A set of starting probabilities <eq env="math">a_{0k}</eq> and ending probabilities <eq env="math">a_{j0}</eq></li>
<li>An alphabet <eq env="math">C</eq> containing <eq env="math">M</eq> characters</li>
<li>A set of emission probabilities for each state <eq env="math">\{e_k(c \in C)\}</eq></li>
<li>The constraints <eq env="math">\sum_k a_0k = a_{k0} + \sum_j a_{kj} = \sum_{c \in C} e_k(c) = 1</eq></li>
</ul></li>
<li>The topology and probabilities of transitions define a global grammar</li>
<li>The emission probabilities cast the propensity for observable symbols in each state</li>
<li>I can write the probability of observing the sequence x of lenght L under the path <eq env="math">\pi</eq> <eq env="displaymath">p(x,\pi) = a_{0\pi_1} \prod_{i=1}^L e_{\pi_i}(x_i) a_{\pi_i \pi_{i+1}}</eq>
<ul>
<li>In this equation <eq env="math">\pi_{L+1} = 0</eq>, so the last transtition is to the END state</li>
<li>The probability of a carachter <eq env="math">x_i</eq> being generated by the model is the product of the probability deriving from the markov chain and the emission probability for that carachter under the current state</li>
<li>The path is typically hidden, so this equation is not useful in practice</li>
</ul></li>
<li>The probability of the above equation under a model M can be rewritten as <eq env="math">p(x,\pi|M)</eq>
<ul>
<li>This can be decomposed as <eq env="math">p(x,\pi|M)=p(x|\pi,M)*p(\pi|M)</eq></li>
</ul></li>
<li>Naive approach: if I want to obtain <eq env="math">p(x|M)</eq> under an HMM I need to sum over all the possible paths <eq env="displaymath">p(x|M) = \sum_\pi p(x,\pi|M)</eq>
<ul>
<li>The number of possible paths is the number of states elevated to the lenght of the sequence
<ul>
<li>No way you can do that brute force</li>
</ul></li>
<li>The time complexity is <eq env="math">O(tn^t)</eq> where n is sequence lenght an t the number of different symbols
<ul>
<li>Danger! NP-hard!</li>
</ul></li>
</ul></li>
<li>There are different algorithms for computing <eq env="math">p(x|M)</eq> under an HMM
<ul>
<li>In general, my aim is to decode the path from the sequence, so that I can assess the true probability</li>
</ul></li>
<li>Viterbi algorithm: dynamic programming for finding the most probable path
<ul>
<li>If I need to choose just 1 path the most probable one is the most logical choice</li>
<li>Let’s define the most probable path <eq env="math">\pi^* = argmax_\pi(p(x,\pi))</eq></li>
<li>I can find <eq env="math">\pi^*</eq> recursively
<ul>
<li>I suppose that the probability of <eq env="math">\pi^*</eq> having state k in position i is <eq env="math">v_k(i)</eq> and it is known for all the states k
<ul>
<li>This means that I know the probability of each state in each position of the most probable path</li>
</ul></li>
<li>I can calculate recursively the probability of state l for position i+1 <eq env="displaymath">v_l(i+1)=e_l(x_{i+1})* max_k(v_k(i)a_{kl})</eq>
<ul>
<li>The first term is the emission probability of the state l for the observed symbol <eq env="math">x_{i+1}</eq></li>
<li>The second term is the probability of having state k in position i times the transition probability from k to l</li>
<li>In the second term I take the max in k, so I choose the k that maximises the quantity</li>
<li>The problem then recurses in calculating <eq env="math">v_k(i)</eq> and so on</li>
</ul></li>
<li>All sequences need to start at some point: the recursion ends in <eq env="math">v_0(0)=1</eq>
<ul>
<li>It is certain that the beginning of the sequence comes from state 0</li>
</ul></li>
</ul></li>
<li>Given this framework, I can create a dynamic programming matrix that finds the optimal path
<ul>
<li>Initialization <eq env="displaymath">i = 0,\; v_0(0) = 1,\; v_k(0) = 0 \quad \mbox{for}\; k &gt; 0</eq></li>
<li>Recursion with i = 1 to L (lenght of sequence) <eq env="displaymath">v_l(i) = e_l(x_i) * max_k(v_k(i-1)a_{kl})</eq> <eq env="displaymath">pointer_l(i) = argmax_k(v_k(i-1)a_{kl})</eq></li>
<li>Termination <eq env="displaymath">p(x, \pi^*)=max_k(v_k(L)a_{k0})</eq> <eq env="displaymath">\pi^*(L) = argmax_k(v_k(L)a_{k0})</eq></li>
<li>Traceback with i = L downto 1 <eq env="displaymath">\pi^*(i-1) = pointer_i(\pi^*_i)</eq></li>
</ul></li>
<li>The probabilities obtained with the Viterbi algorithm are really small and give underflow errors
<ul>
<li>It is better to operate in log space
<ul>
<li>I use <eq env="math">\log{v_l(i)}</eq></li>
</ul></li>
<li>This makes also the products become sums</li>
</ul></li>
</ul></li>
<li>Forward algorithm: why only the most probable path, if I can have all of them?
<ul>
<li>Using only <eq env="math">\pi^*</eq> as in the Viterbi algotihm is a huge approximation, but it works surprisingly well</li>
<li>Actually we don’t need to do so, since we can calculate the complete probability of x for all paths</li>
<li>We can just replace the maximizations of the Viterbi algorithm with sums</li>
<li>We can define <eq env="math">f_k(i)</eq> as the forward parallel of the Viterbi quantity <eq env="math">v_k(i)</eq>
<ul>
<li>It is the probability of state k in position i under the forward algorithm</li>
</ul></li>
<li>The probability of state k in position i is the joint probability of the sequence up to position i and the fact that the current state is k <eq env="displaymath">f_k(i) = p(x1..x_i, \pi_i = k)</eq></li>
<li>The recursion equation is therefore <eq env="displaymath">f_l(i+1)=e_l(x_{i+1})* \sum_{k=0}^i(f_k(i)a_{kl})</eq></li>
<li>Like the Viterbi approach, the forward algorithm can give underflow errors
<ul>
<li>I can correct by operating in log space or scaling the probabilities</li>
<li>In log space the math is not as clean as with the Viterbi</li>
</ul></li>
<li>The time complexity is <eq env="math">O(tn^2)</eq></li>
</ul></li>
<li>Backward algorithm: some as forward, but starting from the end
<ul>
<li>The quantities that I consider here is <eq env="math">b_k(i)</eq></li>
<li>It is the probability of state k in position i</li>
<li>I don’t use the backword algorithm for calculating <eq env="math">p(x)</eq>, since usually I get it with the forward algorithm</li>
</ul></li>
<li>The backward algorithm is useful for calculating posterior probabilities</li>
<li>What I am really interested in is not <eq env="math">p(x, \pi_i = k|M)</eq>, but <eq env="math">p(\pi = k|x, M)</eq>
<ul>
<li>I want to now the probability of the hidden state k being at work given the sequence</li>
<li><eq env="math">p(x,\pi_i = k)</eq> can be decomposed as <eq env="math">f_k(i)b_k(i)</eq>
<ul>
<li>This is the joint probability of having <eq env="math">\pi_i = k</eq> when the sequence up to i and from i to the end is equal to the respective portions of x</li>
</ul></li>
<li>From this I can get the posterior probabilities <eq env="displaymath">p(\pi = k|x) = p(x, \pi_i = k)/p(x) = f_k(i)b_k(i)/ p(x)</eq>
<ul>
<li><eq env="math">p(x)</eq> is the result of the forward algorithm here (!)</li>
</ul></li>
</ul></li>
<li>A posteriori decoding: when choosing the most probable path is not justified
<ul>
<li>In some situations just choosing the most probable path (Viterbi decoding) is not legitimate
<ul>
<li>I can have many paths with similar probabilities, and a posteriori decoding evaluates all of their contributions for any state</li>
</ul></li>
<li>For position i the a posteriori estimate for state <eq env="math">\pi_i</eq> is <eq env="math">\hat{\pi_i}</eq>, as compared to <eq env="math">\pi^*_i</eq> of Viterbi decoding <eq env="displaymath">\hat{\pi_i}=argmax_k(p(\pi_i = k|x))</eq>
<ul>
<li>This probability includes all the possible paths that can bring me in position i at state k</li>
</ul></li>
<li>This definition is not very useful for determining the path, I can only decode a single state</li>
<li>If I use this equation for the whole path, it can be non-sensical!
<ul>
<li>It could include forbidden transitions</li>
<li>It can give the most probable state in position <eq env="math">i</eq> given a path, and then a state in position <eq env="math">i+1</eq> given a different path, when the transition from the 2 is actually impossible</li>
</ul></li>
</ul></li>
</ul>
<h2 id="hmms-for-sequence-alignments">HMMs for Sequence Alignments</h2>
<ul>
<li>I can see a gapped alignment as a finite state automaton (FSA) with a match state M, and two state for the respective insertions, X and Y
<ul>
<li>In this FSA I have score changes at every state transition</li>
</ul></li>
<li>I can similarly create a probabilistic HMM with the same states
<ul>
<li>It does not emit a sequence, but an alignment (!)</li>
<li>It is called pair HMM</li>
</ul></li>
<li>The pair HMM has the following properties
<ul>
<li>The state M has emission probabilities for all possible matches <eq env="math">x_i:y_i</eq></li>
<li>State X has emission probabilities for all possible single charachter insertions in X (or delitions in Y) such as <eq env="math">x_i:gap</eq></li>
<li>State Y has emission probabilities for all possible single charachter insertions in Y (or delitions in X) such as <eq env="math">y_i:gap</eq></li>
<li>We introduce a BEGIN and END state</li>
<li>The transition probabilities are called
<ul>
<li><eq env="math">\delta</eq> for M-&gt;X,Y</li>
<li><eq env="math">\epsilon</eq> for Y-&gt;Y and X-&gt;X</li>
<li><eq env="math">\theta</eq> for M,Y,X -&gt; END</li>
<li>The other probabilities can be derived from the complements to 1</li>
</ul></li>
<li>Every position in the pair HMM has two indexes intead of 1</li>
</ul></li>
<li>The Viterbi path of the pair HMM is the optimal FSA alignment (!)
<ul>
<li>It is the one I would recover from the NW algorithm</li>
</ul></li>
</ul>
<h2 id="profile-hmms">Profile HMMs</h2>
<ul>
<li>Profile HMMs are the most important application of HMMs to bioinformatics
<ul>
<li>They were first done by Krogh (the one from Denmark)</li>
</ul></li>
<li>In the ungapped case, I just want to model the propensity of the position for a symbol
<ul>
<li>The profile HMM will have for each state M emission probabilities deriving from the profile vector at that position</li>
</ul></li>
<li>Introducing gaps, we see how their penalty shouldn’t be the same across the alignment</li>
<li>I can introduce the insert (I) state for modelling insertions in my sequence with respecty to its family profile</li>
<li>The I state has emission probabilities deriving from background distribution
<ul>
<li>We need a transition from <eq env="math">M_i</eq> to <eq env="math">I_i</eq>, a loop from <eq env="math">I_i</eq> to itself and a transition from <eq env="math">I_i</eq> to <eq env="math">M_{i+1}</eq></li>
<li>This model is essentially an affine gap</li>
</ul></li>
<li>The deletion could be modelled by a series of transitions from <eq env="math">M_i</eq> to all <eq env="math">M_{i+k}</eq>, all the subsequent states
<ul>
<li>This requires n(n-1)/2 transitions, and I need probabilities for all of them</li>
</ul></li>
<li>To avoid this I insert silent states D for modelling deletions
<ul>
<li>A silent state is a state that does not emit any symbol</li>
<li>Now I have transitions from <eq env="math">M_i</eq> to <eq env="math">D_{i+1}</eq> and from <eq env="math">D_{i+1}</eq> to <eq env="math">D_{i+k}</eq>
<ul>
<li>I can enter in a delete state (and not add anything to my sequence) and continue there</li>
</ul></li>
<li>I have also transitions <eq env="math">D_i</eq> to <eq env="math">M_{i+1}</eq> for modelling when the deletion ends</li>
<li>In this way I have 4n-8 parameters</li>
</ul></li>
<li>This delete model has different transitions in different positions, so I can include a position-specific gap penalty!</li>
<li>In my insert model I cannot do this, since I have loops for long insertions in the same I state
<ul>
<li>This makes sense since The insertion only matters where it starts and how long it is</li>
<li>For deletions it matters which residues are missing from the family profile!</li>
</ul></li>
<li>As a final refinement, I can include transitions between delete and insert states
<ul>
<li>They are quite unlikely and usually they do not affect much the alignment</li>
<li>These are <eq env="math">D_i</eq> to <eq env="math">I_i</eq> and <eq env="math">I_i</eq> to <eq env="math">D_{i+1}</eq></li>
</ul></li>
<li>Any profile HMM is able to produce any possible seuqence in sequence space</li>
<li>Parameterising a profile HMM means to make the probability distribution of the produced sequences peak around members of the modelled family
<ul>
<li>We can play with transition and emission probabilities, and with the lenght of the model itself</li>
</ul></li>
<li>Modelling the lenght of the model means to decide which MSA columns to assign to match states, and which to insert states
<ul>
<li>A heuristic rule is to assign to insert states the columns that have more than half gaps</li>
</ul></li>
<li>Probabilities can be estimated from the transition and emission frequencies of the sequences in the MSA
<ul>
<li>For this to be meaningful, I need a big training set</li>
<li>There can be transitions or emissions with 0 probability due to sampling limitations
<ul>
<li>We can add pseudocounts for coping with this</li>
</ul></li>
</ul></li>
<li>I can evaluate the score of an alignment to a profile with the profile HMM
<ul>
<li>I can calculate <eq env="math">p(x,\pi^*|M)</eq> with Viterbi or <eq env="math">p(x|M)</eq> with the forward algorithm</li>
</ul></li>
<li>I can use the log-likelyhood <eq env="math">LL = -\log{p(x|M)}</eq> as the alignment score
<ul>
<li>It is strongly lenght dependent and in a not linear fashion (!)</li>
<li>I can normalize it obtaining a Z-score</li>
<li>For the normalization I need a <eq env="math">\mu</eq> and a <eq env="math">\sigma</eq> for the length-dependent score distribution</li>
</ul></li>
<li>I can also use the log-odds against the NULL model
<ul>
<li>This has usually a 3 times better signal-to-noise ratio in discriminating families</li>
<li>The NULL model is obtained from the residue composition of the training set</li>
</ul></li>
<li>The accuracy of a prediction can be evaluated on the confusion matrix
<ul>
<li>It is a simple 2*2 matrix that relates true and false positives and negatives</li>
<li>The variables under consideration are the true condition and the test outcome</li>
</ul></li>
<li>The accuracy ACC can be evaluated from the confusion matrix
<ul>
<li><eq env="math">ACC = (TP+TN)/(TP+TN+FP+FN)</eq></li>
<li>This measure can be biased if the classes (positive and negatives or prediction 1 and prediction 2) are highly unbalanced</li>
</ul></li>
<li>A better approach is to evaluate sensitivity (True positive rate , TPR) and specificity (Positive predicted value, PPV)
<ul>
<li><eq env="math">TPR = TP/(TP+FN)</eq></li>
<li><eq env="math">PPR = TP/(TP+FP)</eq></li>
</ul></li>
<li>The Matthews correlation coefficient (MCC) is the analogous of Pearson for categorical predictions
<ul>
<li><eq env="math">MCC = \frac{(TP*TN)-(FP*FN)}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}</eq></li>
<li>It measures the correlation among predictions and real classes</li>
<li>It is not affected by class unbalance</li>
</ul></li>
<li>The ROC curve (receiver operating carachteristics) is a plot of FPR (false positive rate) against TPR (true positive rate) when a parameter varies
<ul>
<li>This can be a tuning parameter</li>
<li>The area under the ROC curve is 0.5 for random predictions ad 1 for a perfect predicition</li>
</ul></li>
</ul>
<h2 id="hmmer">HMMER</h2>
<ul>
<li>HMMER is a widely used tool for creation of HMMs from MSAs</li>
<li>It is used for searching a sequence database for homologs and for making sequence alignments</li>
<li>It is design for detecting remote homologs relying in the strenght of its probabilistic models</li>
<li>The last version of HMMER is as fast as BLAST</li>
<li>Its HHMs are based on a domain structure
<ul>
<li>They present a domain chain defined by M, I and D states</li>
<li>The domain chain is connected by 2 states that model the C and N terminal regions of the domain</li>
<li>A J state models inter-domain regions</li>
</ul></li>
<li>The score of a sequence is calculated in the Bayesian frame against the NULL model</li>
<li>The NULL model takes into consideration only background frequencies of aminoacids
<ul>
<li>By default the frequencies in Swiss-Prot are used</li>
</ul></li>
<li>The Log-odds score is actually a bit score (it uses a <eq env="math">log_2</eq>)</li>
<li>HMMER takes the trained model and scores 200 randomly generated sequences with it in the Bayesian framework
<ul>
<li>I do not use an analytical estimate of the p-value, but I evaluate it from bootstraps!</li>
<li>The Log odd of the sequences is fitted with a Gumbell distribution</li>
<li>This allows to estimate the distribution parameters <eq env="math">\mu</eq> and <eq env="math">\lambda</eq></li>
</ul></li>
<li>Once I have the score distribution of random sequences, I can get the p-value of a sequence that I score against the model <eq env="displaymath">p(s&gt;t) = 1 - exp(-e^{-\lambda(t - \mu)})</eq></li>
<li>From the p-value I can obtain the e-value, the number of random sequences expected to have score greater than <eq env="math">t</eq>
<ul>
<li><eq env="math">p = 1-e^{-E}</eq></li>
<li>E is the average number of rare events, modelled under the Poisson distribution</li>
</ul></li>
<li>The default input format is the textual Stockholm format, but it accepts many common formats
<ul>
<li>The header is <code># STOCKHOLM 1.0</code> were 1.0 is the version used</li>
<li>It has then one row per sequence in the MSA, wrapped in a readable way</li>
<li>Between sucsessive wrappings of the MSA there is an empty line</li>
<li>Gaps are rendered with <code>.</code></li>
<li>The last row contains always only <code>//</code></li>
<li>It can include also info on secondary structure and can indicate to mask some carachters</li>
<li>These information are rendered with a new line in the MSA that has a peculiar name</li>
<li><code>#=GC SS_cons</code> is used to store information on the consensus secondary structure</li>
<li><code>#=GC RF</code> stores the reference annotation (the consensus sequence)
<ul>
<li>Upper and lower case can be used to denote the conservation level</li>
<li><code>~</code> indicates an unaligned insertion</li>
</ul></li>
<li><code>#=GC MM</code> specifies a model mask, which columns of the MSA should be ignored
<ul>
<li>Ignored columns are assigned a match state with background distribution</li>
</ul></li>
</ul></li>
<li>It is composed of several tools
<ul>
<li><code>hmmbuild</code> takes an msa and gives an HMM</li>
<li><code>hmmemit</code> generates a sample sequence from the HMM</li>
<li><code>hmmsearch</code> searches a database with the HMM returning a list ordered by e-value</li>
<li><code>hmmalign</code> alignes sequences to a given model</li>
<li><code>hmmpress</code> converts one or more concatenated HMMs in binary for subsequent searches</li>
<li><code>hmmscan</code> uses a binary HMM library to scan a sequence
<ul>
<li>It finds which model best matches the sequence</li>
</ul></li>
</ul></li>
<li>The HMMER profile HMM text file is the output of the <code>hmmbuild</code> command
<ul>
<li>It can also contain more than 1 profile separated by a row with <code>//</code></li>
<li>It has an header with miscellaneous key:value pairs for each profile</li>
<li>The header ends where a row starts with the keyword <code>HMM</code></li>
<li>Follows a table of <eq env="math">-log(p-vals)</eq> (<code>*</code> is used when <eq env="math">p-val = 0</eq> and so the <eq env="math">log</eq> is not defined)</li>
<li>An optional <code>COMPO</code> line follows, with the backgroung emission probabilities used for the NULL model</li>
<li>The next 2 lines code information on the BEGIN state
<ul>
<li>The first line codes the INSERT emission probabilities for the 0 state</li>
<li>The second line codes the TRANSITION probabilities for state 0
<ul>
<li>These are <eq env="math">m \to m, m \to i, m \to d, i \to m, i \to i, d \to m, d \to d</eq></li>
</ul></li>
</ul></li>
<li>It contains then 3 rows for each position of the profile
<ul>
<li>The first row contains the 20 emission probabilities for each aminoacid in that position</li>
<li>The second row contains the Insert emission probabilities for that position</li>
<li>The third row contains transition probabilities <eq env="math">m \to m, m \to i, m \to d, i \to m, i \to i, d \to m, d \to d</eq></li>
</ul></li>
</ul></li>
<li>The <code>hmmsearch</code> output is a text file returning a list of sequences that passed the threshold, set of scores for each sequence, and an alignment of each sequence with the profile
<ul>
<li>The header contains a series of key:value pairs
<ul>
<li>MSV is the multi-segment Viterbi filter</li>
<li>The expected counts for the sequences that pass each type of filter should be much lower than the actual counts</li>
</ul></li>
<li>For each sequence several scores are shown in the tabular section
<ul>
<li>The following scores are differentiated as global and best domain scores: E-value, bit-score, bias
<ul>
<li>The bias is a correction factor that should be much smaller than the bit-score</li>
<li>An high bias indicates a biased composition or repetitive sequence</li>
<li>A significant global domain E-value and not significant best-domain E-value may indicate a remote multi-domain homolog with weak similarities that add up on the whole sequence</li>
</ul></li>
</ul></li>
<li>In the alignment part there are 3 rows per sequence
<ul>
<li>The first is the profile row and it codes the preferred symbolin each position
<ul>
<li>Conserved positions are represented in capital letters</li>
</ul></li>
<li>The midline shows the symbol of the position in case of conservation, or <code>+</code> in case of a substitution that has a positive score (conservative substitution)
<ul>
<li>A space is shown when a non-conservative substitution is present</li>
</ul></li>
<li>The bottom line codes the posterior probability for each residue
<ul>
<li><code>*</code> indicates 95-100% probability, and number from 0 to 9 indicate lower probabilities</li>
</ul></li>
<li>Other scores and parameters are detailed in this section
<ul>
<li>The c-Evalue is the conditional E-value: the significance of the domain given that I know the sequences are homologs</li>
<li>The i-Evalue is the independent E-value, the independent significance of the best domain</li>
<li>The envelope (<code>envfrom</code> and <code>envto</code>) is the best aaligned portion</li>
<li><code>acc</code> is the a posteriori mean per residue probability</li>
</ul></li>
</ul></li>
</ul></li>
<li><code>hmmalign</code> produces an MSA in STOCKHOLM format
<ul>
<li></li>
</ul></li>
</ul>
<h1 id="kunitz-domain-project">Kunitz Domain Project</h1>
<ul>
<li>We want to produce a profile HMM of the Kunitz protease inhibitor domain</li>
<li>The Kuniz domain is a protease inhibitor
<ul>
<li>BPTI, APP (Alzheimer) and TFPI (Tissue factor inhibitor) have this domain</li>
</ul></li>
<li>BPTI is an inhibitor of trypsin that blocks the activation of trypsinogen in the pancreas due to spontaneous clevage
<ul>
<li>It is also found in the bovine lung</li>
</ul></li>
<li>Aprotinin (the protein BPTI) is a drug that act as an anti-fibrinolitic, it is used for reducing bleeding during surgery</li>
<li>Apoprotin is a monomeric globular polypeptide obtained from bovine lung tisssue
<ul>
<li>Its molecular mass is 6512 Da and it is composed of 58 residues</li>
<li>It folds into a compact tertiary structure of the small SS-rich type</li>
<li>It contains 3 disulfide bridges, a twisted <eq env="math">\beta</eq>-hairpin and a C-terminal <eq env="math">\alpha</eq> helix</li>
<li>It contains 10 positive residues and only 4 negative residues: it is strongly basic</li>
<li>The stability of the protein is due to the 5:55, 14:38, 30:51 disulfide bridges</li>
<li>Lys15 in the exposed loop binds tightly to the specificity pocket of trypsin</li>
<li>BPTI is synthesized as a longer precursor that is then cleaved to the final form</li>
</ul></li>
<li>The domain is included in single-domain and multiple-domain proteins</li>
<li>It is preferred to build HMMs with single-domain proteins if available in enough number, since other domains nearby could influence the structure of my domain</li>
<li>We want to build our HMM of the Kunitz domain starting from structural information and use the model for annotating SwissProt</li>
<li>The Kunitz domain is exclusive of metazoa with a single exception: <em>Amsacta moorei entomopoxvirus</em></li>
</ul>
<h2 id="retriving-the-structures">Retriving the Structures</h2>
<ul>
<li>Several options available
<ul>
<li>We can use PDBfold searching with a prototype structure</li>
<li>We can search on CATH</li>
</ul></li>
<li>Potential problems
<ul>
<li>PDBs with multiple chains</li>
<li>Chains with multiple domains</li>
<li>Redundant PDB structures: I can get more copies of the same protein</li>
<li>Mutated proteins for experimental reasons</li>
<li>Variable resolution</li>
</ul></li>
<li>Possible solutions
<ul>
<li>Accept only structures with resolution above 2.5-3 <eq env="math">\AA</eq>
<ul>
<li>Interaction and <eq env="math">C\alpha-C\alpha</eq> distances are on this range, while H bonds are around 3 <eq env="math">\AA</eq> long</li>
</ul></li>
</ul></li>
<li>I select a set of seed proteins and I refine them on the basis of the structural alignment</li>
<li>I convert the alignment in Stockholm format with Jalview or with a script</li>
</ul>
<h2 id="creating-and-testing-the-model">Creating and Testing the Model</h2>
<ul>
<li>I use <code>hmmbuild</code> to get the model from the alignment</li>
<li>I scan the original seed set wtih my model with <code>hmmscan</code> to see If I get a significant E-value</li>
<li>I do a real benchmark with non-seed proteins that I know to have the Kunitz domain
<ul>
<li>Since SwissProt is manually annotated, I can define a subset containing all the Kunitz-containing proteins</li>
<li>I also need a set of negatives to test the specificity of the model</li>
</ul></li>
<li>I need to define the confusion matrix for my model</li>
<li>I can use the Matthews correlation to give a quality score to my model</li>
<li>I can plot a ROC curve</li>
<li>The Kunitz domain is really well defined and annotated, so I expect few false negatives and positives
<ul>
<li>I can manually review all of them and understand where the model failed</li>
</ul></li>
<li>It is important to understand the limitations of my method!</li>
</ul>
<h2 id="project-report">Project Report</h2>
<ul>
<li>To be done following the structure of the Oxford Bioinformatics journal</li>
<li>I should put an abstract with information about the problem and summary of the results
<ul>
<li>Should be short!</li>
</ul></li>
<li>Introduction should contain info on the domain, previous work, ecc.
<ul>
<li>It will contain most of the citation</li>
<li>It should describe the state of the art</li>
</ul></li>
<li>Materials and methods: in bioinformatics mostly only methods
<ul>
<li>Defines the reproducibility of the work</li>
<li>I should describe procedures, algorithms, validation</li>
<li>I should describe the dataset</li>
</ul></li>
<li>Results: what I did with the methods and what I got out of it</li>
<li>I should include subsections in all parts
<ul>
<li>People can be interested only in some aspects!</li>
</ul></li>
<li>Conclusion/discussion: summarise, discuss the limitations and future directions</li>
<li>I should start from materials and methods and results</li>
<li>At the end I write conclusion and introduction</li>
<li>Reference: always to be included</li>
<li>Supplementary materials can be included</li>
<li>Note: there will still be a final exam</li>
<li>Deadline: May 18 and then 3 weeks after it for the final version</li>
<li>To be sent by mail at emidio.capriotti@unibo.it with object should be “project lb1b - Saul Pierotti”</li>
</ul>
<h1 id="predicting-structural-and-functional-fetures-from-the-sequence">Predicting Structural and Functional Fetures from the Sequence</h1>
<ul>
<li>The computational approach used for predicting structural features from protein sequence is divided in 3 main branches, depending on the availability of suitable templates: homology (comparative) modelling, fold recognition (threading), and <em>de novo</em> (<em>ab initio</em>) prediction</li>
<li>Comparative modelling requires the availability of a template with more than 30% sequence identity</li>
<li>The use of multiple sequence alignments and HMMs allow to extend the use of comparative modelling to remote homologs</li>
<li>Nonetheless, sometimes comparative modelling fails
<ul>
<li>If no suitable templates exist in the PDB <em>Ab initio</em> methods are required</li>
<li>If there are suitable templates in the PDB but they cannot be recognised, threading/fold recognition can be adopted</li>
</ul></li>
</ul>
<h2 id="ab-initio-structural-predictions"><em>Ab initio</em> Structural Predictions</h2>
<ul>
<li>It is used when there are no structures in the PDB that can be used as templates in modelling my sequence</li>
<li><em>Ab initio</em> predictions are difficult because the conformational space is huge</li>
<li>The goal is to predict the structure using only the sequence</li>
<li>This methods are based on the assumption that the native structure is the one with lowest energy</li>
<li>The CASP (critical assessment of structural predictions) evaluation is a contest from the UC Davis aimed at assessing the performances of protein structural prediction methods
<ul>
<li>It has different sections for <em>ab initio</em> predictions, comparative modelling</li>
</ul></li>
<li><em>Ab initio</em> methods are generally based on molecular dynamics (MD) force fields</li>
<li>CHARMM is the force field that is traditionally used for proteins
<ul>
<li>It was developped by the Nobel prize winner Martin Carplus at Harvard</li>
<li>It uses bonding, angle, dihedral, and non-bonding energy terms</li>
<li>Bonding energy refers to the oscillations around the optimal bond lenght</li>
<li>Angle refers to the angle of 2 bonds among 3 atoms</li>
<li>Dihedral referso to the torsion of a central bond among 3 bonds and 4 atoms</li>
<li>Non-bonding contributions are electrostaic and Lenard-Jones energies</li>
</ul></li>
<li>Molecular dynamics require a lot of computing power and therefore approaches using it are confined to small proteins
<ul>
<li>MD cannot take into account chaperon activity</li>
<li>It is not trivial to define criteria for a successfull model</li>
</ul></li>
<li><em>Ab initio</em> is computationally very hard for sequences longer than 150 aa
<ul>
<li>AlphaFold from Google DeepMind, the winner of CASP13, seems to be able to predict also longer proteins</li>
</ul></li>
<li>Rosetta (David Becker) is the most accurate fragment-based prediction methods according to CASP8
<ul>
<li>It is based on short fragments that are used as building blocks of the global fold</li>
<li>An energy landscape is built for these fragments</li>
<li>All proteins in the PDB were collected and fragmented in blocks of 5 residues, and similar fragments were clustered based on RMSD</li>
<li>A sequence profile was built from each cluster of fragments</li>
<li>When modelling a new sequence, the query was broken in fragments and aligned to the profiles</li>
<li>Some heuristcs were used for choosing the best match</li>
<li>The structure was then refined with molecular dynamics</li>
</ul></li>
</ul>
<h2 id="threading">Threading</h2>
<ul>
<li>Threading is used for sequences that have a suitable template in the PDB but we cannot find it based on sequence alone
<ul>
<li>Proteins with different sequences can have similar structures!</li>
<li>A sequence of unknown structure is threaded onto a known structure and the goodness of fit is evaluated acconrding to a scoring function</li>
</ul></li>
<li>Anna Tramontano was a researcher involved in evaluating the CASP challenge that died 2 years ago</li>
<li>Threading is a generalization of comparative modelling
<ul>
<li>I align the target sequence not to another sequence (comparative modelling) but to structure templates and I evaluate the probability of finding each residue in that conformation</li>
<li>The probability is assessed based on the preferences observed in determined structures</li>
<li>The rationale behind threading is that there are a limeted number of basic folds naturally occurring in proteins and the preferences of aminoacids for different structural environments is sufficient for discriminating a good-fitting fold</li>
</ul></li>
</ul>
<h2 id="fold-recognition">Fold Recognition</h2>
<ul>
<li>Fold recognition is based on the idea that even if I cannot align the sequences of 2 proteins I can probably still align secodary structure elements and other features
<ul>
<li>Other features can be solvent accessibility profiles, disulfide bonds, …</li>
</ul></li>
<li>For doing fold recognition I need to assess the features of the sequence</li>
<li>For doing threading I need to first align the sequences to known folds
<ul>
<li>If the homology is remote I need to align them through fold recognition approaches</li>
</ul></li>
<li>Predicting sequence properties is a mapping problem
<ul>
<li>Secodary structure prediction: from a sequence of aminoacids to a sequence of secondary structures
<ul>
<li>I am mapping a 20-letter alphabet to a 3-letter alphabet</li>
</ul></li>
<li>Transmembrane regions: I want to map the sequence to the possible states in the membrane/out of the membrane</li>
</ul></li>
<li>The simplest approach for addressing these mapping problems is to use a propensity scale for each residue
<ul>
<li>It is a table of preferences of each residue for each mapped state</li>
</ul></li>
<li>The AAindex is a database of propensity scales for residues
<ul>
<li>AAindex1 contains properties at the residue level (hydrophobicity, ecc…)</li>
<li>AAindex2 contains substitution matrices</li>
<li>AAindex3 contains propensities for interactions among aminoacids</li>
<li>It shows for each entry also the correlation with other scales</li>
</ul></li>
<li>The most used propensity scale for secondary structure is the Chou-Fasman scale:
<ul>
<li>First they calculated how frequently a certain residue was found in each secondary structure (<eq env="math">P(A,h)</eq> with A being the residue and h the seocndary structure)</li>
<li>Then they corrected it for the independent probabilities for the residue and for the structure (<eq env="math">P(A), P(h)</eq>)</li>
<li>The propensity is therefore defined as <eq env="math">P(A,h)/(P(A)P(h))</eq></li>
<li>If there is no correlation among <eq env="math">P(A)</eq> and <eq env="math">P(h)</eq>, then <eq env="math">P(A,h) = P(A)P(h)</eq> and therefore the propensity is 1</li>
<li>If the propensity is bigger than 1 the residue tends to be in that structure more than expected</li>
<li>If it is smaller than 1 the residue tends to avoid that structure</li>
</ul></li>
<li>The original Chou-Fasman scale was built in 1974 from only 19 proteins, but the propensity scales did not change much also re-evaluating it with the current dataset
<ul>
<li>An updated version is available at AAindex</li>
</ul></li>
<li>The Chou-Fasman is not accurate, the accuracy evaluated on a set of uncorrelated sequences with known structures is around 50/60% whith 3 classes
<ul>
<li>It is 3-class classification, so it is not actually that bad but also not so useful in practice</li>
<li>It is possible to improve the result by evaluating the average propensity of a sliding window, instead of that of single residues
<ul>
<li>The original algorithm used a 4 residue window</li>
</ul></li>
<li>We can get better results by using neural networks</li>
</ul></li>
<li>The Kyte-Doolittle scale for hydrophobicity combines the partition coefficientin octanol/water with the propensity for a residue to being found in a transmembrane helix in known structures
<ul>
<li>It is positive for hydrophobic residues and negative for hydrophilic ones</li>
</ul></li>
<li>Second generation methods are based on the observation that the structure of a residue in a protein strongly depends on the sequence context in which it is found
<ul>
<li>Sliding windows of -8/8 to -13/13 residues are often used</li>
</ul></li>
<li>The GOR method (Garnier, Osguthorpe, Robson) was developped after the Chou Fasman scale and it also aims at the prediction of secondary structure from sequence
<ul>
<li>It evaluates a score for each structure class at each position as <eq env="displaymath"> S_{ij} = \log \frac{P(SS_i|aa_{i+j})}{p(SS_i)} \qquad j = -8, ...., 8</eq></li>
</ul></li>
<li>The GOR scale was also built from a set of known structures (25 in this case!) but it takes into account not only the propensities of individual amino acids to form particular secondary structures, but also the conditional probability of the amino acid to form a secondary structure given that its immediate neighbors have already formed that structure
<ul>
<li>It uses an -8/8 residues sliding window, assuming that the conformation of the central residue is influencedby the 17 neighboring residues</li>
<li>The frequency of each of the aminoacids in the 17 positions in the window was evaluated for <eq env="math">\alpha, \beta</eq> and turn conformations</li>
<li>This generates 3 17*20 matrices of frequencies</li>
<li>This information is used for predicting the conformation of the central residue in the window</li>
<li>It is more accurate than Chou Fasman but it is still not comparable with modern methods: it reaches 64% accuracy</li>
</ul></li>
<li>The information function for the GOR scale encodes how much information the sequence at <eq env="math">R_j</eq> contains about the structure <eq env="math">S_j</eq> <eq env="displaymath"> I(S_j, R_j) = \log \frac{P(S_j|R_j)}{P(S_j)}</eq>
<ul>
<li><eq env="math">S_j</eq> is one of the 3 possible secondary structures at position <eq env="math">j</eq></li>
<li><eq env="math">R_j</eq> is on of the 20 residues at position <eq env="math">j</eq></li>
<li><eq env="math">P(S_j)</eq> is the prior probability of that structure</li>
<li>If <eq env="math">I=0</eq> it there is no information in <eq env="math">R_j</eq>, if <eq env="math">I &gt; 0</eq> <eq env="math">R_j</eq> favors <eq env="math">S_j</eq>, if <eq env="math">I &lt; 0</eq> <eq env="math">R_j</eq> avoids <eq env="math">S_j</eq></li>
</ul></li>
<li>The GOR scale is making assumptions to simplify the prediction of the secondary structure
<ul>
<li>In reality, the secondary structure depends on the whole sequence, but the GOR scale assumes that it only depends on the local sequence (the sliding window) <eq env="displaymath"> I(S_j, R) \approx I(S_j;R_{j-8}...R_{j+8})</eq></li>
<li>Each position is assumed to be statistically independent, and so the probabilities can be multiplied (summed in log scale) <eq env="displaymath"> I(S_j;R_{j-8}...R_{j+8}) \approx \sum_{m=-8}^8 I(S_j, R_j+m) </eq></li>
</ul></li>
</ul>
<h1 id="protein-structure-analysis">Protein Structure Analysis</h1>
<ul>
<li>Some numbers in biological databases at April 2020
<ul>
<li>GenBank: 216M entries</li>
<li>UniRef90: 109M entries</li>
<li>Swiss-Prot: 562k entries</li>
<li>PDB: 163k entries (redundant!)</li>
</ul></li>
<li>Protein folding is the process by which a protein assumes its native conformation from the unfolded structure</li>
<li>Anfinsen’s hypothesis states that there is a native structure for each protein and the sequence alone contains all the information required for reaching the native form
<ul>
<li>Anfinsen showed that the denatired ribonuclease A returns active after the denaturing agent is removed</li>
</ul></li>
<li>Levinthal’s paradox: the conformational space for a protein is huge and it cannot be fully explored by the protein in a time equal to the age of the universe
<ul>
<li>A 100 residues protein with 2 possible conformations per residue has <eq env="math">2^{100} \approx 10^{30}</eq> possible conformations</li>
<li>If a conformation can be visited in <eq env="math">10^{-12}</eq> seconds, exploring the full conformational space will take <eq env="math">10^{18}</eq> seconds</li>
<li>The univers now is <eq env="math">2-3*10^{17}</eq> seconds old</li>
</ul></li>
<li>The Anfinsen’s dogma postulates the uniqueness, stability, and kinetical accessibility of the native fold
<ul>
<li>The native structure is the only one with a free energy that low, in physiological conditions
<ul>
<li>There are fluctuations and bi-stable conformations, but in general this is true</li>
</ul></li>
<li>The native structure is stable against small fluctuations in the environment: it is in a local (and possibly global) energy minimum</li>
<li>The path in the free energy surface for reaching the folded state has a low kinetic barrier, so that it is possible to reach the folded state in reasonable time</li>
</ul></li>
<li>The solution to protein folding consist of studying 3 different aspects of theproblem: thermodynamics, kinetics, and conformation
<ul>
<li>I want to estimate the stability (thermodynaics) of the native conformation</li>
<li>I want to define the conformational path from the unfolded to the folded state, and the associated kinetics</li>
<li>I want top predict what the native conformation will be</li>
</ul></li>
<li>Thermodynamics: the <eq env="math">\Delta G</eq> of folding is typicall small, around -5/-15 kcal/mol
<ul>
<li>It is less than the energy of a covalent bond (-30/-100 kcal/mol)!</li>
<li>It is comparable to the energy of a salt bridge</li>
<li>Various interactions contribute to the stability of the native state, but they are NOT the driving forces of folding</li>
<li>The hydrophobic collapse of the globular core is the driving force of folding
<ul>
<li>Water molecules form a cage-like structure around hydrophobic surfaces</li>
<li>The <eq env="math">\Delta H</eq> of the hydrophobic effect is positive since breaking the cage requires energy</li>
<li>The <eq env="math">\Delta S</eq> is positive since water molecules are less constrained when freed from the hydrophobic surface</li>
</ul></li>
</ul></li>
<li>Kinetcs: the protein folding mechanism depends on the free energy conformational landscape
<ul>
<li>An higher activation barrier corresponds to a longer folding time</li>
</ul></li>
<li>Protein structure is hierarchically organised in primary, secondary, tertiary, and optionally quaternary structure</li>
<li>Secondary structure elements are the <eq env="math">\alpha</eq>-helix, the <eq env="math">\beta</eq>-sheet and the random coil</li>
<li>The alpha-helix is genrally right handed, with <eq env="math">\phi-60</eq>° and <eq env="math">\psi=-50</eq>°
<ul>
<li>Side chains are projected backwards and outwards</li>
<li>The pitch of the helix is <eq env="math">5.4 \AA</eq></li>
<li>There are 3.65 residues per turn</li>
<li>The core is tightly packed and formed by backbone atoms</li>
<li>The N-H of one aminoacid forms an H bond with the C=O of 4 residues earlier</li>
<li>The H bonding pattern gives to the helix a dipole moment</li>
</ul></li>
<li>The beta-strand has <eq env="math">\phi=140</eq>° and <eq env="math">\psi=-130</eq>°
<ul>
<li>The side chains project alternating above and below the strand</li>
<li>The conformation is more extended than in helices</li>
<li>Beta-strands can form parallel or anti-parallel beta-sheets</li>
<li>Parallel beta-sheets are less stable than the anti-parallel ones</li>
</ul></li>
<li>The arrangement of secondary structure elements forms the tertiary structure of the protein</li>
<li>The arrangement of 2 or more proteins in a complex gives rise to the quaternary structure</li>
<li>A PDB file contains the 3D coordinates of all the atoms (except H usually) in a protein</li>
<li>A protien structure is well defined by <eq env="math">\phi</eq> and <eq env="math">\psi</eq> angles, side chain rotamers, and solvent accessible area
<ul>
<li>The dihedral angles define the secondary structure</li>
</ul></li>
<li>The Ramachandran plot is a plot of <eq env="math">\phi</eq> and <eq env="math">\psi</eq> angles for each residue in a protein
<ul>
<li>Distinct regions of the plot are assigned to <eq env="math">\alpha</eq> and <eq env="math">\beta</eq> conformations</li>
<li>Some regions are not allowed by steric hindrance</li>
<li>The Ramachandran plot of a model can be used to estimate its quality thanks to the knowledge that we have about disallowed regions, deriving from well-determined experimental structures</li>
</ul></li>
<li>DSSP (define secondary structure of proteins) is an algorithm that calculates <eq env="math">\phi</eq> and <eq env="math">\psi</eq> angles, solvent accessibility, secondary structure and other features from a 3D structure
<ul>
<li>It is both available as a web server and as an installable package</li>
<li>For each residue in a PDB structure it calculates the dihedral angles and the solvent accessibility</li>
<li>From these pieces of information it determines the secondary structure using the Ramachandran plot</li>
</ul></li>
</ul>
