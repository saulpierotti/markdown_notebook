<h1 id="part-1---prof.-casadio">Part 1 - Prof. Casadio</h1>
<h2 id="introduction">Introduction</h2>
<ul>
<li>When doing data analysis, check first the quality of your data and ask yourself if it is worth it analysing them</li>
<li>In the real world you will mostly be alone, so learn to be independent</li>
<li>You should be able to create your own tools when pre-made tools are not available</li>
<li>Computational results need to be analysed in terms of their exportability ot the wet lab for validation</li>
<li>Information increases by going from sequences to structures</li>
<li>The CAF (critical assessment of functional annotation) is an important contest for functional annotation</li>
<li>Levitt was a pioneer in molecular dynamics and won a Nobel prize in 2013</li>
<li>Homogeneity of the data is a prerequisite if you want to compare your resoults with other people
<ul>
<li>If I want to predict secondary structure, it is important that we all use the same kind of assignments as labels (dssp)</li>
</ul></li>
<li>A PDB structure is a reduced representation of a protein</li>
<li>In order to calculate the contact surfa among 2 proteins I can just evaluate the difference in solvent accessible area upon binding</li>
<li>A new protein family can be defined when a structure with a totally new fold is discovered
<ul>
<li>This is really rare but possible</li>
</ul></li>
<li>If you want to find a job look at databases, if you are good enough swtich to computational biology and develop algorithms</li>
<li>Database numbers (24/11/2020)</li>
</ul>
<table>
<tbody>
<tr class="odd">
<td>UniProt</td>
<td style="text-align: right;">195 104 019</td>
<td>proteins</td>
</tr>
<tr class="even">
<td>SwissProt</td>
<td style="text-align: right;">563 552</td>
<td>proteins</td>
</tr>
<tr class="odd">
<td>PDB</td>
<td style="text-align: right;">171 313</td>
<td>structures</td>
</tr>
<tr class="even">
<td>Pfam</td>
<td style="text-align: right;">18 259</td>
<td>families</td>
</tr>
</tbody>
</table>
<h2 id="secondary-structure">Secondary Structure</h2>
<ul>
<li>In order to determine the secondary structure of a protein is important to have good resolution structures
<ul>
<li>From a good structure I can understand torsion angles</li>
</ul></li>
<li>jCE is not available anymore at the PDB, you can only do sequence alignments and not structural alignments</li>
<li>For structural alignments we can use TopMatch (for pairwise) or Mustang (multiple)</li>
<li>If I have a CryoEM structure the secondary structure is not easy to determin (depends on resolution)</li>
<li>First problem: I have a crystal structure and I want to determine the secondary structure of a protein</li>
<li>A 3d structure is a reduce representation of the electron density that collapses all the fuzzy electron density of a molecule to discrete atomic coordinates!</li>
<li>It is easier to descibe secondary structure in terms of H bonds, not in terms of torsion angles</li>
<li>First I need to locate the hydrogens, that are usually not seen at normal resolutions
<ul>
<li>H is transparent to X-rays</li>
<li>There are several algortihms for H embedding
<ul>
<li>A famous one was developped by Janet Thornton (EBI director)</li>
<li>Some algorithms are MolProbity and WHATIF</li>
<li>WHATIF was developped by a professor that now lives in the Filippines</li>
</ul></li>
<li>Also molecular visualization software need to fill-in hydrogens to determine which secondary structure to show!</li>
<li>In general I can embed hydrogens by following simple chemical rules</li>
</ul></li>
<li>Once I have the hydrogen positions I can determine the secondary structure from the backbone H-bonding pattern among neighboring residues</li>
<li>I can understand if there is an H bond between 2 C atoms by inspecting their distance (after placing the H)</li>
<li>We will use the dssp algorithm for secondary structure assignment from 3d structure</li>
<li>Torsion angles are not so easy to extract from 3d structures, so they are not the preferred way for secondary structure determination</li>
<li>An H bond is present when the distance among the atoms involved is under 2 <eq env="math">\AA</eq> and the atoms are co-planar
<ul>
<li>H bonds can be formed in proteins mainly by O and N</li>
</ul></li>
<li>H bonding happens both in solvent accessible and inaccessible areas</li>
<li>Side chains do not really determine secondary structure, so predicting secondary structures from sequence can be misleading
<ul>
<li>Propensities are only average properties</li>
</ul></li>
<li>Prediction of secondary structure is important since by assigning a good secondary structure to a sequence I can identify remote homologs
<ul>
<li>In superfamilies structure is conserved but not sequence</li>
<li>If structure is conserved and I can determine the secondary structure of a sequence, I can align secondary structure elements in order to identify remot homologs</li>
</ul></li>
<li>From this, I can maybe get a structural model for proteins lackingg obvious homolog structures in the PDB
<ul>
<li>I can obtain the secondary structure of all the PDB structures using dssp or similar algorithms</li>
<li>I can predict the secondary structure of my sequence</li>
<li>I can try to find a structural template for my protein by aligning secondary structure elements and finding a protein with similar topology (irrespective of sequence)</li>
</ul></li>
<li>The topology of a protein is the order succession of secondary structurer elements</li>
<li>Predicting secondary structure means mapping a 20-charachtr alphabet to a 3-charachter alphabet (H, E, C)</li>
<li>Also here, after I am able to assign a structure to my protein I can trasfer annotation from the template: I can do functional annotation</li>
<li>To date SS predictors are all top-scoring, so there is not much to improve
<ul>
<li>Accuracy is currently <eq env="math">0.8</eq> to <eq env="math">0.9</eq></li>
<li>Burckhard Rost suggested in a paper that we are at an hard limit of accuracy since we are limited by uncertainty in the original structural data</li>
</ul></li>
<li>Secondary structures are cooperative</li>
<li>A secondaary structure is a local motive of order</li>
<li>There are at least 8 types of secondary structures</li>
<li>Helices tend to be easier to predict than strands and coils
<ul>
<li>Sliding windows are grasping the context surrounding the predicted conformation</li>
<li>Since helices are local structures (as compared to strands), they can be better predicted by a sliding window system</li>
</ul></li>
<li>A chamelion segment is a sliding window configuration that can be found in different configurations</li>
<li>A sliding window-based approach can discriminate chamelion segments only if they are shorter of sliding window!</li>
<li>Today, if you need a secondary structure use Jpred (it gives quality scores) or PsiPred (it does not)</li>
</ul>
<h2 id="helices">Helices</h2>
<ul>
<li>Alpha helices are self-organizing local structures
<ul>
<li>They are local in the sense that they do not involve long-range interactions</li>
</ul></li>
<li>In alpha helices the H-bonding happens among residues in position <eq env="math">i</eq> and <eq env="math">i+4</eq></li>
<li>The minimum lenght for an helix is thus 4 residues</li>
<li>There is no real maximum lenght, there are fibers that are even 500 residues long</li>
<li>Alpha-helices are dextrose</li>
<li>A summary of helices dimensions</li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;"><eq env="math">\alpha</eq> helix</th>
<th style="text-align: right;"><eq env="math">3_{10}</eq> helix</th>
<th style="text-align: right;"><eq env="math">\pi</eq> helix</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Residues per turn</td>
<td style="text-align: right;">3.6 aa</td>
<td style="text-align: right;">1.0 aa</td>
<td style="text-align: right;">4.4 aa</td>
</tr>
<tr class="even">
<td>Step per residue</td>
<td style="text-align: right;">1.5 <eq env="math">\AA</eq></td>
<td style="text-align: right;">2.0 <eq env="math">\AA</eq></td>
<td style="text-align: right;">1.1 <eq env="math">\AA</eq></td>
</tr>
<tr class="odd">
<td>Radius</td>
<td style="text-align: right;">2.3 <eq env="math">\AA</eq></td>
<td style="text-align: right;">1.9 <eq env="math">\AA</eq></td>
<td style="text-align: right;">2.8 <eq env="math">\AA</eq></td>
</tr>
<tr class="even">
<td>Pitch</td>
<td style="text-align: right;">5.4 <eq env="math">\AA</eq></td>
<td style="text-align: right;">6.0 <eq env="math">\AA</eq></td>
<td style="text-align: right;">4.8 <eq env="math">\AA</eq></td>
</tr>
</tbody>
</table>
<h2 id="strands">Strands</h2>
<ul>
<li>Beta sheets are not local since they require the interaction of beta strands that can also be far in the sequence</li>
<li>The minimun length for a strend is 2 residues (Prof. Casadio did the statistics on the PDB)</li>
<li>Knowing the minimum length is important in order to assess the reliability of predictions and implement filters</li>
</ul>
<h2 id="deltadelta-g"><eq env="math">\Delta\Delta G</eq></h2>
<ul>
<li>A major source of stability for a protein is its <eq env="math">\Delta G</eq> of folding</li>
<li>The <eq env="math">\Delta G</eq> of folding can be measured in vitro</li>
<li>The difference in <eq env="math">\Delta G</eq> of folding among 2 variants of the same protein is called <eq env="math">\Delta \Delta G</eq> of folding of that variation</li>
<li>A variant shares most of the prilmary sequence with the original protein, and it can change at most in a couple of positions or present indels</li>
<li>Note that <eq env="math">\Delta G</eq> is an experimentally measured quantity with a SD of around <eq env="math">\pm 1 kcal/mol</eq> with current experimental technique
<ul>
<li>When calculating a <eq env="math">\Delta \Delta G</eq> aware of this!</li>
<li>The <eq env="math">\Delta G</eq> of folding can range from <eq env="math">0</eq> to <eq env="math">-50 kcal/mol</eq></li>
<li>A <eq env="math">\Delta G</eq> of <eq env="math">0 kcal/mol</eq> does not mean much when it is affected by an error of <eq env="math">\pm 1 kcal/mol</eq>!</li>
</ul></li>
<li>If <eq env="math">\Delta \Delta G &gt; 1 kcal/mol</eq> I have an increase in protein stability for the variant with respect to the original protein</li>
<li>If <eq env="math">-1 &lt; \Delta \Delta G &lt; 1 kcal/mol</eq> the variant does not affect protein stability</li>
<li>If <eq env="math">\Delta \Delta G &lt; -1 kcal/mol</eq> I have a decrease in protein stability for the variant with respect to the original protein</li>
</ul>
<h2 id="uniprot-biocuration">UniProt Biocuration</h2>
<ul>
<li>UniProt/TrEMBL is annotated manually contributing to SwissProt
<ul>
<li>Protein/Genes in UniProt are first screened to remove redundancies</li>
<li>A sequence analysis step is performed using predictors and expert systems (Pfam, InterPro, …)
<ul>
<li>I am here predicting sequence features</li>
</ul></li>
<li>Literature is mined to establish links between assertions and experimental evidence
<ul>
<li>This is a combination of text mining and actual people reading the papers</li>
</ul></li>
<li>Curation based on protein families (annotation transfer)</li>
<li>Evidence attribution for individal claims</li>
<li>Quality assurance before integration into SwissProt
<ul>
<li>Here the annotation score is assigned</li>
</ul></li>
<li>The PDB structure is included (if it exists)</li>
</ul></li>
<li>SwissProt is used then for the automatic annotation of UniProt
<ul>
<li>Templates are identified by sequence alignment</li>
<li>Build rules that define conditions to be respected for the transfer of annotation
<ul>
<li>SAAS (Statistical Automatic Annotation System) is used for the automatic generation of annotation rules
<ul>
<li>It is a decision tree</li>
</ul></li>
<li>UniRule is a set of manually created rules (If…Else statements)</li>
</ul></li>
<li>The new rules are checked against SwissProt for validation before being applied</li>
<li>The rules are used for the automatic annotation of UniProt</li>
</ul></li>
<li>TrEMBL is obtained by automatic translation of EMBL
<ul>
<li>Finding CDSs on genomic data is not trivial</li>
</ul></li>
<li>Only 5% of the UniProt entries are obtained directly from the sequencing of proteins</li>
<li>Particularly in TrEMBL, annotation can change drammatically among DB version
<ul>
<li>This is due to the update of UniRule and SAAS</li>
</ul></li>
<li>Since release 2020_04 SAAS has been replaced with ARBA (Annotation-Rule Based Annotator)
<ul>
<li>It is a multiclass learning system trained on SwissProt etries</li>
<li>It uses rule-mining techniques to generate human-readable annotation rules</li>
<li>It maintains a set of exclusion rules for properties that cannot be efficiently automatically annotated</li>
</ul></li>
<li>Each annotation in UniProt reports an evidence tag that assert how that annotation was obtained (experimental evidence or prediction)
<ul>
<li>This tag system is called ECO, and it is similar ot GO terms (it is an acyclic graph of descriptors)</li>
</ul></li>
</ul>
<h2 id="short-range-interactions">Short Range Interactions</h2>
<ul>
<li>H bonds are long around <eq env="math">2 \AA</eq> and have an energy of <eq env="math">10 kcal/mol</eq>
<ul>
<li>The bond energy is roughly proportional to the squared distance of the atoms</li>
</ul></li>
<li>Van der Waals interactions (London dispersion energy) are among induced dipoles
<ul>
<li>They are heavily depend on distance <eq env="math">r^{-6}</eq></li>
</ul></li>
<li>Salt bridges are charged-charged particle interactions
<ul>
<li>Their strenght depends on the environment (at the core or in the solvent)</li>
<li>They depend on <eq env="math">r</eq></li>
</ul></li>
<li>Rotating dipole interactions are temperature dependent</li>
</ul>
<h2 id="interpro">InterPro</h2>
<ul>
<li>InterPro is a consortium of many different predictors that converge in the annotation of proteins and families</li>
<li>It is now at version 82, it is frequently updated!</li>
<li>Pfam is included in InterPro, and kind of performs the same function</li>
</ul>
<h2 id="dssp-define-secondary-structure-of-proteins">DSSP (Define Secondary Structure of Proteins)</h2>
<ul>
<li>DSSP (Kabasch and Sanders, Heidelberg) is used for the definitiion of secondary structure in many molecular visualization software</li>
<li>The recognition of secondary structure is based on a set of criteria, not laws</li>
<li>Recognizing secondary structure is a pattern recognition process on H bonds</li>
<li>DSSP is not the only SS predictor from structure, but it is very simple and one of the most used
<ul>
<li>It inspired another predictor, Stride</li>
<li>It is used by basically all the molecular visualization softwares</li>
</ul></li>
<li>The DSSP paper is from 1983, and the first PDB deposition was done in the 70’s</li>
<li>DSSP approximates secondary structures with an objective function</li>
<li>DSSP uses H bonds as a defining feature and not torsion angles or atomic coordinates, since these can be defined by a single parameters (bond energy cutoff)</li>
<li>An <eq env="math">n</eq>-turn is defined as an H bond between the CO of residue <eq env="math">i</eq> and the NH of residue <eq env="math">i+n</eq>, with <eq env="math">n \in \{3,4,5\}</eq></li>
<li>An H bond between residues not near to each other in the sequence defines a bridge</li>
<li>Turns and bridges essentially exhaust all the possible interactions among backbone atoms</li>
<li>DSSP was originally written in Pascal</li>
<li>The current version was written by the authors of WHATIF (Maarten Hekkelman) and it is in C++</li>
<li>The definition of secondary structure is hierarchical: H bonds define turns and bridges, and they define helices and ladders</li>
<li>DSSP assigns a secondary structure to each residue</li>
<li>H bonds are assumed to be present if they have an energy lower than <eq env="math">-0.5 kcal/mol</eq>
<ul>
<li>A good H bond is said in the paper to be of about <eq env="math">-3 kcal/mol</eq></li>
</ul></li>
<li>The energy of H bonds is represented with a heuristic equation (electrostatic model, not quantistic)</li>
<li>The cutoff chosen is permissive to allow for bifurcated H bonds and errors in the coordinates</li>
<li>There is no generally correct H bond definition since there is not sharp transition from the quantistic definition (that dominates at short distances) and the electrostatic definition (that dominates at larger distances)</li>
<li>The angular flexibility was limited to 60°</li>
<li>Helices are represented with H in the output of DSSP, ladders with E (extended)</li>
<li>A series of turns is an helix, a series of bridges is a ladder</li>
<li>A bend is an angle in which the <eq env="math">C_\alpha</eq> in position <eq env="math">i</eq> and <eq env="math">i+2</eq> form an angle higher than <eq env="math">70°</eq></li>
<li>In practice, everithing that is not an helix or a strand will be considered and represented as a coil (C, or -)
<ul>
<li>This includes bends!</li>
</ul></li>
<li>Most helices have positive chirality (right-handedness) while strands have negative chirality</li>
<li>As an addendum, DSSP can determine the solvent accessible area of a protein
<ul>
<li>The solvent exposure calculated is static since it does not include the movent of the protein</li>
<li>The dynamic movent of the protein can alter which portions are accessible!</li>
<li>Physically, we are interested in the number of water molecules in direct contact with the protein</li>
<li>Geometrically, I can roll a sphere representing a water molecule on the surface of the protein and record the positions touched
<ul>
<li>The surface is proportional to the number of water molecules in the first hydratation shell</li>
</ul></li>
</ul></li>
</ul>
<h2 id="neural-networks">Neural Networks</h2>
<ul>
<li>Neural networks are inspired by biological neurons</li>
<li>McCulloch e Pitts (1943) modeled a neuron as an object receiving an input vector and producing a scalar output via linear combination of the inputs (with weights neign the linear coefficients), transformed by a transfer function</li>
<li>A treshold can be considered as just an additional neuron</li>
<li>A simple perceptron has just an input layer and an output layer, and operates only in a feed-forward manner</li>
<li>A multi-layer perceptron contain hidden layers of neurons with only feed-forward connections</li>
<li>A NN is an alternative computational paradigm in which the solution to a problem is automatically learned from a set of examples</li>
<li>A NN performs general non-linear functional mapping (from an input to an output space)</li>
<li>Any model relates variables by the means of parameters</li>
<li>A single-layer perceptron performs a total summation of the inputs <eq env="math">x_i</eq>, each associated with a weight <eq env="math">w_i</eq>, in order to determine the activation <eq env="math">a</eq>, and the output <eq env="math">z</eq> after applying the transfer function <eq env="math">g(a)</eq> <eq env="displaymath">a=\sum_i w_i x_i</eq> <eq env="displaymath">z = g(a)</eq>
<ul>
<li>The bias of the neuron is usually represented as an additional input <eq env="math">x_0</eq></li>
</ul></li>
<li>The error function of the perceptron is proportional to the sum of squared differences between predicted and real values, over all the training examples and over all the output neurons</li>
<li>For the single training point <eq env="math">X^q</eq>, if <eq env="math">D_i^q</eq> is the real desired value of example <eq env="math">q</eq> for output neuron <eq env="math">i</eq> <eq env="displaymath"> E^q = \frac{1}{2} \sum_i (Y_i(X^q)-D_i^q)^2</eq></li>
<li>The NN reaches an optimal configuration when its error function is minimal</li>
<li>The backpropagation algorithm was invented by Rumelhart when he was a PhD student in 1986
<ul>
<li>It is a gradient descent algorithm</li>
<li>It updates the weight by subtracting to the previous weigths the derivative of the error function with respect to the weight (times the learning rate <eq env="math">\eta</eq>) and adding a momentum term</li>
<li>The momentum term is the previous update to the weights times the hypeerparameter <eq env="math">\mu</eq></li>
</ul></li>
<li>The input size of the network is in general, for sequences, the size of the sliding window applied</li>
<li>Neural Networks are convenient when an analitical solution for my problem is not available</li>
<li>I need a lot of data, and data of a very good quality
<ul>
<li>Before starting any machine learning approach, check data quality</li>
</ul></li>
<li>Many journals require specific cross-validation procedures for ML papers</li>
<li>As for ML approaches input is it better to use profiles instead of sequences</li>
<li>Transfer functions can be linear, stepwise, sigmoid</li>
<li>Neural Networks can perform a non linear functional mapping thanks to their nonlinear tranfer function</li>
<li>In order to update the weights in the backpropagation algorithm I use the derivative of the error function with respect to the weights</li>
<li>It is important for tranfer functions to be continuous in order to make the error function derivable</li>
<li>The derivative of the sigmoid is the sigmoid itself, so it is really handy in computations for calculating higher derivatives
<ul>
<li>It is not like this, but so she thinks</li>
<li>It is <eq env="math">\sigma(x)(1-\sigma(x))</eq> where <eq env="math">\sigma(x)</eq> is the sigmoid itself</li>
</ul></li>
<li>Why SVM and NN for predicting protein secondary structure and not HMM?
<ul>
<li>HMMs are global models for sequences, and they are useful for predicting global properties like domains</li>
<li>ML approaches are local (use a sliding window) and so are better for predicting a local motive of order like secondary structures</li>
</ul></li>
</ul>
<h2 id="score-indexes">Score Indexes</h2>
<ul>
<li>The scoring accuracy is the fraction of correct predictions <eq env="displaymath">Q_1 = \sum_i P_i/N</eq></li>
<li>The Matthews correlation coefficient is a correlation coefficient for the classes
<ul>
<li>It is not affected by class umbalance</li>
</ul></li>
<li>The segment overlap measure (SOV) is used for the prediction of sequences <eq env="displaymath">SOV = \sum_s \frac{S_1 \cap S_2}{S_1 \cup S_2} \frac{L_1}{N}</eq> _ It can be defined in at least 3 different ways _ She expect us to put it in the project _ It is a measure of the correct superimposition of characters _ It should be calculated for H, E, and C (the term s in the sum) and summed</li>
<li>Many predictors return also a reliability index for each position in the sequence (in the range 0-9)
<ul>
<li>In NN I can just derive it from the activations of the output layer by subtracting the best output (the highest) to the second best</li>
<li>It is proven that in a NN the output of a network is the probability of a certain answer</li>
</ul></li>
</ul>
<h2 id="history-of-secondary-structure-prediction">History of Secondary Structure Prediction</h2>
<ul>
<li>Secondary structures were predicted by Linus Pauling and Robert Corey in 1951 on the basis of H bonding and cooperativity criteria, before the first crystal structure
<ul>
<li>Pauling got the Nobel prize in chemistry 1954 (for secondary structures!) and in 1962</li>
</ul></li>
<li>NN proved to be the best tool for predicting secondary structure</li>
<li>The prediction of secondary structure is a bioinformatics success story</li>
<li>The first attempt at SS prediction was in 1957</li>
<li>Myoglobin was the first prtoein to be solved at atomic resolution in 1960</li>
<li>The PDB was born in 1973 with 15 structures</li>
<li>First generation SS prediction methods (1960-1970) were based on single residue propensities, often averaged over a sliding window
<ul>
<li>An example is the Chou-Fasman method</li>
</ul></li>
<li>Second generation methods (1970-1990) were based on joint probabilities, and so started to investigate the context in which residues were found
<ul>
<li>Here they started also to use NN</li>
<li>An example is the GOR method</li>
<li>Accuracy stalled at 60%</li>
</ul></li>
<li>Third generation methods (1990) integrate evolutionary information by using sequence profiles
<ul>
<li>They were the first to break 70% accuracy</li>
<li>Seminal Rost and Sanders 1994 paper</li>
</ul></li>
<li>These generations actually do not only apply to secondary structure prediction, but in general to many prediction methods of protein properties</li>
</ul>
<h3 id="first-generation-methods">First Generation Methods</h3>
<ul>
<li>They are based on the computation of the frequency of occurrence of a residue in relation to a given feature</li>
<li>A sliding window is a way for searching for emerging properties in a context
<ul>
<li>I can associate a property to each sliding window, and I can consider it as an emerging property of the window</li>
</ul></li>
<li>ProtScale is a website at ExPasy where I can input a sequence (or an identifier) and I get a series of sequence properties
<ul>
<li>I can get molecular weight, different propensities, …</li>
<li>I can get all the properties averaged over a sliding window (that I can choose)</li>
<li>I can also decide the summarization method over the sliding window</li>
</ul></li>
<li>The Kite-Doolittle scale is an average of several hydrophobicity scales (including a partition coefficient) and it is provided at ProtScale</li>
<li>The Chou-Fasman scale is also in ProtScale</li>
<li>For proteins sliding windows are usually between 3 and 24 residues long
<ul>
<li>A really long window loses in resolution</li>
<li>A small sliding window is too noisy</li>
</ul></li>
<li>Using sliding windows I can see the emergence of regions with certain properties</li>
<li>Chou-Fasman uses an heuristic for resolving inconsistencies (regions with more than 1 propensity)</li>
</ul>
<h3 id="second-generation-methods">Second Generation Methods</h3>
<ul>
<li>Here wee take explicitly advantage of the context</li>
</ul>
<h3 id="third-generation-methods">Third Generation Methods</h3>
<ul>
<li>Here we integrate evolutionary information in the form of sequence profiles</li>
<li>Neural networks learn the mapping from sequence to secondary structure</li>
<li>I want to use NN when I am not able to provide a simple first principle or a model based solution</li>
<li>A sliding window on a profile is a matrix, and it is used as input for ML approaches</li>
<li>Each weight multiplies one vector of that matrix</li>
<li>Suppose that a first NN gives me an answer with a single residue in H conformation
<ul>
<li>I could use its output as input for another network in order to filter out these spourios alignments</li>
<li>This is equivalent to adding layers to the network</li>
</ul></li>
<li>I can use a collection of NN that, for instance, work on different sliding window sizes (or only discriminate one conformation), and then use an ensemble method to get an answer</li>
<li>The biocomp group created secpred in the past</li>
<li>Psipred was created at UCL at can predict, among other things, SS
<ul>
<li>The original was a NN, then it switched to SVM and now maybe itwill switch to deep learning</li>
<li>When the sequence has a structure, they just give the dssp answer</li>
</ul></li>
</ul>
<h2 id="bologna-annotation-resource-bar">Bologna Annotation Resource (BAR)</h2>
<ul>
<li>Data validation is essential in predictions</li>
<li>Experiments to validate protein predictions are lengthy and costly</li>
<li>It is possible that 80% of the entries in UniProt are crap</li>
<li>Protein function can be described with GO terms (cellular component, biological process, molecular function), EC numbers</li>
<li>Each GO term has a tag referring to the way it was annotated (prediction or experimental evidence)
<ul>
<li>When doing prediction train only with experimental data</li>
<li>If you train on noise you get noise squared</li>
</ul></li>
<li>If sequence identity with an annotated template is above 80%, I can most probably just transfer the annotation
<ul>
<li>For molecular function I can transfer quite easily</li>
<li>For cellular component check if the organisms are both eukariotes!</li>
</ul></li>
<li>For identity above 30% I can say that 2 proteins have a similar structure, but I cannot say much for the function</li>
<li>Similarity searches can be done with BLAST, Psi-BLAST, or Pfam (with HMMs)</li>
<li>Professor Casadio became famous since she was able to predict the structure of membrane proteins</li>
<li>Once they published a prediction that, after the crystal was published, was shown to be wrong (1 helics more)
<ul>
<li>That protein belonged to a family without any structure</li>
</ul></li>
<li>Since then, she decided to be very conservative about predictions</li>
<li>Her group was given money to use a GRID as large as Emilia Romagna</li>
<li>They used 30 million sequence from UniProt
<ul>
<li>If you filter out fragments and duplicates UniProt becomes much smaller</li>
</ul></li>
<li>They alligned all those sequences pairwise to create a network of sequences that they called BAR+</li>
<li>Today they repeated with more sequences and released BAR+</li>
<li>For the alignment they used BLAST
<ul>
<li>Even if BLAST is a local alignment tool, they were able to use it by forcing a minimum alignment lenght</li>
</ul></li>
<li>They called this network the protein sequence universe</li>
<li>Many proteins in UniProt have some kind of annotation (PDB, GO, Pfam annotation)</li>
<li>On the other side, I have all the poorly annotated NCBI proteomes</li>
<li>They creatad a sequence similarity network by linking all the sequences that, pairwise, have identity higher than 40% and minimum reciprocal coverage of 90%</li>
<li>Each connected component of the network forms a cluster</li>
<li>The statistically validated the annotation of each cluster and trasfered annotations to all the cluster members</li>
<li>From each cluster I can get an HMM by alligning members of the cluster that have a structure and members directly connected to them</li>
<li>Once I have the HMM I can use it to align sequences to it and use modeler to get a structural model</li>
<li>Now they are trying to trasfer also EC numbers</li>
<li>BAR contains millions of clusters!</li>
<li>To statistically validatej each annotation they did bootstrapping and calculated a p-value, and corrected it with Bonferroni</li>
<li>There are around 10k cluster HMMs in BAR</li>
<li>23% of the sequences are in cluster that have a PDB!</li>
<li>By setting a p-value thershold of 0.01, they were able to increase the nnotation of TrEMBL by 54%!</li>
<li>In very large cluster, you can use a community detection algorithm to identify sub-clusters</li>
<li>For some enzymes, sub-clusters have different specificities!</li>
<li>They took part at the CAFA challange on functional annotation with BAR</li>
<li>BAR allows to find the protein types that are most common in different genomes and to do sequence annotation</li>
<li>I can transfer annotation among sequences with low similarity thanks to the intermediate alignments in BAR</li>
<li>Different BAR clusters can belong to the same family</li>
</ul>
<h2 id="the-multi-dimensional-problem-of-protein-protein-interactions">The Multi-Dimensional Problem of Protein-Protein Interactions</h2>
<ul>
<li>PPI is a multi-dimensional problem since it can be studied at the level of interacting residues or at the mesoscopic level of PPI networks</li>
<li>Castrense devolepped the latest PPI predictiors</li>
<li>It is possibles to annotate proteins by predicting their interacting partners</li>
<li>PPI predictions are one of the main bioinformatics fields where ML is used</li>
<li>Macromolecular crowding is underapprectiated</li>
<li>Protein phase separation: at high enough concentration of proteins in solution in the cytoplasm, the protein phase can separate
<ul>
<li>We observe aggregates of proteins</li>
<li>This is a well known chemical phenomenon called liquid phase separation</li>
</ul></li>
<li>An important question is if liquid phase separation has a biological role (in favoring PPIs)
<ul>
<li>This is membrane-less compartimentalization</li>
</ul></li>
<li>Protein flexibility may allow proteins to aggregate with different partners</li>
<li>An aggregate can be a single complex or a full metabolic pathway</li>
<li>Single cell sequencing is the future since it does not average over a large number of cells</li>
<li>Data quality is highly strategic, and there are companies specialised in measuring it</li>
<li>I can measure data quality in term of completeness
<ul>
<li>If my data are not complete either I produce them or buy them</li>
</ul></li>
<li>Conformity among databases is also important (same format for same data)</li>
<li>Consistency: are my data contradicting each other</li>
<li>Accuracy: how much can I trust the data?</li>
<li>Uniqueness: do I have duplicates?</li>
<li>Integrity: are data are linked correctly to each other</li>
<li>How do I assure that data is correctly mantained?</li>
<li>Deep learning, contrary to ML, In general does do feature extraction by itself</li>
<li>Interactomic studies have very low reproducibility for technical reasons</li>
<li>The coverage of proteomic and interactomic studies is never complete</li>
<li>Many interactomic studies neglect subcellular localization</li>
<li>It is also difficult to detect liquid-phase separation clusters in vivo</li>
<li>When I see a network in STRING I do not now if all the interaction happen in the same subcellular compartment!</li>
<li>The first account of PPI was in 1905 about trypsin and its regulators</li>
<li>Interactions documented in the PDB are PPIs and ligand-protein interactions</li>
<li>About 1/3 of the PDB structures are complexes</li>
<li>There is not significance difference in the distribution of areas of homo and hetero-interfaces (but homointerfaces tend to be larger)</li>
<li>The distribution of residuea is similar for homo and hetero interfaces (but Cys tend to be more common in heterointerfaces)</li>
<li>Note that in X-ray structures not all the interfaces are functional!
<ul>
<li>Some interfaces can only be due to the unit cell configuration</li>
</ul></li>
<li>An interface can be defined as the set of residues that undergo a difference in solvent accessible area of more than <eq env="math">1 \AA^2</eq> (<eq env="math">\Delta{ASA}</eq>) upon binding (as calculated by DSSP)</li>
<li>Another definition can be the set of residues whose <eq env="math">C_\alpha</eq> are at less then <eq env="math">12\AA</eq> Euclidean distance</li>
<li>Proteases have a very easily recognizable interaction surface
<ul>
<li>In this way every interacting residue is described by a matrix</li>
</ul></li>
<li>There are not strong residue propensities for interaction interfaces (just a little enrichment in hydrophobic residues)</li>
<li>Since there are no major emerging features for PPI surfaces, ML is the go-to method</li>
<li>Casadio’s group created the ISPRED series (ISPRED1 to 4) of PPI predictors
<ul>
<li>ISPRED predicts interacting residues from <strong>structure</strong></li>
<li>The last version uses hidden support SVMs and conditional random fields
<ul>
<li>An HMM-SVM is an SVM applied to the output of an HMM</li>
</ul></li>
<li>The first implementations were based on NN</li>
<li>They used DSSP to get residues that are more than 16% exposed, and repesented them only in terms of <eq env="math">C_\alpha</eq>
<ul>
<li>This was done in order to reduce the input space</li>
</ul></li>
<li>They then created a window around the interacting residues (selecting the 10 closest exposed residues in space at less than 12 nm distance) and built a profile on it (accornding to a MSA)</li>
<li>Current accuracy is arounf 0.7 and MCC 0.45</li>
</ul></li>
<li>These predictors can be used as a preliminary operation to docking in order to constrain the docking space</li>
<li>Note that these predictors do not predict who the interactors are!</li>
<li>Now they are working on the prediction of interacting residues from sequence
<ul>
<li>Current MCC is 0.3</li>
<li>Note that this requires first to predict the solvent accessible area from sequence (which is hard in itself)</li>
</ul></li>
<li>Database of PPI are BioGRID, ComplexPortal, IntAct, MINT, STRING
<ul>
<li>They usually do not agree on the interactions reported!</li>
<li>Usually if I need to build an interaction network I want to use the intersection of these databases, so the interactions reported by all of them</li>
</ul></li>
<li>Note that many interaction experiments are done in vitro so I am not sure if the interaction actually happens in cells!</li>
<li>An interactome is a partial view of reality!
<ul>
<li>MINT was in Rome but now it is in Milan</li>
</ul></li>
<li>Scoring a PPI predictor against interactions observed in the PDB can be misleading, since many interactions have not been crystallized
<ul>
<li>What I see as a false positive can actually be a true positive!</li>
</ul></li>
<li>Interacting residues tend to not to be alone but to form patches of interaction
<ul>
<li>A patch should contain at least 4-5 residues</li>
</ul></li>
<li>Another interesting task is to predict the number of interaction of a given protein
<ul>
<li>This is not easy since on the PDB I have a bias for stable interactions</li>
<li>I can consider my false positives as putative new interaction sites</li>
<li>I can compare the number of predicted patches with the number of interactors on PPI databases (the degree of the protein)
<ul>
<li>I can calculate the correlation among them</li>
<li>Filtering interactions by subcellular colocalization was able to improve the correlation</li>
</ul></li>
</ul></li>
<li>In order to understand if a set of interactors in STRING is meaningful I can look for an enrichment in specific GO biological process terms among the interactors
<ul>
<li>The enrichment reported by STRING is a statistical enrichment, it does not consider the connectivity of the network
<ul>
<li>This can be a t-test</li>
</ul></li>
<li>Another approach is to use NET-GE (biocomp-unibo)
<ul>
<li>I identify modules in the network and then perform enrichment on the modules</li>
<li>To identify the modules, I start from the set of proteins sharing the same GO term, called seeds</li>
<li>I calculate all the shortest paths among the seed nodes, and include the connecting nodes in the module</li>
<li>I rank the connecting nodes in terms of various measures and prune the low-ranking ones (but preserving the shortest path)</li>
<li>I calculate the enrichment of the GO biological process term used for obtaining the seeds</li>
<li>A multiple testing correction is also implemented</li>
<li>An important point is that I can start from a gene that I want to study, and after putting it in a network I can observe an enrichment for a GO term that was not even annotated to that gene (since it interacts with protein with that term)</li>
<li>Besides STRING, NET-GE can also use KEGG, Reactom, IntAct and other networks</li>
</ul></li>
</ul></li>
<li>In summary, PPI are multi-dimensional and
<ul>
<li>At the molecular level machine learning is useful</li>
<li>Network of PPIs can be used for gene enrichment studies</li>
</ul></li>
</ul>
<h2 id="data-analysis-in-translational-medicine">Data Analysis in Translational Medicine</h2>
<ul>
<li>Stratified medicine: matching terapies with biomarkers</li>
<li>Precision medicine: integration of molecular research with single-patient data</li>
<li>P4 medicine: clinical application of system’s biology</li>
<li>Personalised medicine: omics and patient empowerment</li>
<li>This field is a good idea for a PhD</li>
<li>The real challange is the single patient condition, not average data</li>
<li>The US is leader in this field</li>
<li>A big challenge of translation medicine is interoperability among different institutions</li>
<li>ELIXIR has the EBI as central hub, and then many other hubs</li>
<li>Interoperabilty is well developped at INFN in Bologna</li>
<li>A protein variant is an alterntive of a protein affected by a variatio
<ul>
<li>A variation is a change in aminoacids in a protein</li>
</ul></li>
<li>SNPs are reported in dbSNP, and SNPs associated with genetic illnesses are reported in OMIM</li>
<li>Goh was the first to propose a human disease network (diseasome)
<ul>
<li>It is a network of human diseases with edges represented by being caused by the same protein</li>
</ul></li>
<li>Many protein variants are unstable and cannot fold properly</li>
<li>By looking at neutral and disease-associated mutations, we can make some statistics
<ul>
<li>There are no sigificant differenc on which aminoacid is mutated and pathogenicity</li>
</ul></li>
<li>Casadio and Fariselli developped the residue disease index
<ul>
<li>It is a matrix of single aminoacid mutations (SAP)</li>
<li>Not all the possible mutations are observed in databases!</li>
<li>The probability of being disease-related for each observed mutation is reported</li>
<li>They results were quite surprising since mutations among similar residues sometimes were more likely to cause disease than mutation to completely different residues</li>
</ul></li>
<li>ProTherm is a discontinued project to measure the <eq env="math">\Delta\Delta{G}</eq> of folding of protein variants
<ul>
<li>It is a mess to measure many of them experimentally</li>
</ul></li>
<li>Casadio was using ProTherm to create another index, the perturbation probability index
<ul>
<li>It is the probability of perturbing protein stability (either increasing or decreasing it)</li>
<li>They observed that disease-related variations not necessarily are linked to perturbations in protein stability</li>
<li>There is nonetheless a good (0.88) linear correlation among the 2</li>
</ul></li>
</ul>
<h2 id="predicting-the-pathogenicity-of-protein-variants">Predicting the Pathogenicity of Protein Variants</h2>
<ul>
<li>They created an SVM system for the prediction of the pathogenicity of protein variants (SNP&amp;GO)</li>
<li>They included also GO terms of the protein in the SVM implementation</li>
<li>The information content of a GO term is inversely proportional to the distance from the GO root</li>
<li>Barabasi grouped diseases by tissue and formed a network of these disese
<ul>
<li>Each node is a disease (grouped by tissue to avoid naming inconsistencies)</li>
<li>Connections are shared proteins</li>
</ul></li>
</ul>
<h1 id="part-2---prof.-savojardo">Part 2 - Prof. Savojardo</h1>
<h2 id="introduction-to-the-project">Introduction to the Project</h2>
<ul>
<li>Here we will extend our practice in functional annotation with a project</li>
<li>We will compare the GOR method and SVM for the prediction of secondary structure form sequence</li>
<li>We will be provided with data (fasta sequences and dssp assignments) and CV splits</li>
<li>We will need to generate a testing set from the PDB database</li>
<li>We will prepare our data and implement the GOR and SVM predictors</li>
<li>We will discuss critically the results and write a manuscript</li>
<li>The manuscript should be compliant with the OUP Bioinformatics journal guidelines</li>
</ul>
<h2 id="virtual-machine-configuration">Virtual Machine Configuration</h2>
<ul>
<li>All the computationally-intensive tasks will be done in a VM that they will provide
<ul>
<li>The VM has 2 cores, 8 Gb of RAM, 50 Gb of HDD</li>
<li>All the software needed will be available in a conda environment</li>
</ul></li>
<li>The VMs will be accessed through a VPN and SSH</li>
<li>When you close a terminal, you also close all of its child processes</li>
<li>In order to let computationally-intensive tasks run while disconnected from the VM we will use screen
<ul>
<li>It is possible also to use nohup or at</li>
</ul></li>
<li>Screen is a terminal multiplexer: you can start a screen session and then into it start as many virtual terminals as you want</li>
<li>Screen sessions are persistent when the terminal that originated them is closed</li>
<li>A new screen session is created with the command <code>screen</code>, which also creates a window inside the session and starts a shell into it</li>
<li>To detach from the current session I press <code>Ctrl+a d</code></li>
<li>I can list detached sessions with <code>screen -ls</code></li>
<li>I can reattach a detached session with <code>screen -r</code></li>
<li>I can create a new window in the same session with <code>Ctrl+a c</code></li>
<li>I can list all the windows in the current session with <code>Ctrl+a "</code></li>
<li>I can show an help with <code>Ctrl+a ?</code></li>
</ul>
<h2 id="dssp">DSSP</h2>
<ul>
<li>DSSP is both used to indicate the software and a databae of SS assignments</li>
<li>DSSP is not a predictor, it uses an algorithm to assign a secondary structure</li>
<li>It is run with the command <code>mkdssp</code>, and by defaults it outputs to STDOUT (I can specify an output file with the <code>-o</code> parameter)</li>
<li>The output is a fixed-width flat-file that can be parsed by extracting substrings</li>
</ul>
<pre><code>HEADER    HYDROLASE   (SERINE PROTEINASE)         17-MAY-76   1EST
...
  240  1  4  4  0 TOTAL NUMBER OF RESIDUES, NUMBER OF CHAINS,
                  NUMBER OF SS-BRIDGES(TOTAL,INTRACHAIN,INTERCHAIN)                .
 10891.0   ACCESSIBLE SURFACE OF PROTEIN (ANGSTROM**2)
  162 67.5   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)--&gt;H-N(J)  ; PER 100 RESIDUES
    0  0.0   TOTAL NUMBER OF HYDROGEN BONDS IN     PARALLEL BRIDGES; PER 100 RESIDUES
   84 35.0   TOTAL NUMBER OF HYDROGEN BONDS IN ANTIPARALLEL BRIDGES; PER 100 RESIDUES
...
   26 10.8   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)--&gt;H-N(I+2)
   30 12.5   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)--&gt;H-N(I+3)
   10  4.2   TOTAL NUMBER OF HYDROGEN BONDS OF TYPE O(I)--&gt;H-N(I+4)
...
  #  RESIDUE AA STRUCTURE BP1 BP2  ACC   N-H--&gt;O  O--&gt;H-N  N-H--&gt;O  O--&gt;H-N
    2   17   V  B 3   +A  182   0A   8  180,-2.5 180,-1.9   1,-0.2 134,-0.1
                                   ...Next two lines wrapped as a pair...
                                    TCO  KAPPA ALPHA  PHI   PSI    X-CA   Y-CA   Z-CA
                                  -0.776 360.0   8.1 -84.5 125.5  -14.7   34.4   34.8
                                   ...Next two lines wrapped as a pair...
                                               CHAIN AUTHCHAIN
                                                   A         A
....;....1....;....2....;....3....;....4....;....5....;....6....;....7..
    .-- sequential resnumber, including chain breaks as extra residues
    |    .-- original PDB resname, not nec. sequential, may contain letters
    |    | .-- one-letter chain ID, if any
    |    | | .-- amino acid sequence in one letter code
    |    | | |  .-- secondary structure summary based on columns 19-38
    |    | | |  | xxxxxxxxxxxxxxxxxxxx recommend columns for secstruc details
    |    | | |  | .-- 3-turns/helix
    |    | | |  | |.-- 4-turns/helix
    |    | | |  | ||.-- 5-turns/helix
    |    | | |  | |||.-- geometrical bend
    |    | | |  | ||||.-- chirality
    |    | | |  | |||||.-- beta bridge label
    |    | | |  | ||||||.-- beta bridge label
    |    | | |  | |||||||   .-- beta bridge partner resnum
    |    | | |  | |||||||   |   .-- beta bridge partner resnum
    |    | | |  | |||||||   |   |.-- beta sheet label
    |    | | |  | |||||||   |   ||   .-- solvent accessibility
    |    | | |  | |||||||   |   ||   |
  #  RESIDUE AA STRUCTURE BP1 BP2  ACC
    |    | | |  | |||||||   |   ||   |
   35   47 A I  E     +     0   0    2
   36   48 A R  E &gt;  S- K   0  39C  97
   37   49 A Q  T 3  S+     0   0   86
   38   50 A N  T 3  S+     0   0   34
   39   51 A W  E &lt;   -KL  36  98C   6</code></pre>
<ul>
<li>The first column contains an internal residue identifier (different from the PDB one)
<ul>
<li>It contains <code>!</code> when a chain break is detected by dssp itself (because 2 successive <eq env="math">C_\alpha</eq> are too far from each other)</li>
<li>It contains <code>!*</code> when there is a chin break in the PDB file</li>
</ul></li>
<li>DSSP produces 8 different SS types, that then are usually reduced to H, E and C (externally to dssp)</li>
<li>Typical mapping is
<ul>
<li>H, G, I <eq env="math">\to</eq> H</li>
<li>B, E <eq env="math">\to</eq> E (sometimes B is mapped to C to avoid short strands)</li>
<li>T, S, ” ” <eq env="math">\to</eq> C</li>
</ul></li>
<li>The last column is the absolute solvent accessibility
<ul>
<li>Note that the calculation ignores HETATM and unusual residues, so I can get unexpectedly large valuues when these are present</li>
<li>If I have an oligomer the accessibility is returned for the entire assembly, so it neglects the interaction surface!
<ul>
<li>Extract the chains first if you want the accessibility of the monomer</li>
</ul></li>
</ul></li>
<li>Residues in a disulphide bridge are reported with the same lowercase letter</li>
</ul>
<h2 id="dataset">Dataset</h2>
<ul>
<li>Our dataset was the one used for training Jpred4, one of the most recent SS prediction methods</li>
<li>The starting set contained 1987 representative domain sequences from each 2.04 SCOP superfamily
<ul>
<li>They did like this to exclude obvios sequence similarities</li>
</ul></li>
<li>They filtered out the set to 1497 proteins by removing
<ul>
<li>Proteins with a structure worse than <eq env="math">2.5 \AA</eq></li>
<li>Sequences shorter than 30 residues (they cannot contain a domain) or longer than 800 (to avoid long Blast runs)</li>
<li>Missing dssp assignments for more than 9 residues consecutively</li>
<li>Other filters</li>
</ul></li>
<li>They split the dataset in a training set (1348 sequences) and a blind test set (149 sequences)</li>
<li>We will use only the train split, and we will build our own test set</li>
<li>We will need to produce some statistics on the dataset in the paper</li>
</ul>
<h2 id="data-visualization">Data Visualization</h2>
<ul>
<li>Barplots are used for visualizing a quantitative and a qualitative variable</li>
<li>Histograms are used with 1 quantitative variable discretized in bins
<ul>
<li>The dependent variable is the frequency of each bin</li>
<li>The size of the bin is essential for good visualization</li>
</ul></li>
<li>Density plots are similar to histograms but do not use bins
<ul>
<li>They use kernel smoothing for plotting probability densities</li>
<li>They are better than histograms since they are not influenced by bin size</li>
<li>A gaussian kernel interpolates a series of gaussians centered into each datapoint</li>
</ul></li>
<li>Heatmaps visualize trivariate data with 2 independent variables</li>
</ul>
<h2 id="sequence-profiles">Sequence Profiles</h2>
<ul>
<li>PsiBLAST is an iterative algorithm that searches a profile against a database using multiple BLAST interations
<ul>
<li>BLAST does a single iteration using PAM or BLOSUM matrices</li>
<li>PsiBLAST does a first standard BLAST interation</li>
<li>From the result, it builds a position-specific scoring matrix (PSSM)</li>
<li>A PSSM is obtained from a profile by applaying a log ratio against a null model</li>
<li>Instead of being a mtrix of all-against all residues like BLOSUM, PSSM is a matrix of likelihood of each residue in each position of the alignment</li>
<li>It iterates using the PSSM as a scoring matrix (and rebuilds the PSSM)</li>
<li>It stops after a fixed number of iterations (3 or 4) or at convergence</li>
<li>In the checkpoint file PSiBLAST returns the last PSSM and a profile</li>
<li>The profile is calculated using sequence weighting
<ul>
<li>Sequences are clustered by identity (60-70%) and their weight in the profile is the reciprocal of the cluster size</li>
</ul></li>
</ul></li>
<li>Phmmer and Jackhmmer use HMMs, and Jackhmmer is the iterative version (similar to PsiBLAST)</li>
<li>HHBlits performs HMM-HMM alignments</li>
<li>We obtained the profiles for our sequences (trai nand test) by doing PSI-BLAST against SwissProt (usually UniRef90 is used but it would take forever)
<ul>
<li>We obtained the profiles from the checkpoint file</li>
</ul></li>
</ul>
<h2 id="gor-method">GOR Method</h2>
<ul>
<li>SS prediction is basically a solved problem since we are at 90% accuracy</li>
<li>SS can be used to align distantly related sequences</li>
<li>Some other third generation SS preditction methods
<ul>
<li>PSIPRED uses 2 feed-forward NNs that process PSIBlast output and uses dynamic programmming to refine the NN output (remove unacceptable assignments)</li>
</ul></li>
<li>GOR differently from the Chou-Fasman uses a sliding window</li>
<li>Training means creating 3 propensity matrices for each SS conformation</li>
<li>We will use a sliding window applied to a profile</li>
<li>The propensity for a reside type for a given conformation is calculated as the log ratio of the joint probability of the residue and the SS and the product of the marginal probabilities</li>
<li>The log ratio is positive if the joint probability is higher than what expected from the marginal probabilities</li>
<li>For each positon, the assigned conformation is the one with the highest value of the information function</li>
<li>Using a sliding window I just sum all the information functions for each position in order to assign the conformation of the central residue
<ul>
<li>NOTE: I have a different training table for each SS and for each position in the sliding window!</li>
<li>NOTE2: I am assuming that each position in the sliding window is independent</li>
<li>I need to do like this since for many sliding window configurations I do not have any count!</li>
<li>This is a biologically wrong assumption, hence the lower performances of GOR</li>
<li>The ends of the sequence can be dealt with by padding with 0s</li>
<li>The independence assumption makes the GOR be a linear model (it is a linear combination of information functions)</li>
</ul></li>
</ul>
