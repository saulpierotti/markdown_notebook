<h1 id="introduction-and-topics">Introduction and topics</h1>
<ul>
<li>Linear algebra is one of the main topics for developing algorithms</li>
<li>Biology ha a high level of noise in its data</li>
<li>I should know the formula of binomial and normal distribution</li>
<li>We will study tests like T-Student, ANOVA</li>
<li>Slides: ww.biocomp.unibo.it/gigi/2019-2020/ECB</li>
</ul>
<h1 id="linear-algebra">Linear algebra</h1>
<h2 id="vectors">Vectors</h2>
<ul>
<li>Given a reference system, a vector is represented by its components on the axes</li>
<li>The span of n vectors is the set of all possible vectors that you can represent by their linear combinations</li>
<li>If a vector can be expressed as a linear combination of another, it is said to be linear dependent from it</li>
<li>The basis of a vector space is a set of linearly independent vectors that span the full space</li>
<li><eq env="math">\vec{x} \in \mathbb{R}^n</eq> means x is a real vector in an n-dimensional space</li>
<li><eq env="math">\vec{x} \in \mathbb{C}^n</eq> means x is a complex vector in an n-dimensional space</li>
<li>Sum of vectors is done by summing their components or graphically with the parallelogram rule
<ul>
<li><eq env="math">\vec{c} = \vec{a} + \vec{b} \implies c_i = a_i + b_i \:\: \forall \:\: i = 1 \to n</eq></li>
<li>Difference is the same concept</li>
<li><eq env="math">\forall \: \vec{v} \:\:\exists \:\: \vec{0} \: : \: \vec{v} + \vec{0} = \vec{v}</eq></li>
<li>You can only sum vectors in the same vector space</li>
</ul></li>
<li>The norm of a vector <eq env="math">||\vec{v}||</eq> is its length
<ul>
<li>Can be computed with the pitagorean theorem <eq env="math">||\vec{v}||=\sqrt{\sum_{i=1}^{n}{v_i^2}}</eq></li>
<li>The norm of the sum is less or equal to the sum of the norm of the components
<ul>
<li>This follows from the geometry of a triangle</li>
</ul></li>
<li>The scalar product of a norm is the norm of the scalar product
<ul>
<li><eq env="math">\lambda ||\vec{v}||=||\lambda \vec{v}||</eq></li>
</ul></li>
</ul></li>
<li>The distance between points in space is the norm of the difference between the vectors defining the points
<ul>
<li><eq env="math">d(a,b)=||\vec{a}-\vec{b}||</eq></li>
</ul></li>
<li>Scalar multiplication
<ul>
<li><eq env="math">\vec{c} = \lambda \vec{a} \implies c_i = a_i \lambda</eq></li>
<li>A scalar multiplication of a sum is the sum of the scalar multiplications of the components</li>
</ul></li>
<li>Dot product, also called scalar or inner product
<ul>
<li>You can use the notation <eq env="math">&lt;A,B&gt;</eq></li>
<li>It is used in physics to calculate work</li>
<li><eq env="math">\vec{w}=||\vec{F}||*||\vec{s}||*cos{\theta}=\sum_{i=1}^{n}{F_i*s_i}</eq></li>
<li>It is a number, complex or real depending on the vectors (!)</li>
<li>It is commutative and distributive</li>
<li><eq env="math">&lt;x,x&gt; = ||\vec{x}||^2</eq></li>
<li>It is positive when the angle is acute</li>
<li>No cancellation rule
<ul>
<li><eq env="math">&lt;A,B&gt;=&lt;A,C&gt; \: \: \not\!\!\!\implies \vec{B}=\vec{C}</eq></li>
</ul></li>
</ul></li>
<li>Angle between vectors
<ul>
<li>Can be calculated inverting the dot product</li>
</ul></li>
<li>A line passing through the origin can be defined as the set of points orthogonal to a vector <eq env="math">\vec{w}</eq>
<ul>
<li><eq env="math">w_1x_1+w_2x_2=0</eq></li>
<li>In higher dimensions this describes an hyperplane (an n-1 dimensional object)</li>
</ul></li>
<li>All the point on a hyperplane have the same projection on its defining vector <eq env="math">\vec{w}</eq>
<ul>
<li>The projection p of <eq env="math">\vec{x}</eq> on <eq env="math">\vec{w}</eq> is calculated as <eq env="math">\vec{x}*cos\theta</eq></li>
<li>An hyperplane is therefore an object subjected to the constraint <eq env="math">\vec{x}*cos\theta=p</eq></li>
<li>Given that <eq env="math">&lt;\vec{x},\vec{w}&gt;=||\vec{x}||*||\vec{w}||*cos \theta</eq> we have that <eq env="math">p=\frac{&lt;\vec{x},\vec{w}&gt;}{||\vec{w}||}</eq></li>
<li>If p&gt;0 the hyperplane is in the direction of <eq env="math">\vec{w}</eq>, if it is negative it is in the opposite direction</li>
<li>Defining <eq env="math">b=-\frac{p}{||\vec{w}||}</eq> we have the canonical equation for the hyperplane
<ul>
<li><eq env="math">w_1x_1+w_2x_2+b=0</eq> in 2 dimensions</li>
<li><eq env="math">&lt;\vec{w},\vec{x}&gt;+b=W^tX+b=0</eq> in n dimensions</li>
</ul></li>
<li>An hyperplane is useful for subdividing space</li>
</ul></li>
<li>2 hyperplanes are parallel if their are defined by the same vector <eq env="math">\vec{w}</eq> allowing for a scaling factor <eq env="math">\lambda</eq>
<ul>
<li><eq env="math">&lt;Y,W&gt;=\lambda&lt;X,W&gt;</eq></li>
</ul></li>
<li>The distance between parallel hyperplanes is computed as the difference of their projections on <eq env="math">\vec{w}</eq>
<ul>
<li><eq env="math">d(X,Y)=p_y-p_x=\frac{b_x-b_y}{||W||}</eq></li>
</ul></li>
<li>The distance of a point A from a hyperplane is the projection of the point on the defining vector <eq env="math">\vec{w}</eq>, minus the projection of the hyperplane on the same vector
<ul>
<li><eq env="math">D(A,X)=p_a-p_x=\frac{&lt;A,W&gt;+b}{||W||}</eq></li>
</ul></li>
<li>Hyperplanes are useful for the separation of classes of data</li>
<li>Every column of a matrix can be thought of as a vector
<ul>
<li>To make the dot product of 2 vectors using matrices you can multiply one vector for the transpose of the second</li>
<li><eq env="math">&lt;\vec{a}, \vec{b}&gt;=A*B^t</eq></li>
</ul></li>
</ul>
<h2 id="matrices">Matrices</h2>
<ul>
<li>A matrix is an array of numbers arranged in a rectangular structure</li>
<li>The columns of a matrix are the coordinates where the basis vectors land after the transformation</li>
<li>It has m rows and n columns, it is represented as <eq env="math">A\in \mathbb{R}^{m*n}</eq>
<ul>
<li><eq env="math">A=\begin{pmatrix}a_{11}&amp;a_{12}&amp;...&amp;a_{1n}\\a_{21}&amp;a_{22}&amp;...&amp;a_{2n}\\...&amp;...&amp;...&amp;...\\a_{m1}&amp;a,_{m2}&amp;...&amp;a_{mn}\end{pmatrix}</eq></li>
<li>The single <eq env="math">a_{ij}</eq> numbers are called elements</li>
<li>The index of an element is always mn, meaning first row and then column</li>
</ul></li>
<li>If n=1, the matrix is called column matrix, which is a vector</li>
<li>If m=1, the matrix is a row matrix</li>
<li>A and B are equal if they have the same dimensions and they are equal element by element
<ul>
<li><eq env="math">A=B\: \iff \: a_{ij}=b_{ij}</eq></li>
</ul></li>
<li>The 0 matrix contains all 0 elements and does not change the matrix it is added to</li>
<li>The sum is defined as the sum of the respective elements
<ul>
<li>We can sum only matrices of the same dimensions, they are said to be conformable for addition</li>
<li><eq env="math">C=B+A\: \iff \:c_{ij}=b_{ij}+a_{ij}</eq></li>
<li>The difference operates in the same way</li>
</ul></li>
<li>Scalar multiplication is performed multiplying all the elements of the matrix for the scalar
<ul>
<li><eq env="math">C=\lambda A</eq> implies <eq env="math">c_{ij}=\lambda a_{ij}</eq></li>
</ul></li>
<li>The negative of A is -A, defined as <eq env="math">-1*A</eq>
<ul>
<li><eq env="math">A-A=0</eq></li>
</ul></li>
<li>Matrix addition and scalar multiplication are commutative, associative and distributive</li>
<li>Matrix product is an operation that is defined only if the number of columns of the first matrix is equal to the number of rows of the second (the matrices are conformable for the product)
<ul>
<li>A is of dimensions <eq env="math">m*p</eq> and B of dimensions <eq env="math">p*n</eq>, if <eq env="math">C=A*B</eq></li>
<li><eq env="math">c_{ij}=\sum_{k=1}^{p}a_{ik}b_{kj}</eq></li>
<li><eq env="math">C=A*B</eq> can be computed as row by column product</li>
<li>It can be defined only if the number of columns in the first matrix is equal to the number of rows of the second
<ul>
<li><eq env="math">R^{m*p}*R^{p*n}\implies R^{m*n}</eq></li>
</ul></li>
<li>The result is a matrix with the same number of rows as the first, and the same number of columns as the second</li>
<li>The product between matrices is NOT commutative (!)</li>
<li><eq env="math">A(B+C)=AB+AC</eq></li>
<li><eq env="math">(A+B)C=AC+BC</eq></li>
<li><eq env="math">A(BC)=(AB)C</eq></li>
<li>Be aware!
<ul>
<li>If <eq env="math">AB=0</eq> we can NOT conclude that B or C are 0</li>
<li>If <eq env="math">AB=AC</eq> we can NOT conclude that <eq env="math">B=C</eq></li>
</ul></li>
</ul></li>
<li>A square matrix has m=n</li>
<li>An upper triangular matrix has all the elements below the diagonal equal to 0, and a lower triangular the ones above it</li>
<li>A diagonal matrix has all the elements outside the diagonal equal to 0</li>
<li>A diagonal matrix with all 1 elements is the identity matrix I
<ul>
<li>It does not change the square matrix it is multiplied to</li>
<li>In this case, <eq env="math">AI=IA=A</eq></li>
</ul></li>
<li>If AB=BA, A and B are said to commute
<ul>
<li>If A is a square matrix, it commutes with itself and with I</li>
</ul></li>
<li>If AB=-BA, A and B are said to anti-commute</li>
<li>The transposition of a <eq env="math">n*m</eq> matrix is a <eq env="math">m*n</eq> matrix, called <eq env="math">A^t</eq>, where <eq env="math">[A^t]_{ij}=A_{ji}</eq>
<ul>
<li>A and <eq env="math">A^t</eq> are always conformable to product, in both directions</li>
<li><eq env="math">(A^t)^t=A</eq></li>
</ul></li>
<li>A square matrix is symmetric if <eq env="math">A=A^t</eq>, antisymmetric (skew-symmetric) if <eq env="math">A=-A^t</eq>
<ul>
<li><eq env="math">A+A^t</eq> is always symmetric</li>
<li><eq env="math">A-A^t</eq> is always antisymmetric</li>
<li>An antisymmetric matrix has a 0 diagonal and antisymmetrical elements otherwise</li>
</ul></li>
<li>The inverse of a matrix A, called <eq env="math">A^{-1}</eq>, is a matrix such that <eq env="math">A*A^{-1}=A^{-1}*A=I</eq>
<ul>
<li>It is defined only if <eq env="math">det(A)\not=0</eq></li>
</ul></li>
<li>An orthogonal matrix has its inverse equal to the transpose, <eq env="math">A^{-1}=A^t</eq>
<ul>
<li>An orthogonal matrix describes a spatial rotation</li>
<li>Therefore, <eq env="math">AA^t=A^tA=I</eq>
<ul>
<li>You can check for orthogonality by checking that <eq env="math">A*A^t=I</eq></li>
</ul></li>
</ul></li>
<li>Some properties of transpose and inverse matrices
<ul>
<li><eq env="math">(AB)^{-1}=B^{-1}*A^{-1}</eq>, but only if <eq env="math">(AB)^{-1}</eq> exists(!)</li>
<li><eq env="math">(AB)^t=B^t*A^t</eq></li>
</ul></li>
<li>It is possible to associate a number called determinant to any square matrix
<ul>
<li><eq env="math">det(A)=|A|\in \mathbb{R}</eq></li>
<li>For an order 2 square matrix, that is computed subtracting the product of the second diagonal to that of the first
<ul>
<li><eq env="math">det(A)=a_{11}*a_{22}-a_{12}*a_{21}</eq></li>
</ul></li>
<li>It represents the area of the unit square after the transformation</li>
<li>Its sign reflects the orientation of space
<ul>
<li>If it is negative, the transformation flips the axis</li>
</ul></li>
</ul></li>
<li>The rank of a transformation is the dimensionality of its output space, called column space
<ul>
<li>The column space of a transformation is the span of the basis vectors defined by its columns</li>
</ul></li>
<li>Some proprieties of determinants
<ul>
<li>If an entire row or column is equal to 0, then the determinant of the matrix is 0</li>
<li><eq env="math">det(A*B)=det(A)*det(B)</eq></li>
<li>The determinant of an orthogonal matrix is either 1 or -1</li>
<li><eq env="math">det(A)=det(A^t)</eq></li>
</ul></li>
<li>How to compute the inverse of 2*2 matrices
<ul>
<li>Given the definition <eq env="math">A*A^{-1}=I</eq> if <eq env="math">det(A)\not=0</eq></li>
<li>It follows <eq env="math">A^{-1}=\frac{1}{det(A)} \begin{pmatrix}  a_{22} &amp; -a_{12}\\  -a_{21} &amp; a_{11}  \end{pmatrix}</eq></li>
</ul></li>
<li>A minor <eq env="math">M_{ij}</eq> of a matrix A is the determinant of any square submatrix of A</li>
<li>The cofactor of the element <eq env="math">a_{ij}</eq> is <eq env="math">C_{ij}:C_{ij}=M_{ij}*(-1)^{i+j}</eq></li>
<li>To compute the determinant of any matrix you pick any row or column and sum the product of any element in it for its cofactor
<ul>
<li>In a column <eq env="math">det(A)=\sum\limits_{i=1}^n a_{ij}*C_{ij}</eq></li>
<li>In a row <eq env="math">det(A)=\sum\limits_{j=1}^n a_{ij}*C_{ij}</eq></li>
<li>It is convenient to choose the row or column with most 0 for the computation</li>
<li>If 2 rows are identical, det(A)=0</li>
<li>If one row is 0, then det(A)=0</li>
<li>If you exchange 2 rows, det(A’)=-det(A)</li>
<li>The determinant of a triangular matrix is the product of the diagonal elements</li>
<li>If B is obtained by multiplying every element in a row of A by <eq env="math">\lambda</eq>, <eq env="math">det(B)=\lambda det(A)</eq></li>
<li>For any n*n square matrix, <eq env="math">det(\lambda A)=\lambda^n det(A)</eq></li>
<li>If A and B are of the same order, <eq env="math">det(AB)=det(A)det(B)</eq></li>
</ul></li>
<li>The cofactor matrix of A, called <eq env="math">A^c</eq>, is a matrix with each element equal to the cofactor of the same element in a
<ul>
<li><eq env="math">A^c:a^c_{ij}=C_{ij}</eq></li>
</ul></li>
<li>The adjugate matrix of A is the transpose of its cofactor matrix
<ul>
<li><eq env="math">A^a=(A^c)^t</eq></li>
</ul></li>
<li>The inverse matrix can be obtained by dividing the adjugate of a matrix for its determinant
<ul>
<li><eq env="math">A^{-1}=\frac{1}{|A|}A^a</eq></li>
</ul></li>
<li>Matrices can represent systems of linear equations
<ul>
<li>The system <eq env="math">\begin{cases}x+y=7\\3x-y=5\end{cases}</eq> can be represented as <eq env="math">\begin{pmatrix}1&amp;1\\3&amp;-1\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}7\\5\end{pmatrix}</eq></li>
<li>The system has a solution if the coefficient matrix is invertible</li>
</ul></li>
</ul>
<h2 id="linear-transformations">Linear transformations</h2>
<ul>
<li>A matrix can be thought of as a linear transformation of a vector space
<ul>
<li><eq env="math">A^{m*n}:R^n</eq> → <eq env="math">R^m</eq></li>
<li>A linear transformation is a transformation that preserves linearity and does not move the origin
<ul>
<li><eq env="math">A(\vec{v}+\vec{u})=A\vec{v}+A\vec{u}</eq> and <eq env="math">A(\lambda\vec{v})=\lambda A\vec{v}</eq></li>
</ul></li>
</ul></li>
<li>A rotation by an angle <eq env="math">\theta</eq> can be describe by the transformation
<ul>
<li><eq env="math">A=\begin{pmatrix}\cos \theta&amp;-\sin \theta\\\sin \theta&amp;\cos \theta\end{pmatrix}</eq></li>
</ul></li>
<li>The inverse transformation takes a transformed vector and restores the original one
<ul>
<li>Sometimes it does not exist (!) when det(A)=0</li>
</ul></li>
<li>If det(A)=0 the transformation squishes space to a lower-dimensional vector space</li>
<li>The composition of the transformations A followed by B is <eq env="math">C=BA\not = AB</eq></li>
<li>The scalar product of the transformation of a vector <eq env="math">A\vec{v}</eq> and the vector <eq env="math">\vec{w}</eq> is equal to the scalar product of the first vector with the second vector transformed by the transpose of A
<ul>
<li><eq env="math">A\vec{v}*\vec{w}=\vec{v}*A^t\vec{w}</eq></li>
</ul></li>
<li>The null space of a transformation is the set of vectors that get squished to <eq env="math">\vec{0}</eq> by the transformation
<ul>
<li><eq env="math">\vec{b}\in Null(A) \iff A\vec{b}=\vec{0}</eq></li>
<li>A trivial null space is always <eq env="math">\vec{0}</eq> itself</li>
<li>There is a true null space only if <eq env="math">det(A)=0</eq></li>
<li>If <eq env="math">det(A)\not=0</eq>, the only null space is <eq env="math">\vec{0}</eq> itself</li>
</ul></li>
<li>The null space of a square matrix can be computed setting up a linear system of equations
<ul>
<li>For a matrix <eq env="math">A=\begin{pmatrix}1&amp;2\\2&amp;4\end{pmatrix}</eq>, <eq env="math">det(A)=0</eq></li>
<li><eq env="math">A\vec{b}=\vec{0} \implies \begin{pmatrix}1&amp;2\\2&amp;4\end{pmatrix} \begin{pmatrix}b_1\\b_2\end{pmatrix}=\begin{pmatrix}0\\0\end{pmatrix}\implies \begin{cases}b_1+2b_2=0\\2b_1+4b_2=0\end{cases} \implies b_1=-2b_2 \implies \vec{b}=\lambda \begin{pmatrix}-2\\1\end{pmatrix}</eq></li>
</ul></li>
</ul>
<h2 id="eigenstuff">Eigenstuff</h2>
<ul>
<li>For the transformation A, if <eq env="math">A\vec{b}=\lambda\vec{b}</eq>, <eq env="math">\vec{b}</eq> is an eigenvector of A and <eq env="math">\lambda</eq> is its eigenvalue
<ul>
<li>An eigenvector of a transformation is a vector that is only rescaled by the transformation</li>
<li>An eigenvalue is the scaling factor to which the vector is subjected by the transformation</li>
<li>The <eq env="math">\vec{0}</eq> vector can never be an eigenvector even though <eq env="math">A\vec{0}=\lambda\vec{0}</eq> is always true
<ul>
<li>On the contrary, it is possible that <eq env="math">\lambda=0</eq></li>
</ul></li>
</ul></li>
<li>How to find eigenvalues for the matrix A
<ul>
<li><eq env="math">A\vec{b}=\lambda\vec{b} \implies A\vec{b}-\lambda\vec{b}=0 \implies (A-\lambda I)\vec{b}=0</eq></li>
<li>This means that the eigenvectors <eq env="math">\vec{b}</eq> are the non-trivial null space of the transformation <eq env="math">(A-\lambda I)</eq></li>
<li>It is required that <eq env="math">det(A-\lambda I)=0</eq>, otherwise there are no eigenvectors</li>
<li>The equation <eq env="math">det(A-\lambda I)=0</eq> is called characteristic equation of A and its root allows to recover the eigenvalues of the matrix</li>
</ul></li>
<li>How to find the eigenvectors for the matrix A
<ul>
<li>Once I have the eigenvalues <eq env="math">\lambda</eq>, the eigenvectors can be found by solving <eq env="math">(A-\lambda I)\vec{b}=0</eq> for <eq env="math">\vec{b}</eq>, using all the <eq env="math">\lambda</eq></li>
</ul></li>
<li>The number of eigenvalues and eigenvectors (families of linearly dependent eigenvectors) is equal to the dimensions of the vector space
<ul>
<li>I always have n families of eigenvectors in the vector space <eq env="math">R^n</eq></li>
<li>The families of eigenvectors are orthogonal to each other iff the transformation is symmetric</li>
<li>They define a convenient reference frame, even if they are not orthogonal</li>
<li>Each eigenvalue scales one of the families of eigenvectors, meaning that it stretches one of the dimensions of the new reference frame</li>
</ul></li>
<li>Note that in a triangular or diagonal matrix the diagonal elements are its eigenvalues</li>
<li>The product of the eigenvalues is equal to the determinant of the matrix
<ul>
<li>A non-invertible matrix (also called singular matrix) has always at least a 0 eigenvalue</li>
<li>The inverse matrix has reciprocal eigenvalues (<eq env="math">\frac{1}{\lambda}</eq>)</li>
<li>The eigenvalue of kA is <eq env="math">k\lambda</eq></li>
<li>The eigenvalue of <eq env="math">A^n</eq> is <eq env="math">\lambda^n</eq></li>
<li>Transposition does not change the eigenvalues</li>
</ul></li>
<li>The sum of the eigenvalues is the trace of the matrix
<ul>
<li>The trace of a matrix is the sum of its diagonal elements</li>
</ul></li>
</ul>
<h2 id="complex-field">Complex field</h2>
<ul>
<li>Sometimes there can be no real eigenvalues, but there are always solutions in the complex field
<ul>
<li>This happens when the characteristic equation of the matrix has <eq env="math">\Delta&lt;0</eq></li>
</ul></li>
<li>A complex number z is written as <eq env="math">z=a+ib</eq> where <eq env="math">i=\sqrt{-1}</eq>
<ul>
<li>a is called real part</li>
<li>b is called imaginary part</li>
</ul></li>
<li>The reference axis of a vector space are NOT necessarily orthogonal to each other (!)
<ul>
<li>But they must be linearly independent</li>
</ul></li>
</ul>
<h2 id="change-of-basis-and-diagonalization">Change of basis and diagonalization</h2>
<ul>
<li>To go from one system to the other we need the representation of the old basis vectors in term of the new ones
<ul>
<li><eq env="math">\hat{i}=a\hat{i'}+b\hat{j'}</eq> and <eq env="math">\hat{j}=a\hat{i'}+b\hat{j'}</eq></li>
<li>If <eq env="math">U=\begin{pmatrix}a&amp;c\\b&amp;d\end{pmatrix}</eq> we have that <eq env="math">\vec{v'}=U\vec{v}</eq></li>
<li>It is possible to go back to the original system of reference using <eq env="math">U^{-1}</eq></li>
</ul></li>
<li>If I want to use a system of which I know the coordinate of the basis vectors in term of my current basis vectors I need to use the inverse of the matrix containing these coordinates</li>
<li>If a <eq env="math">n*n</eq> matrix A has n eigenvectors which are linearly independent, I can write the <eq env="math">n*n</eq> matrix U containing all the eigenvectors, and use it to convert to a new system of reference
<ul>
<li>The eigenvectors of A will be the new basis vectors</li>
</ul></li>
<li>I can compute A in the new reference frame forming <eq env="math">\Lambda=U^{-1}AU</eq>
<ul>
<li>Given a vector <eq env="math">\vec{v}</eq>, I first convert it to the new reference frame where the eigenvectors are the basis vectors using U, then I apply A and finally I go back to the old system of reference using <eq env="math">U^{-1}</eq></li>
<li>This new matrix <eq env="math">\Lambda</eq> will be diagonal (!)</li>
<li>Each column will be made of one eigenvector multiplied by its eigenvalue</li>
<li>It is good to choose normalized vectors for the change of basis, meaning that their norm should be 1
<ul>
<li>In this case <eq env="math">det(U)=1</eq>, meaning that areas are preserved by the transformation</li>
</ul></li>
</ul></li>
<li>Why do I want to use eigenvectors as reference frames?
<ul>
<li>Because the components of any vector are only rescaled by the original transformation A in this reference frame</li>
<li>This makes much easier to compute transformations</li>
</ul></li>
<li>If a matrix is symmetric (<eq env="math">A=A^t</eq>) its eigenvalues are real and its eigenvectors are orthogonal
<ul>
<li>If the eigenvectors are normalized <eq env="math">U^{-1}=U^t</eq>, therefore <eq env="math">\Lambda=U^tAU</eq></li>
</ul></li>
<li>In the same way that a linear form can be represented as all the points orthogonal to a vector with a projection p onto it, a matrix can describe a quadratic form</li>
</ul>
<h2 id="quadratic-forms">Quadratic forms</h2>
<ul>
<li>A quadratic form is an equation in more than 1 variable were each term has a variable squared or multiplied to another variable
<ul>
<li>An example is <eq env="math">ax^2+bxy+cy^2=0</eq></li>
</ul></li>
<li>This is represented as <eq env="math">\vec{x}^tA\vec{x}+b=0</eq>, where <eq env="math">\vec{x}</eq> is the vector containing the variables, and A is a matrix of coefficients
<ul>
<li>The vector is multiplied 2 times to reflect the fact that the expression is quadratic</li>
<li>The second time the transpose is used in order to allow the product</li>
</ul></li>
<li>By rescaling A, we can obtain the standard form <eq env="math">\vec{x}^tA\vec{x}=1</eq></li>
<li>The matrix that describes a quadratic form is always symmetric
<ul>
<li>If it is not singular (non-invertible), it can be diagonalised as <eq env="math">\Lambda=U^tAU</eq></li>
<li>If <eq env="math">\vec{x'}=U^t\vec{x}</eq>, the quadratic form becomes <eq env="math">\vec{x'}^t\Lambda\vec{x'}</eq>, defined canonical form</li>
</ul></li>
<li>In the canonical form, a <eq env="math">2*2</eq> <eq env="math">\Lambda</eq> contains the eigenvalues of the transformation in the diagonal
<ul>
<li>If they are both positive, the quadratic is an ellipse
<ul>
<li>If they are equal, it is a circle</li>
</ul></li>
<li>If they are of opposite sign, it is an hyperbole</li>
<li>If they are both negative, there is no real solution</li>
</ul></li>
<li>In 3d, I can get an ellipsoid, a hyperboloid of 1 sheet or an hyperboloid of 2 sheets</li>
</ul>
<h1 id="calculus">Calculus</h1>
<h2 id="functions">Functions</h2>
<ul>
<li>Calculus is the study of functions
<ul>
<li>Functions are univocal relations between the sets domain and codomain</li>
</ul></li>
<li>The function <eq env="math">f(x)=mx+q</eq> is a line passing through <eq env="math">q</eq> at <eq env="math">x=0</eq> with slope <eq env="math">m</eq></li>
<li>The inverse of a function correlates <eq env="math">f(x)</eq> to <eq env="math">x</eq>
<ul>
<li>It is the reflection of <eq env="math">f(x)</eq> on the line <eq env="math">g(x)=x</eq></li>
</ul></li>
<li>The function <eq env="math">f(x)=a^x</eq> is an exponential
<ul>
<li>It passes through 1 at <eq env="math">x=0</eq></li>
<li><eq env="math">\lim_{x\to-\infty}f(x)=0</eq></li>
<li><eq env="math">\lim_{x\to+\infty}f(x)=+\infty</eq></li>
</ul></li>
<li>The function <eq env="math">f(x)=\log_a (x)</eq> is a logarithmic function
<ul>
<li>It passes through 1 at <eq env="math">x=0</eq></li>
<li>Common bases <eq env="math">a</eq> are 10, 2 and <eq env="math">e</eq></li>
<li><eq env="math">\lim_{x\to 0}f(x)=-\infty</eq></li>
<li><eq env="math">\lim_{x\to +\infty}f(x)=+\infty</eq></li>
<li>Logarithms are useful for performing products</li>
<li><eq env="math">\log_a(xy)=\log_a(x)+\log_a(y)</eq></li>
<li><eq env="math">\log_a(x^y)=y\log_a(x)</eq></li>
<li><eq env="math">\log_a(x)=\frac{log_b(x)}{\log_b(a)}</eq></li>
</ul></li>
<li>Trigonometric functions
<ul>
<li>The cosine is an even function because <eq env="math">\cos(\theta)=\cos(-\theta)</eq>
<ul>
<li><eq env="math">\cos(\frac{\pi}{2})=0</eq></li>
<li><eq env="math">\cos(0)=1</eq></li>
</ul></li>
<li>The sine is an odd function because <eq env="math">\sin(-\theta)=\sin(-\theta)</eq></li>
<li>Sine and cosine are periodical with a <eq env="math">2\pi</eq> period</li>
<li>The secant is the reciprocal of cosine</li>
<li>Trigonometric functions can be inverted only in a subdomain</li>
<li>They are continuous functions</li>
</ul></li>
<li>A function is continuous at a point if the limit at that point is equal to the value of the function at that same point
<ul>
<li>The composition of continuous function is a continuous function</li>
</ul></li>
<li>The intermediate value theorem: a continuous function between two points takes any possible value between them</li>
<li>Discontinuities can be removed in some cases, but essential discontinuities such as oscillating points, jumps and infinities cannot be removed</li>
</ul>
<h2 id="derivatives">Derivatives</h2>
<ul>
<li>The slope of a line is defined as <eq env="math">\frac{\Delta y}{\Delta x}</eq></li>
<li>Therefore, the slope of the secant of a function between two points <eq env="math">f(a)</eq> and <eq env="math">f(a+h)</eq> is <eq env="math">\frac{\Delta y}{\Delta x}=\frac{f(a+h)-f(a)}{h}</eq></li>
<li>If we try to reduce h as much as possible we obtain the slope of the tangent at point a
<ul>
<li><eq env="math">m=\lim_{h \to 0}\frac{f(a+h)-f(a)}{h}</eq></li>
<li>The tangent at a point is an estimation of the rate of change of the function at that point</li>
</ul></li>
<li>The derivative of a function is another function that describes its rate of change, it takes the value of the slope of the tangent of the original function at each point
<ul>
<li><eq env="math">f'(x)|_a=\lim_{h \to 0}\frac{f(a+h)-f(a)}{h}</eq></li>
</ul></li>
<li>A function to be derivable must be continuous and must have one-sided derivatives defined at the end-points
<ul>
<li>However, there are functions that are continuous but not derivable</li>
<li>Points of non-derivability are cusps, corners, discontinuities and points with vertical tangent</li>
</ul></li>
<li>Some derivatives
<ul>
<li><eq env="math">\frac{d}{dx}[a]=0</eq></li>
<li><eq env="math">\frac{d}{dx}[ax]=a</eq></li>
<li><eq env="math">\frac{d}{dx}[x^n]=nx^{n-1}</eq></li>
<li><eq env="math">\frac{d}{dx}[\cos(x)]=-sin(x)</eq></li>
<li><eq env="math">\frac{d}{dx}[\sin(x)]=cos(x)</eq></li>
<li><eq env="math">\frac{d}{dx}[e^x]=e^x</eq></li>
<li><eq env="math">\frac{d}{dx}[a^x]=\frac{d}{dx}[e^{\ln(a)x}]=\ln a*e^{\ln(a)x}=\ln(a)*a^x</eq></li>
<li><eq env="math">\frac{d}{dx}[\ln(x)]=\frac{1}{x}</eq></li>
<li><eq env="math">\frac{d}{dx}[\log_a(x)]=\frac{1}{x*\ln(a)}</eq></li>
</ul></li>
<li>Rules for derivation
<ul>
<li><eq env="math">\frac{d}{dx}[f(x)+g(x)]=f'(x)+g'(x)</eq></li>
<li><eq env="math">\frac{d}{dx}[k*f(x)]=k f'(x)</eq></li>
<li><eq env="math">\frac{d}{dx}[g(x)*f(x)]=g(x)*f'(x)+g'(x)*f(x)</eq></li>
<li><eq env="math">\frac{d}{dx}[g(f(x))]=\frac{dg}{df}*\frac{df}{dx}</eq></li>
<li><eq env="math">\frac{d}{dx}[\frac{g(x)}{f(x)}]=\frac{g(x)*f'(x)+g'(x)*f(x)}{f(x)^2}</eq></li>
</ul></li>
<li>Higher order derivatives are computed as the derivative of the derivative
<ul>
<li>For the second derivative of f(x) we write <eq env="math">f''(x)=\frac{d(df/dx)}{dx}=\frac{d^2f}{dx^2}</eq></li>
<li>In the same way, the third derivative <eq env="math">f'''(x)=\frac{d^3f}{dx^3}</eq> and so on for higher orders</li>
</ul></li>
<li>The second derivative reflects the convexity of the function
<ul>
<li>It is the rate at which the slope of the tangent increases</li>
</ul></li>
<li>Derivatives can help to study the behavior of a function</li>
<li>In a function there are global and local maxima and minima, defined as extremes
<ul>
<li>There are NOT methods to compute global extrema, but only local ones</li>
</ul></li>
<li>A local extreme is referred to an open interval
<ul>
<li>The derivative at that point is 0
<ul>
<li>This is NOT sufficient, it can also be a flexus</li>
</ul></li>
<li>A minimum has a derivative with positive slope when it intersects the x axis
<ul>
<li>In other words, the second derivative is positive</li>
</ul></li>
<li>A maximum has a derivative with negative slope when it intersects the x axis
<ul>
<li>In other words, the second derivative is negative</li>
</ul></li>
</ul></li>
<li>A critical point of a function is a point where the derivative is 0 or undefined</li>
</ul>
<h2 id="integrals">Integrals</h2>
<ul>
<li>We can find the area under a curve f(x) by adding rectangles with height f(x) and width dx, in what is called a Riemann sum
<ul>
<li>The width dx is also called subinterval</li>
<li>The area of each rectangle will be then <eq env="math">A|_x=f(x)*dx</eq></li>
<li>If we sum the area of all the rectangles while letting dx be as small as possible we obtain a new function F(x) called integral of f(x), which for any x gives the area under f(x) from <eq env="math">-\infty</eq> to that point
<ul>
<li><eq env="math">\lim_{dx \to 0 }\sum_i f(x_i)*dx_i=\int f(x) dx=F(x)</eq></li>
</ul></li>
</ul></li>
<li>The derivation process is insensitive to constants, so we have a family of integrals for any given function, that differ by a constant
<ul>
<li><eq env="math">\int f(x) dx =F(x)+c \:\:\: \forall \: c \in \mathbb{R}</eq></li>
</ul></li>
<li>The difference of the integral F(x) evaluated at point b and a is the area under f(x) between the point b and a
<ul>
<li><eq env="math">AUC|_a^b f(x)=\int_a^bf(x)=F(b)-F(a)</eq></li>
<li>The portion of f(x) between a and b is called partition</li>
</ul></li>
<li>The fundamental theorem of calculus: the integral of a derivative of a function is the function itself
<ul>
<li><eq env="math">F(x)=\int_{a}^{b}f(x)dx=\lim_{h\to 0} \sum_{k=1}^n(f(x)*h)</eq></li>
</ul></li>
<li>The areas computed by integrals have a sign (!)
<ul>
<li>They are positive above the x axis, negative below it</li>
</ul></li>
<li>Some integrals
<ul>
<li><eq env="math">\int a \:dx = ax+c</eq></li>
<li><eq env="math">\int x^n dx= \frac{1}{n+1}x^{n+1}+c</eq></li>
<li><eq env="math">\int \frac{1}{x}dx=\ln(|x|)+c</eq></li>
<li><eq env="math">\int e^x dx = e^x+c</eq></li>
<li><eq env="math">\int a^x dx = \frac{1}{\ln(a)}a^x+c</eq></li>
<li><eq env="math">\int \sin(x)dx=-\cos(x) +c</eq></li>
<li><eq env="math">\int \cos(x)dx=\sin(x) +c</eq></li>
</ul></li>
<li>Rules for integration
<ul>
<li><eq env="math">\int_a^bf(x)dx=-\int_b^af(x)dx</eq></li>
<li><eq env="math">\int_a^af(x)dx=0</eq></li>
<li><eq env="math">\int k * f(x)dx=k*\int f(x)dx</eq></li>
<li><eq env="math">\int_a^bf(x)dx+\int_b^cf(x)dx=\int_a^c f(x)dx</eq></li>
<li><eq env="math">\int [f(x)+g(x)]dx=\int f(x)dx+ \int g(x) dx</eq></li>
</ul></li>
<li>Finding derivatives is easy because of the chain rule, but finding integrals is hard</li>
<li>Sometimes it is possible to solve an integral using substitution to rewrite a function that can be integrated
<ul>
<li>Usually we can substitute a polynomial in x with the new variable u</li>
<li><eq env="math">u=P(x)</eq></li>
<li>Since <eq env="math">\frac{du}{dx}=P'(x) \implies du=P'(x) dx \implies dx = \frac{1}{P'(x)}du</eq></li>
<li>Therefore, in general we can write <eq env="math">\int f(g(x)) g'(x) dx=\int f(u) du|_{u=g(x)}</eq></li>
</ul></li>
<li>We can integrate by parts by inverting the product rule for derivatives
<ul>
<li><eq env="math">\frac{d}{dx} [f(x)*g(x)]=f'(x)g(x)+f(x)g'(x) \implies \int [f(x)*g(x)]' dx=\int f'(x)g(x)dx+\int f(x)g'(x)dx</eq></li>
<li><eq env="math">f(x)g(x)=\int f'(x)g(x)dx+\int f(x)g'(x)dx \implies f(x)g(x)-\int f'(x)g(x)dx=\int f(x)g'(x)dx</eq></li>
<li>Therefore, if we let <eq env="math">u=f(x)</eq> and <eq env="math">du=f'(x)dx</eq>, and <eq env="math">v=g(x)</eq> with <eq env="math">dv=g'(x)dx</eq></li>
<li><eq env="math">\int u \: dv = uv-\int v \:du</eq></li>
<li>This is useful when we recognize a product between a function that is the derivative of something and a function for which we know the derivative</li>
<li>It can also be used to integrate a function for which we know the derivative by adding a 1 multiplicative constant, that we will integrate</li>
</ul></li>
</ul>
<h2 id="taylor-series">Taylor series</h2>
<ul>
<li>Polynomials are easier than other functions to work with, therefore approximating a non-polynomial function with a polynomial is really useful</li>
<li>To find a polynomial <eq env="math">P(x)</eq> of degree n that best approximates the non-polynomial function <eq env="math">f(x)</eq> around the point <eq env="math">x=0</eq> I can proceed by layers</li>
<li>The first constraint for my polynomial is that at <eq env="math">x=0</eq> it should be equal to the original function
<ul>
<li><eq env="math">P(x)|_{x=0}=f(x)|_{x=0}</eq></li>
<li>If <eq env="math">P(x)=c_0+c_1x+c_2x^2+c_3x^3+...+c_nx^n</eq>, at <eq env="math">x=0</eq> all the terms but <eq env="math">c_0</eq> cancel out</li>
<li>Therefore, since <eq env="math">P(x)|_{x=0}=c_0</eq> I can set <eq env="math">c_0=f(x)|_{x=0}</eq></li>
</ul></li>
<li>In order to better approximate <eq env="math">f(x)</eq>, I also want the tangent to <eq env="math">P(x)</eq> to be equal to that of the original function at <eq env="math">x=0</eq>
<ul>
<li><eq env="math">P'(x)|_{x=0}=f'(x)|_{x=0}</eq></li>
<li><eq env="math">P'(x)=c_1+2c_2x+3c_3x^2+...+nc_nx^{n-1}</eq></li>
<li><eq env="math">P'(x)|_{x=0}=c_1 \implies c_1=f'(x)|_{x=0}</eq></li>
<li><eq env="math">P'(x)=c_1+2c_2x+3c_3x^2+...+nc_nx^{n-1}</eq></li>
</ul></li>
<li>I can also desire that the concavity of <eq env="math">P(x)</eq> be equal to that of <eq env="math">f(x)</eq>
<ul>
<li><eq env="math">P''(x)|_{x=0}=f''(x)|_{x=0}</eq></li>
<li><eq env="math">P''(x)=2c_2+3*2c_3x+...+n*(n-1)c_nx^{n-2}</eq></li>
<li><eq env="math">P''(x)|_{x=0}=2c_2 \implies c_2=\frac{1}{2}f''(x)|_{x=0}</eq></li>
</ul></li>
<li>I can proceed like this for higher derivatives to find higher-degree coefficients, until I get to the desired n
<ul>
<li><eq env="math">c_n=\frac{1}{n!}\frac{d^nf}{dx^n}f(x)|_{x=0}</eq></li>
<li>The more degrees that I use, the better the approximation but the more complex the polynomial</li>
</ul></li>
<li>The infinite series of polynomial terms that approximate <eq env="math">f(x)</eq> at the point <eq env="math">x=0</eq> is called Maclaurin series
<ul>
<li>Note that this nice cancellation of higher-order polynomials that allow to easily compute high derivatives happens only at <eq env="math">x=0</eq></li>
</ul></li>
<li>In order to approximate <eq env="math">f(x)</eq> at a point <eq env="math">x_a \not= 0</eq> I can construct the polynomial so to have the variable <eq env="math">u=x-x_a</eq>
<ul>
<li>In this way, at <eq env="math">x=x_a \implies u=0</eq></li>
<li>This restore the nice behaviour observed at <eq env="math">x=0</eq></li>
<li>After the expansion, I can then substitute back <eq env="math">u=x-x_a</eq></li>
</ul></li>
<li>The infinite series that generalizes the Maclaurin series at any point <eq env="math">x=x_a</eq> is called Taylor series</li>
<li>We can give the Taylor series of <eq env="math">f(x)</eq> at the point <eq env="math">x_a</eq>
<ul>
<li><eq env="math">P(x)=\sum_{i=1}^n [\frac{(x-x_a)^i}{i!}\frac{d^if}{dx^i}|_{x=x_a}]</eq></li>
<li><eq env="math">P(x)=f(x)|_{x=x_a}+(x-x_a)\frac{df}{dx}|_{x=x_a}+\frac{(x-x_a)^2}{2!}\frac{d^2f}{dx^2}|_{x=x_a}+\frac{(x-x_a)^3}{3!}\frac{d^3f}{dx^3}|_{x=x_a}+...+\frac{(x-x_a)^n}{n!}\frac{d^nf}{dx^n}|_{x=x_a}</eq></li>
</ul></li>
<li>The Taylor series is an infinite sum, when we consider a certain degree polynomial we call it Taylor polynomial</li>
<li>The Taylor series is convergent for some functions, like <eq env="math">f(x)=e^x</eq>, and divergent for others, like <eq env="math">f(x)=\ln(x)</eq>
<ul>
<li>The maximum distance from <eq env="math">x_a</eq> and the points where the series converges on <eq env="math">f(x)</eq> is the radius of convergence of the Taylor series</li>
</ul></li>
</ul>
<h2 id="functions-in-more-than-1-variable">Functions in more than 1 variable</h2>
<ul>
<li>A 2 dimensional function takes 2 inputs <eq env="math">x,y</eq> and gives the output <eq env="math">z</eq>
<ul>
<li><eq env="math">z=f(x,y)</eq></li>
<li>They are usually represented with 3d surfaces or with level curves on the <eq env="math">x,y</eq> plane</li>
</ul></li>
<li>A level curve or contour level is defined as the set of point that respect the constraint <eq env="math">f(x,y)=c</eq></li>
<li>It is not possible to compute single derivatives of the function</li>
<li>We can fix <eq env="math">y</eq> and compute the derivative only with respect to <eq env="math">x</eq>
<ul>
<li>This is a 2-dimensional function that gives the slope of the tangent to the 2d curve in the <eq env="math">x,z</eq> plane that cuts the function at a certain value of <eq env="math">y</eq></li>
<li>This derivative of <eq env="math">f(x,y)</eq> is called partial derivative in <eq env="math">x</eq></li>
<li><eq env="math">f_x(x,y)=\frac{\partial f}{\partial x}=\lim_{h \to 0}\frac{f(x+h, y)-f(x,y)}{h}</eq></li>
<li>It is computed like a normal derivative, but considering the fixed variable like a constant</li>
</ul></li>
<li>Of course we can do the same fixing <eq env="math">x</eq> and taking the partial derivative in <eq env="math">y</eq>
<ul>
<li><eq env="math">f_y(x,y)=\frac{\partial f}{\partial y}=\lim_{h \to 0}\frac{f(x, y+h)-f(x,y)}{h}</eq></li>
</ul></li>
<li>It is possible to take second partial derivatives by taking the partial derivative in <eq env="math">x</eq> of <eq env="math">\frac{\partial f}{\partial x}</eq> or the partial derivative in <eq env="math">y</eq> of <eq env="math">\frac{\partial f}{\partial y}</eq>
<ul>
<li>We will then obtain <eq env="math">f_{xx}(x,y)=\frac{\partial^2f}{\partial x^2}</eq> and <eq env="math">f_{yy}(x,y)=\frac{\partial^2f}{\partial y^2}</eq></li>
</ul></li>
<li>We can also take mixed partial derivatives by taking the derivative in <eq env="math">x</eq> of <eq env="math">\frac{\partial f}{\partial y}</eq>, or the derivative in <eq env="math">y</eq> of <eq env="math">\frac{\partial f}{\partial x}</eq>
<ul>
<li>The notation for these derivatives is <eq env="math">f_{xy}(x,y)=\frac{\partial^2 f}{\partial x\partial y}</eq> and <eq env="math">f_{yx}(x,y)=\frac{\partial^2 f}{\partial y\partial x}</eq></li>
</ul></li>
<li>A fundamental propriety is that the order of differentiation does not matter (!)
<ul>
<li><eq env="math">\frac{\partial }{\partial x}[\frac{\partial f}{\partial y}]=\frac{\partial }{\partial y}[\frac{\partial f}{\partial x}] \iff f_{xy}(x,y)=f_{yx}(x,y)</eq></li>
</ul></li>
<li>We can generalise this concepts to a function of n variables <eq env="math">f(x_1,x_2,...,x_n)</eq>
<ul>
<li>In any case a consider all the variables constant except the one that I am deriving</li>
<li>I will have n first order partial derivatives for a function with n inputs</li>
</ul></li>
</ul>
<h2 id="gradient-and-hessian">Gradient and Hessian</h2>
<ul>
<li>The vector containing all the first partial derivatives of a function is called gradient of that function, indicated with <eq env="math">\nabla</eq>
<ul>
<li><eq env="math">\nabla_{f(x_1,x_2,...,x_n)}=(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n})</eq></li>
<li>The gradient evaluated at any point <eq env="math">\nabla f|_{\vec{x_a}}</eq> is orthogonal to the contour level at that point</li>
<li>The gradient represents the direction of steepest ascent, because it is a vector with components which ones evaluated at a certain point are the slope of the tangent along the 2 axes</li>
<li>We can see the gradient as a vector field, which always points in the direction that maximizes the increase in <eq env="math">f</eq>, with length proportional to that increase</li>
</ul></li>
<li>The matrix containing all the second partial derivatives of a function is called Hessian of that function, indicated with H
<ul>
<li><eq env="math">H_{f(x_1,x_2,...,x_n)}=\begin{pmatrix}  \frac{\partial^2 f}{\partial x_1^2}&amp;\frac{\partial^2 f}{\partial x_1\partial x_2}&amp;...&amp;\frac{\partial^2 f}{\partial x_1\partial x_n}\\  \frac{\partial^2 f}{\partial x_2 \partial x_1}&amp;\frac{\partial^2 f}{\partial x_2^2}&amp;...&amp;\frac{\partial^2 f}{\partial x_2\partial x_n}\\  ...&amp;...&amp;...&amp;...\\  \frac{\partial^2 f}{\partial x_n \partial x_1}&amp;\frac{\partial^2 f}{\partial x_n\partial x_2}&amp;...&amp;\frac{\partial^2 f}{\partial x_n^2}\\  \end{pmatrix}</eq></li>
<li>Since the mixed partial derivatives are insensitive to the order of differentiation, the Hessian is always symmetric</li>
</ul></li>
</ul>
<h2 id="taylor-expansion-of-a-function-in-more-than-1-variable">Taylor expansion of a function in more than 1 variable</h2>
<ul>
<li>The Taylor expansion of grade 2 of a multivariable function can be represented with gradient and Hessian
<ul>
<li>We can consider the function as operating on a vector of variables <eq env="math">\vec{x}</eq>, with dimensionality equal to the number of variables
<ul>
<li><eq env="math">f(x_1,x_2,...,x_n)=f(\vec{x})</eq></li>
</ul></li>
<li>Let <eq env="math">\vec{x_a}</eq> be the point at which we want to do the expansion
<ul>
<li><eq env="math">f(\vec{x})=f(\vec{x_a})+(\nabla\:f|_{\vec{x_a}})^t*(\vec{x}-\vec{x_a})+\frac{1}{2}(\vec{x}-\vec{x_a})^tH(\vec{x}-\vec{x_a})+...</eq></li>
</ul></li>
</ul></li>
<li>Terms with degree higher than 2 will require the analogous of the gradient and Hessian at higher dimensions
<ul>
<li>These will be multi-dimensional object difficult to treat</li>
</ul></li>
</ul>
<h2 id="local-extrema-in-multi-variable-functions">Local extrema in multi-variable functions</h2>
<ul>
<li>The local extrema of a multivariable function are those points where the gradient of the function is 0
<ul>
<li>In those points, the function has an horizontal tangent along all the axes</li>
</ul></li>
<li>In order to understand if a critical point <eq env="math">\vec{x_a}</eq> where <eq env="math">\nabla\:f|_{\vec{x_a}}=0</eq> is a local maximum or a local minimum, I need to consider the behavior of the Hessian at that point
<ul>
<li>This is analogous to the evaluation of the second derivative in a standard function in order to understand its concavity</li>
</ul></li>
<li>Since the Hessian is a n*n symmetric matrix, it has n real eigenvalues
<ul>
<li>If all the eigenvalues of the Hessian are positive, the point is a minimum</li>
<li>If all the eigenvalues of the Hessian are negative, the point is a maximum</li>
<li>If the eigenvalues of the Hessian are some positive and some negative, the point is a saddle point is <eq env="math">\mathbb{R}^2</eq>, and a more complex shape in other dimensions</li>
</ul></li>
<li>When the Hessian is not diagonal, it can be diagonalized so that we can have the eigenvalues on the diagonal
<ul>
<li>This corresponds to rotating the system of reference so to align it to the directions of more rapid change in concavity</li>
<li>Note that I do not need to go back to the previous system of reference, from the Hessian I only want a qualitative information on the concavity, not a number</li>
</ul></li>
</ul>
<h2 id="constrained-optimization-problems-lagrange">Constrained optimization problems (Lagrange)</h2>
<ul>
<li>A constraint optimization problem deals with finding the extrema of a function on more than one variable subjected to a constraint
<ul>
<li>This has many applications, for example we may want to maximize a function while there is a physical constraint on the variables that we cannot avoid</li>
<li>The constraint is a set of points that respects a condition
<ul>
<li>An example is the constraint <eq env="math">g(x,y)=x^2+y^2-1=0</eq>, which means that we are limited to the points on the unit circle in the x,y plane</li>
</ul></li>
<li>In 3d we can project the constraint on the surface of our function <eq env="math">f(x,y)</eq> and the problem translates to finding the highest point on the circle</li>
</ul></li>
<li>It is easier to visualize the problem in term of contour lines in 2d
<ul>
<li>The solution consists then in finding a point where a level curve is tangent to the constraint</li>
</ul></li>
<li>Generalizing to any number of dimensions, if we consider that a level curve is an object that respects the condition <eq env="math">f(\vec{x})=c</eq>, the constraint optimization problem can be translated in finding the points of tangency between the constraint curve <eq env="math">g(\vec{x})=\vec{0}</eq> and the level curves <eq env="math">f(\vec{x})=\vec{c}</eq></li>
<li>Since the gradient of a function is perpendicular to the contour level, we can solve the problem by finding the points where the gradient of the function and that of the constraint are parallel
<ul>
<li>If the 2 vectors are parallel, they are the same vector scaled by a constant</li>
<li><eq env="math">\nabla\:g(\vec{x})|_{\vec{x_a}}=\lambda*\nabla\:f(\vec{x})|_{\vec{x_a}}</eq></li>
<li>The factor <eq env="math">\lambda</eq> is called Lagrange multiplier</li>
</ul></li>
<li>We have therefore n+1 variables to find (all the coordinates of <eq env="math">\vec{x}</eq> and <eq env="math">\lambda</eq>) and n+1 equations
<ul>
<li>n equations are embedded in <eq env="math">\nabla\:g(x,y)|_{x_a,y_a}=\lambda*\nabla\:f(x,y)|_{x_a,y_a}</eq></li>
<li>The remaining equation is the constraint itself <eq env="math">g(x,y)=0</eq></li>
</ul></li>
<li>All these equations can be expressed in a compact way with a new function, called Lagrangian
<ul>
<li><eq env="math">\mathcal{L}(\vec{x},\lambda)=f(\vec{x})-\lambda*g(\vec{x})</eq></li>
<li>It is not necessary to form the Lagrangian when computing by hand, we can just solve a normal system of equations</li>
<li>In computational applications however, computers are much faster in solving the gradient of the Lagrangian, and this is a much more compact way of representing the same information</li>
</ul></li>
<li>The solutions of the constrained optimization problem then corresponds to finding the values for <eq env="math">\vec{x}</eq> and <eq env="math">\lambda</eq> for which the gradient of the Lagrangian is 0
<ul>
<li><eq env="math">\nabla \mathcal{L}(\vec{x},\lambda) = \vec{0}</eq></li>
<li>The solutions <eq env="math">\vec{x}</eq> can be more than one, however they are usually in a number that can be easily computed</li>
<li>I can then use my solutions as an input for <eq env="math">f(\vec{x})</eq> and find the one that is higher, or lower, depending what I am looking for</li>
<li>The Lagrange multiplier <eq env="math">\lambda</eq> associated with a specific solution tells me how much the function <eq env="math">f(\vec{x})</eq> is sensitive to variations in the constraint <eq env="math">g(\vec{x})=\vec{0}</eq></li>
</ul></li>
<li>If we have multiple constraints, we will have a Lagrange multiplier for each constraint</li>
</ul>
<h2 id="information-entropy">Information entropy</h2>
<ul>
<li>The constrained optimization problem can be used to solve the problem of information entropy: maximize how much information a signal carries</li>
<li>We can think of a signal with n different possible values <eq env="math">v_i\:,\:\:i=1...n</eq></li>
<li>Each value can occur with the respective probability <eq env="math">p_i</eq></li>
<li>The Shannon entropy for the signal is given by the function <eq env="math">S=-\sum_i [p_i\ln(p_i)]</eq></li>
<li>If we want to maximize the information carried by the signal, we have to maximize the function <eq env="math">S</eq></li>
<li>Since we are talking about probabilities, we are under the constraint <eq env="math">C=\sum_i [p_i]-1=0</eq></li>
<li>We have a function and a constraint: we can write the Lagrangian
<ul>
<li><eq env="math">\mathcal{L}=S+\lambda(C)=-\sum_i [p_i\ln(p_i)]-\lambda(\sum_i[p_i]-1)</eq></li>
</ul></li>
<li>By solving <eq env="math">\nabla \mathcal{L}=\vec{0}</eq> we find that <eq env="math">p_i=e^{-1-\lambda}</eq></li>
<li>All the <eq env="math">p_i</eq> are equal and they must sum to 1, therefore <eq env="math">p_i=\frac{1}{n}</eq></li>
<li>By substituting on the original function then we find that the maximum information content of the signal is <eq env="math">S_{max}=-\sum_i [\frac{1}{n}\ln(\frac{1}{n})]=\sum_i [\frac{1}{n}\ln(n)]=\ln(n)</eq></li>
</ul>
<h1 id="statistics">Statistics</h1>
<h2 id="set-theory">Set theory</h2>
<ul>
<li>A set is an unordered collection of objects, also called space
<ul>
<li><eq env="math">C=\{x_1, x_2, x_3,...,x_n\}</eq></li>
</ul></li>
<li>An object that belongs to a set is said to be an element of that set
<ul>
<li><eq env="math">x \in C</eq> means that the object x is an element of the set C</li>
</ul></li>
<li>A subset of a set is another set such that all the elements it contains are also contained by the main set
<ul>
<li><eq env="math">A \subseteq B \iff [\forall \: x \in A \implies x \in B]</eq></li>
<li><eq env="math">A \subseteq B \land B \subseteq A \iff A = B</eq></li>
</ul></li>
<li>A set without elements is called null set, denoted by <eq env="math">C=\phi</eq></li>
<li>The union of 2 sets is another set containing all the element contained in one of the sets, or in both
<ul>
<li><eq env="math">A \cup B</eq></li>
<li>It corresponds to a logical OR</li>
</ul></li>
<li>The intersection of 2 sets is the set containing the elements that belong to both sets
<ul>
<li><eq env="math">A \cap B</eq></li>
<li>It corresponds to a logical AND</li>
</ul></li>
<li>If <eq env="math">A \cup B = \phi</eq> the 2 sets are mutually exclusive</li>
<li>The complement of a subset is the set of all elements in the set but not in the subset
<ul>
<li><eq env="math">A \subset B \implies A^c=B-A</eq></li>
</ul></li>
<li>De Morgan’s laws
<ul>
<li><eq env="math">(A \cap B)^c=A^c \cap B^c</eq></li>
<li><eq env="math">(A \cup B)^c=A^c \cup B^c</eq></li>
</ul></li>
<li>Set union and intersection are commutative and associative
<ul>
<li><eq env="math">A \cap B = B \cap A</eq></li>
<li><eq env="math">A \cup B = B \cup A</eq></li>
<li><eq env="math">(A \cap B)\cap C=(A\cap B)\cap(B\cap C)</eq></li>
<li><eq env="math">(A \cup B)\cup C=(A\cup B)\cup(B\cup C)</eq></li>
</ul></li>
</ul>
<h2 id="probability">Probability</h2>
<ul>
<li>Probability is a mathematical model for random phenomena</li>
<li>A phenomenon is probabilistic if the outcome of an experiment is uncertain, but over large numbers we observe a regular distribution</li>
<li>An experiment is any procedure that can be repeated in theory an infinite number of times and has a well-defined set of possible outcomes</li>
<li>The sample space of an experiment is the set of all its possible outcomes</li>
<li>An event is a subset of the sample space</li>
<li>In a frequency approach, probability is the ratio between the number of favorable events and that of total events
<ul>
<li>Let C be the sample space of an experiment such that <eq env="math">C=\{E_1, E_2, E_3, ..., E_n\}</eq>, containing n elements</li>
<li>Let F be an event subset of C containing all the outcomes that are considered favorable, <eq env="math">F \subseteq C</eq></li>
<li>Let F contain m elements</li>
<li>Then, the probability that the event F will happen is given by <eq env="math">P(F) \approx \frac{m}{n}</eq></li>
</ul></li>
<li>Probability can also be viewed as the confidence in an event happening</li>
<li>A probability is a number between 0 and 1
<ul>
<li><eq env="math">0\leq P(E_i)\leq1</eq></li>
</ul></li>
<li>The sum of the probabilities of all the possible outcomes of an experiment is equal to 1, meaning that there will definitely be 1 outcome
<ul>
<li><eq env="math">P(\cup E_i)=1</eq></li>
</ul></li>
<li>The probability of 1 of 2 events happening is equal to the sum of the probabilities minus their intersection
<ul>
<li><eq env="math">P(A\cup B)=P(A)+P(B)-P(A\cap B)</eq></li>
</ul></li>
<li>If 2 events are mutually exclusive (they do not have an intersection!), the probability of their union is the sum of their probabilities
<ul>
<li><eq env="math">P(A\cup B)=P(A)+P(B)</eq></li>
</ul></li>
<li>The intersection of 2 events that are independent and not mutually exclusive is the product of their probabilities
<ul>
<li><eq env="math">P(A \cap B)=P(A)*P(B)</eq></li>
</ul></li>
<li>The conditional probability of <eq env="math">A</eq> given <eq env="math">B</eq> is represented as <eq env="math">P(A|B)</eq>
<ul>
<li>If the events are independent, <eq env="math">P(A|B)=P(A)</eq></li>
<li>If they are not independent, <eq env="math">P(A|B)=\frac{P(A\cap B)}{P(B)}</eq></li>
<li>Conditioning on an event means that the total event space is reduced to that event</li>
</ul></li>
<li>The intersection of 2 events that are NOT independent and NOT mutually exclusive is given by
<ul>
<li><eq env="math">P(A\cap B)=P(A)*P(B|A)=P(B)*P(A|B)</eq></li>
</ul></li>
<li>2 mutually exclusive events cannot be independent, and vice-versa</li>
<li>The bayes formula: <eq env="math">P(A|B)</eq> is different from <eq env="math">P(B|A)</eq>
<ul>
<li><eq env="math">P(B|A)=\frac{P(A|B)*P(B)}{P(A)}</eq></li>
<li>Note that this is just a rearrangement of the intersection of non-independent events</li>
</ul></li>
<li>To test if A and B are independent events, we can test that both <eq env="math">P(A|B)=P(A) \land P(B|A)=P(B)</eq> be true</li>
<li>The odds ratio of 2 events is a statistic that quantifies the strength of the association between them
<ul>
<li><eq env="math">odd(A,B)=\frac{P(A\cap B}{P(A)*P(B)}</eq></li>
<li>If <eq env="math">OR=1</eq> the events are independent</li>
<li>Frequently <eq env="math">\log(odd(A,B))</eq> is used</li>
</ul></li>
<li>A partition of the sample space U is an event <eq env="math">E_i</eq> such that the sum of all the <eq env="math">E_i</eq> is equal to U itself, without holes and sovrappositions
<ul>
<li><eq env="math">U=\cup_i[ E_i] \:\:\land\:\: E_i \cap E_j = \phi \:\:\:\:\forall\: i \not= j</eq></li>
</ul></li>
<li>If <eq env="math">E_i</eq> is a partition of U and A a subset of U, then
<ul>
<li><eq env="math">P(A)=\cup_i [P(A \cap E_i)]=\cup_i [P(A|E_i)P(E_i)]</eq></li>
</ul></li>
</ul>
<h2 id="counting">Counting</h2>
<ul>
<li>A permutation is an ordered arrangement of objects</li>
<li>A combination is a set of objects, without considering their order
<ul>
<li>It is expressed as <eq env="math">_nC_r=\begin{pmatrix}n\\r\end{pmatrix}</eq>, which is read “n choose r”</li>
</ul></li>
<li>We can have situations in which the same object can be drawn an infinite number of times
<ul>
<li>In this case we talk of replacement</li>
<li>An example is the possible permutations of letters in a 10bp DNA sequence
<ul>
<li>A,T,C,G are the objects, but each of them can be drawn more than once</li>
</ul></li>
</ul></li>
<li>In other situations an object can be drawn only once
<ul>
<li>I take objects from a physical stack of objects: I cannot take it again after the first time</li>
</ul></li>
<li>Permutations with replacement: the sequence of numbers that I can get from 3 dice rolls
<ul>
<li>Let n be the number of possible outcomes
<ul>
<li>In the case of a dice, there are 6 possible outcomes so <eq env="math">n=6</eq></li>
</ul></li>
<li>Let r be the number of outcomes that I consider (the length of the sequence of outcomes)
<ul>
<li>For example, how many times I roll my dice</li>
</ul></li>
<li>Then, the number of permutations p is given by <eq env="math">p=n^r</eq>
<ul>
<li>This is the possible sequences of 3 numbers that I can obtain from 3 dice rolls (<eq env="math">6^3=216</eq>)</li>
</ul></li>
</ul></li>
<li>Permutations without replacement: in how many ways, considering order, I can sit 6 people in 3 chairs
<ul>
<li>Let n be the numerosity of my object pool
<ul>
<li>In this case, there are 6 people so <eq env="math">n=6</eq></li>
</ul></li>
<li>Let r be the number of objects that I will extract from the pool
<ul>
<li>For example, how many chairs do I have</li>
</ul></li>
<li>We can reason that the first object can be 1 of the n different objects available, the second 1 of the n-1 remaining and so on</li>
<li>Therefore, the number of permutation without replacement is given by <eq env="math">n*(n-1)*(n-2)*...*(n-r)</eq>
<ul>
<li>We can cleanly express this with factorials</li>
</ul></li>
<li>Then, the number of permutations p without replacement is given by <eq env="math">p=\frac{n!}{(n-r)!}</eq>
<ul>
<li>This is the possible ordered ways I can sit 6 people in 3 chairs (<eq env="math">6!/(6-3)!=4*5*6=120</eq>)</li>
</ul></li>
</ul></li>
<li>Combinations without replacement: how many different unordered groups of 3 people can I get by choosing from a pool of 6 people
<ul>
<li>Let n be the numerosity of my object pool
<ul>
<li>In this case, there are 6 people so <eq env="math">n=6</eq></li>
</ul></li>
<li>Let r be the number of objects that I will extract from the event pool
<ul>
<li>For example, how many people will be in the final group that I want to extract</li>
</ul></li>
<li>We can reason that the number of combinations without replacement is necessarily a subset of the number of permutations without replacement
<ul>
<li>It is the number of permutations minus the number of permutations containing the same elements in a different order</li>
</ul></li>
<li>The number of permutations would be <eq env="math">\frac{n!}{(n-r)!}</eq>, and each unique set can be expressed in <eq env="math">r!</eq> different combinations
<ul>
<li>The final set of combination would be <eq env="math">1/r!</eq> of the set of permutations</li>
</ul></li>
<li>Therefore, the number of combinations without replacement is given by <eq env="math">c=\begin{pmatrix}n\\r\end{pmatrix}=\frac{n!}{(n-r)!r!}</eq>
<ul>
<li>This is the number of possible different groups of 3 people that I can form from a pool of 6 people (<eq env="math">6!/[(6-3)!3!]=4*5*6/2*3=120/6=20</eq>)</li>
</ul></li>
</ul></li>
<li>Combinations with replacement: the unordered set of numbers that I can get from 3 dice rolls
<ul>
<li>Let n be the number of possible outcomes per event
<ul>
<li>In the case of a dice, there are 6 possible outcomes so <eq env="math">n=6</eq></li>
</ul></li>
<li>Let r be the number of outcomes that I consider
<ul>
<li>For example, how many times I roll my dice</li>
</ul></li>
<li>We can reason that the event pool is made of n objects that get regenerated when I choose one of them, but not for the last one since I will not choose after that
<ul>
<li>Therefore, we can choose r objects among n+r-1 non-replaceable objects</li>
</ul></li>
<li>Then, the number of combinations with replacement is given by <eq env="math">c=\begin{pmatrix}n+r-1\\r\end{pmatrix}=\frac{(n+r-1)!}{(n-1)!r!}</eq>
<ul>
<li>This is the possible sets of 3 numbers that I can obtain from 3 dice rolls (<eq env="math">_8C_3=8!/(5!*3!)=56</eq>)</li>
</ul></li>
</ul></li>
</ul>
<h2 id="discrete-distributions">Discrete distributions</h2>
<ul>
<li>Probabilities can be described with distributions</li>
<li>I represent with a capital letter the random variable, with a normal letter one of its values</li>
<li>A random variable is a way of mapping the outcome of an experiment to a number</li>
<li>Discrete variables can only take specific values
<ul>
<li><eq env="math">x_i \in I=\{x_i, i \in \mathbb{N}\}</eq></li>
<li>The probability distribution is defined by the function <eq env="math">f(x_i)</eq>, which for every <eq env="math">x_i</eq> gives the corresponding probability</li>
<li>The cumulative distribution is given by the function <eq env="math">F(x_i)</eq>, which for every <eq env="math">x_i</eq> gives the probability for a value <eq env="math">\leq x_i</eq></li>
</ul></li>
<li>A discrete distribution can be normalized, meaning that it is rescaled so to have the total sum of probabilities equal to 1
<ul>
<li><eq env="math">\sum_{i \in I}f(x_i)=1</eq></li>
<li><eq env="math">F(x_{max(i)})=1</eq></li>
</ul></li>
<li>Discrete distribution are represented with histograms</li>
<li>The probability mass function of a discrete distribution (PMF) is a function that given a value of the random variable X, it produces a probability of that occurring
<ul>
<li>It is what defines the distribution itself</li>
<li>If we plot the random variable and the PMF (the probability!) I obtain the histogram of the distribution</li>
</ul></li>
<li>The mode of a discrete distribution is the value that occurs with the highest probability
<ul>
<li>It is the highest peak of the histogram</li>
<li>If there are 2 peaks, the distribution is called bimodal</li>
</ul></li>
<li>The median of a discrete distribution is the value of the random variable for which <eq env="math">P(X &gt; x_{med})=P(X &lt; x_{med})=\frac{1}{2}</eq>
<ul>
<li>The first step in computing the median is to order the observations from lowest to highest</li>
<li>If the number of observations is odd, the median is the middle value in the series</li>
<li>If the number of observations is even, the median is the average of the 2 central observations</li>
<li>In the same way, the values of x that split the distribution in quarters is called quartile, in fifths quintile and so on</li>
<li>The distance between the first and the third quartile is called inter-quartile range, and it is a measure of the spreading of the data</li>
</ul></li>
<li>The mean or average of a discrete distribution is the expected value of the random variable X (it can be represented as <eq env="math">E[X]</eq>, <eq env="math">\mu</eq> or <eq env="math">&lt;X&gt;</eq>)
<ul>
<li>It is also called first moment of the random variable, while <eq env="math">k^{th}</eq> moment represents the expected value of <eq env="math">x^k</eq></li>
<li><eq env="math">E[f(x)]=\sum_{i \in I} f(x_i)p(x_i)</eq>, where <eq env="math">f(x)</eq> is a function defined over discrete random variables and <eq env="math">p(x)</eq> is the probability distribution
<ul>
<li>It is essentially a weighted average of the probabilities for a function of X to have a certain value</li>
<li>In the simplest case <eq env="math">f(x)=x</eq>, therefore I am just taking the mean value of the variable</li>
</ul></li>
<li>For an empirical distribution it is calculated as <eq env="math">E[x]=\frac{1}{n}\sum_{i=1}^n x_i</eq></li>
</ul></li>
<li>The variance is the mean squared distance of x from the mean of the distribution
<ul>
<li><eq env="math">\sigma^2[x]=Var(x)=E[(x-E[x])^2]=E[x^2]-E[x]^2</eq></li>
<li>It is a measure of how much the distribution is spread out</li>
</ul></li>
<li>The standard deviation has the same meaning of the variance, but is more useful because it has the same dimensionality of the random variable x
<ul>
<li><eq env="math">\sigma [x]=\sqrt{Var(x)}</eq></li>
</ul></li>
<li>Some general properties of expected values and variances
<ul>
<li><eq env="math">E[x_1+X_2]=E[x_1]+E[x_2]</eq></li>
<li><eq env="math">E[x+k]=E[x]+k]</eq></li>
<li><eq env="math">E[ax_1+bx_2]=aE[x_1]+bE[x_2]</eq></li>
<li><eq env="math">Var(a*x)=a^2*Var(x)</eq></li>
</ul></li>
<li>If and only if <eq env="math">x_1</eq> and <eq env="math">x_2</eq> are independent random variables
<ul>
<li><eq env="math">E[x_1*x_2]=E[x_1]*E[x_2]</eq></li>
<li><eq env="math">Var(x_1+x_2)=Var(x_1)+Var(x_2)</eq></li>
</ul></li>
<li>The covariance of 2 random variables describes how their respective variations are related
<ul>
<li><eq env="math">Cov(x_1,x_2)=E[(x_1-E[x_1])(x_2-E[x_2])]</eq></li>
<li>If the 2 random variables are independent <eq env="math">Cov(x_1,x_2)=0</eq></li>
</ul></li>
<li>For any pair of random variables
<ul>
<li><eq env="math">Var(x_1+x_2)=Var(x_1)+Var(x_2)+2*Cov(x_1,x_2)</eq></li>
</ul></li>
<li>The Bernoulli distribution models a single trial that can have 2 mutually exclusive outcomes
<ul>
<li>Let’s call the 2 outcomes success and failure, with probabilities p and 1-p</li>
<li>Let the random variable X be 0 for failure an 1 for success</li>
<li>The PMF is <eq env="math">P(X=x)=p^x(1-p)^{1-x}</eq></li>
<li>The Bernoulli distribution is obvious, but Bernoulli trials are at the foundations of many discrete distributions</li>
</ul></li>
<li>The binomial distribution models the number of successes in n independent Bernoulli trials
<ul>
<li>Let the same conditions of the Bernoulli trial hold, so 2 mutually exclusive outcomes with probability p and 1-p</li>
<li>Let the random variable X represent the number of successes in n trials</li>
<li>The PMF is <eq env="math">P(X=x)=\begin{pmatrix}n\\x\end{pmatrix}p^x(1-p)^{n-x}</eq>
<ul>
<li><eq env="math">p^x(1-p)^{n-x}</eq> is the probability of having a specific sequence of length n with x successes</li>
<li><eq env="math">_nC_x</eq> is the number of different combinations of x successes that I can get in n trials</li>
<li>So the PMF is the number of possible sequences of outcomes where X=x, times the probability of each of them</li>
</ul></li>
<li>The mean is the number of trials times the probability of the favorable event
<ul>
<li><eq env="math">E[X]=np</eq></li>
</ul></li>
<li>The variance is
<ul>
<li><eq env="math">Var(X)=np(1-p)</eq></li>
</ul></li>
</ul></li>
<li>The Poisson distribution models how many Bernoulli successes will occur in a given unit of a continuous axis (time, volume, …), when the probability of the success is constant
<ul>
<li>The random variable X counts the number of successes in the unit of time (or area, volume, …)</li>
<li>The PMF is <eq env="math">P(X=x)=\frac{\lambda^x*e^{-\lambda}}{x!}</eq>
<ul>
<li>We can consider that in an interval of length 1, we have a mean number of successes called <eq env="math">\lambda</eq></li>
<li>Suppose we subdivide the interval in n parts of equal length <eq env="math">\frac{1}{n}</eq></li>
<li>Therefore, if n is big enough each subinterval will have a probability of having 2 successes <eq env="math">\approx 0</eq> and the probability of having a success <eq env="math">\frac{\lambda}{n}</eq></li>
<li>We can see the process as a binomial distribution with probability of success <eq env="math">p=\frac{\lambda}{n}</eq>, with n trials</li>
<li>If we want the probability of having x successes in the unit interval, we can phrase it as the probability of having a success in exactly x of the subintervals</li>
<li>We therefore have <eq env="math">P(X=x)\approx \begin{pmatrix}n\\x\end{pmatrix}\frac{\lambda}{n}^x(1-\frac{\lambda}{n})^{n-x}</eq></li>
<li>Let’s consider <eq env="math">\lim_{n \to \infty}</eq>, so what happens in infinitely many intervals that are infinitely small</li>
<li><eq env="math">P(X=x)=\lim_{n \to \infty}[\begin{pmatrix}n\\x\end{pmatrix}\frac{\lambda}{n}^x(1-\frac{\lambda}{n})^{n-x}]=\frac{\lambda^x*e^{-\lambda}}{x!}</eq></li>
</ul></li>
<li>The mean and the variance are both equal to <eq env="math">E[X]=Var(X)=\lambda</eq></li>
<li><eq env="math">\lambda</eq> is the mean number of successes in the given unit interval</li>
</ul></li>
<li>Other discrete distributions
<ul>
<li>The geometric distribution describes the probability of waiting for X Bernoulli trials for seeing the first success</li>
<li>The negative binomial or Pascal distribution is a generalization of the geometric distribution that describes the probability of waiting for X Bernoulli trials for seeing n successes</li>
<li>The hypergeometric distribution models the number of successes in n trials like the binomial, but considers the case of non-independent trials</li>
</ul></li>
<li>Maximum likelihood estimation is a technique that allows fitting a distribution to my data
<ul>
<li>Likelihood is a concept different from probability
<ul>
<li>Probability refers to the observed data with respect to a certain distribution</li>
<li>Likelihood refers to having a certain distribution parameter given the data</li>
</ul></li>
<li>I want to find the best parameters t for my data d, supposing they follow a certain distribution m</li>
<li>I can solve for <eq env="math">t^*=argmax[P(d|t,m)]</eq></li>
<li>Supposing that my model is <eq env="math">m(d|t)</eq>, I have a likelihood function <eq env="math">L(t|d,m)</eq></li>
<li>Since I want to find a maximum of this function, I can set <eq env="math">\frac{dL}{dt}=0</eq> and find the correspondent <eq env="math">t^*</eq></li>
</ul></li>
</ul>
<h2 id="continuous-distributions">Continuous distributions</h2>
<ul>
<li>Continuous variables can take infinite values in any interval
<ul>
<li><eq env="math">x \in I \subseteq \mathbb{R}</eq></li>
<li>In a continuous distribution the function does NOT represent a probability, but a probability density (!)</li>
</ul></li>
<li>It is meaningful to talk about probability only over an interval (!)
<ul>
<li><eq env="math">p(x=x_i)=0 \: \forall \: x_i</eq></li>
</ul></li>
<li>The cumulative probability is <eq env="math">F(x)=\int f(x) dx</eq>, and for a normalize random variable <eq env="math">F(x)=1</eq></li>
<li>The PDF is the derivative of the cumulative probability <eq env="math">f(x)=\frac{dF}{dx}</eq></li>
<li>The mean is <eq env="math">E[x]=\int x*f(x)dx</eq>
<ul>
<li>It is the continuous version (integral) of a weighted sum</li>
</ul></li>
<li>The variance is <eq env="math">Var(x)=E[(x-\mu)^2]=\int(x-\mu)^2*f(x)dx</eq></li>
<li>The uniform probability distribution has a constant value in an interval and 0 outside of it
<ul>
<li>The PDF is <eq env="math">\begin{cases}f(x)=\frac{1}{b-a} &amp; a\leq x\leq b\\f(x)=0 &amp; x\leq a \land x\geq b \end{cases}</eq></li>
<li>The mean is <eq env="math">E[x]=\frac{a+b}{2}</eq></li>
<li>The variance is <eq env="math">Var[x]=\frac{(b-a)^2}{12}</eq>, it can be derived by integration</li>
</ul></li>
<li>The normal or Gaussian distribution is the most important continuous distribution
<ul>
<li>For the central limit theorem, the sum of a number of random variables approaches a normal distribution as the number of variables increases</li>
<li>The PDF is <eq env="math">f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</eq></li>
<li>To say that a random variable follows a certain normal distribution, we can write <eq env="math">X\sim N(\mu,\sigma^2)</eq></li>
<li>The mean of the normal distribution is <eq env="math">E[x]=\int x * f(x) dx=\mu</eq></li>
<li>The variance of the normal distribution is <eq env="math">Var(x)=\int (x-\mu)^2 * f(x) dx=\sigma^2</eq></li>
<li>The points <eq env="math">\mu-\sigma</eq> and <eq env="math">\mu+\sigma</eq> are the flexus points of the curve
<ul>
<li>The integral between <eq env="math">i\mu+\sigma</eq> and <eq env="math">\mu-\sigma</eq> is <eq env="math">\approx 68\%</eq></li>
</ul></li>
<li>The integral between <eq env="math">\mu-2\sigma</eq> and <eq env="math">\mu+2\sigma</eq> is <eq env="math">\approx 95\%</eq></li>
<li>The integral between <eq env="math">\mu-3\sigma</eq> and <eq env="math">\mu+3\sigma</eq> is <eq env="math">\approx 99.7\%</eq></li>
<li>The standard normal distribution is a normal distribution with mean 0 and variance 1, indicated with z
<ul>
<li><eq env="math">z \sim N(0,1)</eq></li>
<li>Any distribution can be normalized by introducing the standardized variable z such that <eq env="math">z=\frac{x-\mu}{\sigma}</eq></li>
</ul></li>
</ul></li>
<li>Central limit theorem: the sample mean is normally distributed for large sample sizes, regardless of the original distribution of the population
<ul>
<li>We can assume that our data is normally distributed if the sample size is large enough</li>
</ul></li>
<li>The normal distribution cannot be integrated analytically, therefore the integral is computed with tables for the standard normal distribution or via software</li>
<li>In an n-dimensional space the normal distribution is computed with vectors and matrices</li>
<li>The estimated mean of the distribution of a set of normally distributed data is <eq env="math">\frac{1}{n}\sum_{i=1}^nx_i</eq>
<ul>
<li>This can be derived by a long integration of the formula for the normal distribution</li>
<li>This is an unbiased estimate</li>
</ul></li>
<li>The estimated variance of the distribution of a set of normally distributed data is <eq env="math">\frac{1}{n-1}\sum_{i=1}^n(x_i-\mu)^2</eq>
<ul>
<li>The sample variance underestimates the variance of the population, as can be derived by expanding the variance formula</li>
<li>Sometimes the variance of the sample is named <eq env="math">S^2</eq> to differentiate it from the real variance</li>
</ul></li>
<li>The sample mean <eq env="math">\bar x</eq> is normally distributed with mean equal to the mean of the distribution and variance equal to the variance of the distribution divided by n
<ul>
<li><eq env="math">E[\bar x]=\mu</eq></li>
<li><eq env="math">Var[\bar x]=\frac{1}{n}\sigma^2</eq></li>
</ul></li>
<li>The sample variance is distributed following the <eq env="math">\chi^2</eq> distribution with n-1 degrees of freedom
<ul>
<li>The degrees of freedom <eq env="math">df=n-1</eq> are the number of independent <eq env="math">x_i</eq></li>
</ul></li>
<li>The <eq env="math">\chi^2</eq> distribution is a family of asymmetrical distribution for different degrees of freedom which models the distribution of the sample variance of a normal population
<ul>
<li>The distribution of the square of a normal distribution is a <eq env="math">\chi^2</eq> distribution with <eq env="math">df=1</eq></li>
</ul></li>
<li>The Student t distribution is the distribution of the variable <eq env="math">t=\frac{z}{\sqrt{\frac{u}{v}}}</eq>, where z is a standard normally distributed random variable, u is a random variable with <eq env="math">\chi^2</eq> distribution with v degrees of freedom, and z and u are independent
<ul>
<li>It is used in the Student t-test to compare 2 sample means and determine the probability that they come from the same population</li>
<li>If we draw n independent observation from a normal population the quantity <eq env="math">\frac{\bar X-\mu}{S/\sqrt{n}}</eq> has a t distribution with <eq env="math">df=n-1</eq></li>
<li>It is similar to the normal distribution, but it is more spread out</li>
<li>Its PDF is quite complex</li>
<li>It converges to the normal distribution for <eq env="math">k \to +\infty</eq></li>
<li>Its areas are computed via tables or software</li>
</ul></li>
<li>The Gumbel or extreme value distribution is used to model the extreme values of a number of samples of various distributions
<ul>
<li>It was invented to model the distribution of extreme temperatures during the year, and it is used for modeling rare phenomena</li>
<li>The PDF is the exponential of an exponential, it decreases very, very fast
<ul>
<li><eq env="math">f(x)=e^{-e^{(x-\mu) /\beta}}</eq></li>
</ul></li>
<li>The Gumbel distribution is used by BLAST for calculating the E-value
<ul>
<li>The expected score E of a match in a database is the number of times that my sequence would obtain a score S higher than the observed one in a random database of the same size</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>rewied until here</p>
<h2 id="hypothesis-testing">Hypothesis testing</h2>
<ul>
<li>When performing an experiment, before analyzing the result I need to formulate hypotheses and assumptions
<ul>
<li>I should state a null hypothesis that I want to disprove</li>
<li>I should state what are my assumptions: statistical independence, distribution of the data…</li>
<li>I can compute the needed statistics from my data</li>
<li>I can check the probability of observing my statistics under the null hypothesis, for the distribution given by my assumptions</li>
<li>If my statistics are more extreme than the critical value chosen, I can reject the null hypothesis</li>
<li>If my statistics are not more extreme, I fail to reject the null hypothesis</li>
</ul></li>
<li>I can never accept the null hypothesis: it is assumed true from the beginning, and failing to disprove does not mean to prove a hypothesis
<ul>
<li>This is a logical fallacy known as argument from ignorance</li>
</ul></li>
<li>The p-value is the conditional probability of observing a result more extreme or equal to the test statistic given the null hypothesis
<ul>
<li><eq env="math">p=P(X\geq x|H_0)</eq></li>
<li>It was invented by Fisher as a rough estimate of the strength of evidence against the null hypothesis</li>
</ul></li>
<li>The significance of a test is the probability that the test rejects <eq env="math">H_0</eq> if it actually holds
<ul>
<li><eq env="math">\alpha=P(false\:\: positives)</eq></li>
</ul></li>
<li>The power of a test <eq env="math">\-\beta</eq> is the probability that the test rejects a false null hypothesis
<ul>
<li><eq env="math">\beta=P(false negative)</eq></li>
</ul></li>
<li>In any test, we want <eq env="math">\alpha</eq> and <eq env="math">\beta</eq> as small as possible</li>
<li>The threshold (critical value) used depends on the significance that we desire
<ul>
<li>In biology it is common to use <eq env="math">p=0.05</eq></li>
<li>In physics it is used <eq env="math">p=10^{-10}</eq> or even lower</li>
</ul></li>
<li>The critical value is the value for which the area under the curve from that value to the extreme of the distribution is equal to the desired p-value</li>
<li>The p-value is currently heavily criticized
<ul>
<li>From the view of Fisher himself, the p-value is a measure of significance, importance of a result, but not a proof: it requires further testing</li>
<li>It is often misinterpreted by researchers</li>
<li>It does not give any information on the magnitude or physical meaning of the observed statistical difference</li>
<li>Two studies can have similar results but very different p-values because of different sample sizes</li>
<li><eq env="math">P \leq x</eq> and <eq env="math">P=x</eq> have a very different meaning</li>
<li>We can and should use both sides of the distribution for calculating p-value, but sometimes one-sided values are reported
<ul>
<li>This gives to the p-value more assumptions based on the belief of the researcher, and it is not fair</li>
</ul></li>
</ul></li>
<li>The Bayesian approach to data validation is an alternative to the p-value
<ul>
<li>It aims at giving a more useful estimate: <eq env="math">P(H_0|X\geq x)</eq></li>
<li>It is computed with the Bayes theorem from p-value, <eq env="math">P(H_0)</eq> and <eq env="math">P(X\geq x)</eq></li>
<li>I need the <em>a priori</em> probability of the observation, which usually is not available</li>
<li>The Bayesian factor is the ratio between the probability of my data give the null hypothesis and union of the probabilities of my data given any other hypothesis</li>
</ul></li>
<li>The Fisher test is aimed at determining the probability of observing the data under the assumption that the categories are independent
<ul>
<li>I prepare a contingency table where I put the number of subjects in each combination of categories</li>
<li>From the table I take the various coefficients and compute the probability of obtaining that table under <eq env="math">H_0</eq> (no association between the variables)</li>
<li>The probability is computed using the binomial</li>
<li>The output is a number between 0 and 1 which is the probability of obtaining that table</li>
<li>It does NOT give the p-value because it doesn’t consider tables that are more extreme of the observed one</li>
<li>In a 2*2 table with coefficients a, b, c, d and total number of subjects n <eq env="math">p=\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{n!a!b!c!d!}</eq></li>
</ul></li>
<li>When I do n tests on the same data, I can expect to have many false positives, in a way that is proportional to n
<ul>
<li>The simplest correction that can be made to preserve the significance is the Bonferroni correction
<ul>
<li><eq env="math">\alpha'=\alpha/n</eq></li>
<li>The actual significance to be accepted is the original significance divided by the number of tests</li>
<li>Sometimes it is too strict, it fails to reject <eq env="math">H_0</eq> that are false</li>
</ul></li>
<li>Another common correction is the Benjamini-Hochberg</li>
</ul></li>
<li>The means of samples from the same population are normally distributed with a mean equal to the mean of the population and variance equal to the variance of the population divided by the size of the samples
<ul>
<li>The sample mean is <eq env="math">M=\frac{1}{n}\sum_{i=1}^n x_i</eq></li>
<li>The mean of the population <eq env="math">\mu</eq> is best approximated by the mean of the sample means
<ul>
<li><eq env="math">\mu=E[M]</eq></li>
</ul></li>
<li>The variance of the sample mean is the variance of the population divided by the sample size
<ul>
<li><eq env="math">Var[M]=\frac{1}{n} \sigma^2</eq></li>
<li>The unbiased variance is used, computed by <eq env="math">\frac{1}{n-1}\sum_{i=1}^n(x_i-M)^2</eq></li>
</ul></li>
</ul></li>
<li>The z-test uses the z-tables to compute the probability of observing a certain standardized value
<ul>
<li>If our <eq env="math">H_0</eq> gives us a true population mean (<eq env="math">H_0:M=\mu</eq>), we can compute the standardized variable <eq env="math">z=\frac{M-\mu}{\sigma/\sqrt{n}}</eq>
<ul>
<li>I use <eq env="math">\sigma/\sqrt{n}</eq> beacause I need the standard deviation of the sample means, not of the samples</li>
<li>Note that for the z-test I need the real variance of the population, <eq env="math">\sigma^2</eq>, not its estimate based on the sample variance</li>
</ul></li>
<li>I need to use the 2 tails if I cannot completely exclude deviations in one direction</li>
</ul></li>
<li>If instead that the actual variance of the population I use its estimate, the unbiased sample variance S, I need to use the t-distribution with n-1 degrees of freedom
<ul>
<li>The variable t is computed as <eq env="math">t=\frac{M-\mu}{S/\sqrt{n}}</eq></li>
</ul></li>
<li>The t-test can also be used to compare the means of 2 samples, to test if they are significantly different
<ul>
<li>In this case I require the t-distribution with 2n-2 degrees of freedom</li>
<li>The formula for computing the variable t is quite complex</li>
</ul></li>
<li>The ANOVA (ANalisys Of VAriance), also called F-test, is a generalization of the t-test used to compare k means
<ul>
<li>When comparing 2 means, the ANOVA reduces to the t-test with relation <eq env="math">F=t^2</eq></li>
<li>It assumes that the samples are random and the errors are independent, that the populations are normally distributed and that for each condition the populations have the same variance</li>
</ul></li>
<li>The one-way ANOVA compares means of different samples (treatments in the ANOVA jargon)
<ul>
<li>The <eq env="math">H_0</eq> is that all the means are equal</li>
<li>From the different treatments we can compute the means for each treatment and the global mean of all data</li>
<li>We can compute the treatment variances and the global variance in the same way</li>
<li>The total variance can be partitioned in random variation and in variation between treatments
<ul>
<li>The sum of squares within (SSW) is computed as <eq env="math">\sum_{i,j} (x_{i,j}*\mu_j)^2</eq></li>
<li>Each value is compared to the mean of its treatment</li>
<li>The sum of squares between (SSB) is computed as <eq env="math">\sum_{j} n_j(\mu_j*\mu)^2</eq></li>
<li>Each treatment mean is compared to the global mean and multiplied for the number of samples in the treatment</li>
</ul></li>
<li>It can be proven that the sum of squares total (SST) is equal to the sum of SSW and SSB
<ul>
<li><eq env="math">SST=\sigma^2*n_{tot}=\sum_i (x_i-\mu)^2=SSW+SSB</eq></li>
</ul></li>
<li>We can then define the variable <eq env="math">f \: : \: f=\frac{MSB}{MSW}</eq>
<ul>
<li><eq env="math">MSW=SSW/(n_{tot}-n_{treatments})</eq>
<ul>
<li>It is the mean square within</li>
</ul></li>
<li><eq env="math">MSB=SSB/(n_{treatments}-1)</eq>
<ul>
<li>It is the mean square between</li>
</ul></li>
</ul></li>
<li>The variable f is distributed following the Fisher-Snedecor F distribution
<ul>
<li>There is one distribution for each combination of degrees of freedom within and between</li>
</ul></li>
<li>The null hypothesis can be rejected when f is high enough</li>
<li>If <eq env="math">f\leq1</eq> the variability between is equal or lower than the variability within</li>
</ul></li>
<li>Two-way ANOVA can be used to compare the response to different combination of 2 variables
<ul>
<li>It can be used in different treatments defined as different combinations of doses of 2 different drugs</li>
<li>There are 3 <eq env="math">H_0</eq>, if I call the treatments A and B
<ul>
<li>No difference in means due to factor A</li>
<li>No difference in means due to factor B</li>
<li>No interaction between A and B</li>
</ul></li>
<li>It tests for the effect of the single variables and for interaction between them</li>
<li>If we have the variable A and B, SST is composed of SSW, SSB(A), SSB(B), SSB(A,B)</li>
<li>I have a SSB for each treatment level, and for any treatment combination</li>
<li>In order to be powerful, if we increase the number of treatments we need a lot of data</li>
<li>It is frequently used for the analysis of expression data</li>
</ul></li>
<li>The <eq env="math">\chi^2</eq>-test is used to determine if the unbiased sample variance is significantly different from the variance of the population
<ul>
<li>We can define the sum of squared distances from the mean <eq env="math">Q=\sum (x_i-\mu)^2</eq></li>
<li>Q is distributed with a <eq env="math">\chi^2</eq> distribution with n-1 degrees of freedom</li>
</ul></li>
<li>The Pearson’s <eq env="math">\chi^2</eq> test is used to determine if some data fits a certain distribution, with certain parameters
<ul>
<li><eq env="math">\chi^2=\sum \frac{(O_i-E_i)^2}{E_i}</eq></li>
<li>This variable follows a <eq env="math">\chi^2</eq> distribution with n-s-1 degrees of freedom, where s is the number of parameters that define the specific distribution tested</li>
<li><eq env="math">O_i</eq> is the observed value, <eq env="math">E_i</eq> the expected value given the model</li>
</ul></li>
<li>Non-parametric tests make very few assumptions about the data
<ul>
<li>They do not estimate parameters from the data and do not assume any distribution</li>
<li>They can be used with qualitative data and ordinal data
<ul>
<li>Ordinal data relies on median instead of mean</li>
</ul></li>
<li>They are not susceptible to outliers</li>
<li>They are tipically used when the data are too skewed for a paramatric test</li>
<li>They are less powerfull than parametric tests, they are more susceptible to type II errors</li>
<li>Each parametric test has a non-parametric alternative</li>
</ul></li>
</ul>
<h2 id="correlation-and-regression">Correlation and regression</h2>
<ul>
<li>Correlation can be measured by a coefficient <eq env="math">\rho</eq>
<ul>
<li>Given the variables x and y, we can compute <eq env="math">\mu_x</eq> and <eq env="math">\mu_y</eq></li>
<li>If we graph a point (<eq env="math">\mu_x,\mu_y)</eq> we define the centroid of the data</li>
<li>The covariance is a good measure of linear dependence between x and y
<ul>
<li><eq env="math">cov(x,y)=\frac{1}{n-1}\sum_i (x_i-\mu_x)(y_i-\mu_y)</eq></li>
</ul></li>
<li>We can then define the Pearson’s correlation coefficient <eq env="math">\rho=\frac{cov(x,y)}{\sigma_x \sigma_y}</eq>
<ul>
<li>It is a number between -1 and 1 because it is rescaled by the standard deviations of x and y</li>
<li>The Pearson’s coefficient tests only linear dependency (!)
<ul>
<li>If <eq env="math">\rho=0</eq> we can NOT say that x and y are independent, they can have higher-order dependencies</li>
</ul></li>
</ul></li>
<li>The significance of the coefficient is strongly dependent on the number of points
<ul>
<li>The value of the coefficient <em>per se</em> does not mean anything, it has always to be tested</li>
<li>It can be tested using the Student distribution</li>
</ul></li>
<li>The Person’s coefficient is very sensitive to outliers</li>
<li>Always look at the graph before drawing conclusions, because it can be different fro what you think</li>
</ul></li>
<li>CORRELATION DOES NOT IMPLY CAUSATION</li>
<li>The Spearman’s correlation coefficient measure monotonic dependency
<ul>
<li>It is the Pearson’s coefficient of the rank of the variables, it is its non-paramtric alternative</li>
<li>If we make a ranking of values from the lowest to the highest, the rank of a value is its position in the ranking</li>
<li>We have to test the significance also of this coefficient, that depends on the amount of data</li>
</ul></li>
<li>The Matthews correlation coefficient (MCC) is used for categorical variables
<ul>
<li>We can assign the values 0 and 1 to the categories in both values</li>
<li>We can take the Pearson’s coefficient of these values, which is the MCC</li>
<li>It is used in machine learning in a table real vs predicted</li>
</ul></li>
<li>When the dependency is more complex, we can use the mutual information
<ul>
<li>It will be a topic for next year</li>
</ul></li>
<li>If we have data with a good Pearson correlation, we can define a linear regression</li>
<li>One technique is to minimize the distance between the points and the line (best fit with least squares)
<ul>
<li>We want to minimize the function <eq env="math">f(a,b)=\sum_i [y_i-(ax_i+b)]^2</eq></li>
</ul></li>
<li>The same technique can be applied for fitting any polynomial
<ul>
<li><eq env="math">y=P(x)\sum_{k=0}^p a_k*x^k</eq></li>
</ul></li>
<li>If I use a high degree polynomial I risk doing overfitting
<ul>
<li>I have overfitting when the number of data is of the same order of the number of parameters</li>
<li>In overfitting the values of the parameters are often absurd (really high in absolute value), without any physical meaning</li>
</ul></li>
<li>The error can be estimated considering overfitting by giving a penalty for high coefficients</li>
<li>To test the quality of a model we need to use data not used for building the model itself (!)</li>
<li>Cross-validation is very dangerous</li>
</ul>
<h2 id="principal-component-analysis">Principal component analysis</h2>
<ul>
<li>Principal component analysis is used with high-dimensional data
<ul>
<li>It reduces the number of dimensions, so to be able to plot the results</li>
<li>A good idea to preliminarily decrease the number of variables is to remove the ones with the lowest variance</li>
<li>If we want to find a better system of reference, we can choose the axes with 0 covariance</li>
</ul></li>
<li>I can build the covariance matrix of the variables
<ul>
<li>The covariance matrix is symmetric and therefore it can be diagonalized easily</li>
<li>The change of basis matrix U is orthogonal and therefore represents a rotation</li>
<li><eq env="math">\Lambda=\begin{pmatrix}\lambda_1&amp;0\\0&amp;\lambda_2\end{pmatrix}</eq></li>
<li>If I rank the eigenvalues from highest to lowest, I can choose to use only a subset of dimensions that maximizes the variation</li>
</ul></li>
<li>PCA can discriminate only linear dependencies (!) by rotating the frame of reference so to align it with the variation</li>
<li>If a variable has a relatively high covariance, it will dominate the principal components</li>
<li>It makes sense to compare variances only if they are in the same unit of measure</li>
<li>I can standardize the variables so to have unit variance and 0 mean to avoid this problems</li>
<li>Instead of a covariance matrix, I can do PCA with a correlation matrix</li>
<li>The correlation of a variable on each of the eigenvectors is called loading</li>
<li>To choose the number of principal components, a common rule is to exclude the dimension with <eq env="math">\lambda &lt; 1</eq></li>
</ul>
